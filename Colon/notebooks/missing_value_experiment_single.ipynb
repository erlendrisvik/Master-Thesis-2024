{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Disclaimer\n",
    "The experiment was initially conducted with both nunmerical and categorical variables. After discussions with supervisors, we found it reasonable to stick to only categorical variables due to the problem of RMSE under MAR. This made changes to the code and a lot of variables may have redundant namings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Missing value experiment single"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notebook to overhaul previous version of generating missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import TargetEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.impute import KNNImputer\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "from lifelines import CoxPHFitter\n",
    "from sksurv.linear_model import CoxPHSurvivalAnalysis\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sksurv.metrics import (\n",
    "    concordance_index_censored\n",
    ")\n",
    "from sksurv.util import Surv\n",
    "\n",
    "try:\n",
    "    from utils.utils import *\n",
    "    from utils.encoders import MultiLabelEncoder\n",
    "except:\n",
    "    import sys\n",
    "    sys.path.append('../')\n",
    "    from utils.utils import *\n",
    "    from utils.encoders import MultiLabelEncoder    \n",
    "    \n",
    "\n",
    "def mode_imputation(X_train, X_test):\n",
    "    \"\"\"\n",
    "    Impute missing values using mode imputation.\n",
    "    Note: Only for categorical features.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    X_train : torch.DoubleTensor, shape (n_train, d)\n",
    "        Data matrix with missing values.\n",
    "\n",
    "    X_test : torch.DoubleTensor, shape (n_test, d)\n",
    "        Data matrix with missing values.\n",
    "    \n",
    "    Returns\n",
    "    ----------\n",
    "    X_imp_train : torch.DoubleTensor, shape (n_train, d)\n",
    "            Data matrix with missing values imputed using mode imputation.\n",
    "\n",
    "    X_imp_test : torch.DoubleTensor, shape (n_test, d)\n",
    "            Data matrix with missing values imputed using mode imputation from train.\n",
    "    \"\"\"\n",
    "    \n",
    "    X_imp_train = X_train.clone()\n",
    "    X_imp_test = X_test.clone()\n",
    "    mask_train = torch.isnan(X_imp_train)\n",
    "    mask_test = torch.isnan(X_imp_test)\n",
    "\n",
    "    mode_values = torch.mode(X_imp_train, dim = 0)[0]\n",
    "\n",
    "    for i in range(X_imp_train.shape[1]):\n",
    "        X_imp_train[mask_train[:, i], i] = mode_values[i]\n",
    "        X_imp_test[mask_test[:, i], i] = mode_values[i]\n",
    "\n",
    "    return {\"train\": X_imp_train, \"test\" : X_imp_test}\n",
    "\n",
    "def knn_imputation(X_train, X_test, n_neighbors = 2):\n",
    "    \"\"\"\n",
    "    Impute missing values using KNN imputation. Data is scaled.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    X_train : torch.DoubleTensor, shape (n_train, d)\n",
    "        Data matrix with missing values.\n",
    "\n",
    "    X_test : torch.DoubleTensor, shape (n_test, d)\n",
    "        Data matrix with missing values.\n",
    "\n",
    "    n_neighbors : int\n",
    "        Number of neighbors to use for imputation.\n",
    "    \n",
    "    Returns\n",
    "    ----------\n",
    "    X_train_imp : torch.DoubleTensor, shape (n_train, d)\n",
    "            Data matrix with missing values imputed using KNN imputation.\n",
    "\n",
    "    X_test_imp : torch.DoubleTensor, shape (n_test, d)  \n",
    "            Data matrix with missing values imputed using KNN imputation from train.\n",
    "    \"\"\"\n",
    "\n",
    "    X_imp_train = X_train.clone()\n",
    "    X_imp_test = X_test.clone()\n",
    "\n",
    "    # Impute train\n",
    "    imputer = KNNImputer(n_neighbors = n_neighbors)\n",
    "    scaler = StandardScaler()\n",
    "    X_imp_train_sc = scaler.fit_transform(X_imp_train)\n",
    "    imputed_train = imputer.fit_transform(X_imp_train_sc)\n",
    "    imputed_train = scaler.inverse_transform(imputed_train)\n",
    "\n",
    "    # Impute test using fit from train\n",
    "    X_imp_test_sc = scaler.transform(X_imp_test)\n",
    "    imputed_test = imputer.transform(X_imp_test_sc)\n",
    "    imputed_test = scaler.inverse_transform(imputed_test)\n",
    "\n",
    "    # Round to assure integers\n",
    "    imputed_train, imputed_test = torch.from_numpy(imputed_train).round(), torch.from_numpy(imputed_test).round()\n",
    "    imputed_train, imputed_test = imputed_train.double(), imputed_test.double()\n",
    "    \n",
    "    return {\"train\": imputed_train, \"test\" : imputed_test}\n",
    "        \n",
    "def accuracy(X_imp, X_true, mask, column_wise = False):\n",
    "    \"\"\"\n",
    "    Accuracy between imputed variables and ground truth.\n",
    "    Pytorch/Numpy agnostic\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    X_imp : torch.DoubleTensor or np.ndarray, shape (n, d)\n",
    "        Data with imputed variables.\n",
    "    \n",
    "    X_true : torch.DoubleTensor or np.ndarray, shape (n, d)\n",
    "        Ground truth.\n",
    "    \n",
    "    mask : torch.BoolTensor or np.ndarray of booleans, shape (n, d)\n",
    "        Missing value mask (missing if True)\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    acc : torch.DoubleTensor or np.ndarray of floats, shape (d,) if column_wise = True, else float\n",
    "        accuracy between imputed variables and ground truth.\n",
    "    \"\"\"\n",
    "\n",
    "    if not mask.any():\n",
    "        return torch.tensor(1, dtype=torch.float64)   \n",
    "    \n",
    "    if torch.is_tensor(X_imp):\n",
    "        if column_wise:\n",
    "            acc = []\n",
    "            for col in range(X_imp.shape[1]):\n",
    "                if not mask[:, col].any():\n",
    "                    acc.append(torch.tensor(0, dtype=torch.float64))\n",
    "                else:\n",
    "                    diff = (X_imp[:, col] == X_true[:, col]).double()\n",
    "                    valid_diff = diff[mask[:, col]]\n",
    "                    acc.append(valid_diff.mean())\n",
    "            return torch.stack(acc)\n",
    "        else:\n",
    "            return ((X_imp[mask] == X_true[mask]).double().mean())\n",
    "    else:  # NumPy array\n",
    "        if column_wise:\n",
    "            acc = []\n",
    "            for col in range(X_imp.shape[1]):\n",
    "                if not mask[:, col].any():\n",
    "                    acc.append(0)\n",
    "                else:\n",
    "                    diff = (X_imp[:, col] == X_true[:, col]).double()\n",
    "                    valid_diff = diff[mask[:, col]]\n",
    "                    acc.append(valid_diff.mean())\n",
    "            return np.array(acc)\n",
    "        else:\n",
    "            return ((X_imp[mask] == X_true[mask]).mean())\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimulationMV():\n",
    "    def __init__(self, X, duration_col, event_col, cat_colnames):\n",
    "        \"\"\"\n",
    "        Initialize the SimulationMV class. \n",
    "        Event and duration is removed for X before simulation.\n",
    "        Encoding types used for modelling are specified for each categorical column.\n",
    "\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : pandas.DataFrame\n",
    "            Complete data matrix\n",
    "        duration_col : str\n",
    "            Name of the column containing the durations.\n",
    "        event_col : str\n",
    "            Name of the column containing the events.\n",
    "        cat_colnames : list of str\n",
    "            List of categorical column names.        \n",
    "        \"\"\"\n",
    "\n",
    "        # Assign parameters to instance\n",
    "        self.duration_col = duration_col\n",
    "        self.event_col = event_col\n",
    "\n",
    "        self.categorical_colnames = cat_colnames\n",
    "\n",
    "        # Reorder the columns\n",
    "        new_order = cat_colnames+[duration_col, event_col]\n",
    "        X = X[new_order]\n",
    "\n",
    "        # Store original X for baseline evaluation\n",
    "        self.X_original = X.copy()\n",
    "\n",
    "        # Remove duration and event col. We don't want to impute and evaluate on these\n",
    "        X_to_sim = self._remove_event_and_duration(X)\n",
    "        X_to_sim = X_to_sim.to_numpy()\n",
    "\n",
    "        # split into numerical and categorical\n",
    "        X_cat_init = X_to_sim\n",
    "        \n",
    "        # Define ordinal, one-hot and target encoding columns\n",
    "        self.ordinal_idx = []\n",
    "        self.one_hot_idx = []\n",
    "        self.target_idx = []\n",
    "\n",
    "        for i in range(X_cat_init.shape[1]):\n",
    "            if not isinstance(X_cat_init[0, i], str):\n",
    "                self.ordinal_idx.append(i)\n",
    "            else:\n",
    "                if len(np.unique(X_cat_init[:, i])) == 2:\n",
    "                    self.one_hot_idx.append(i)\n",
    "                else:\n",
    "                    self.target_idx.append(i)\n",
    "        \n",
    "        self.not_ordinal_idx = [i for i in range(X_cat_init.shape[1]) if i not in self.ordinal_idx] \n",
    "        self.one_hot_cols = [self.categorical_colnames[i] for i in self.one_hot_idx]\n",
    "        self.target_enc_cols = [self.categorical_colnames[i] for i in self.target_idx]\n",
    "        \n",
    "        # (Temporarily) encode map categorical variables to integers for simulating missing values\n",
    "        # Torch is unable to handle strings.\n",
    "        X_cat_enc = self._map_categorical(X_cat_init)\n",
    "        \n",
    "        self.X = X_cat_enc\n",
    "\n",
    "        # Evaluate groundtruth coxPH\n",
    "        self.GT_weights, self.GT_concordance = self._fit_single_cox_PH(self.X_original, None).values()\n",
    "\n",
    "    def _remove_event_and_duration(self, X):\n",
    "        \"\"\"\n",
    "        Remove event and duration columns from the data.\n",
    "        Re-set the indices for the categorical and numerical columns.\n",
    "        This is to avoid simulating missing values with these columns.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : pandas.DataFrame\n",
    "            Data matrix.\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        pandas.DataFrame\n",
    "            Data matrix with event and duration columns removed.\n",
    "        \"\"\"\n",
    "\n",
    "        X = X.drop(columns = [self.duration_col, self.event_col])\n",
    "\n",
    "        cat_idx = [X.columns.get_loc(col) for col in self.categorical_colnames]\n",
    "        self.categorical_cols = cat_idx\n",
    "        return X\n",
    "    \n",
    "    def _add_event_and_duration(self, X, duration_col, event_col):\n",
    "        \"\"\"\n",
    "        Add event and duration columns to the data.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : pandas.DataFrame or np.ndarray\n",
    "            Data matrix.\n",
    "\n",
    "        duration_col : pandas.Series\n",
    "            Duration column.\n",
    "\n",
    "        event_col : pandas.Series\n",
    "            Event column.\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        pandas.DataFrame\n",
    "            Data matrix with event and duration columns added.\n",
    "        \"\"\"\n",
    "\n",
    "        if isinstance(X, pd.DataFrame):\n",
    "            # Add columns\n",
    "            X[self.duration_col] = duration_col\n",
    "            X[self.event_col] = event_col\n",
    "        else:\n",
    "            X = np.concatenate(X, [duration_col, event_col], axis = 1)\n",
    "        return X\n",
    "\n",
    "    def _fit_single_cox_PH(self, X_train, X_test, penalizer = 0):\n",
    "        \"\"\" \n",
    "        Fit cox PH model to the training data. Evaluates concordance on both train and test data.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X_train : pandas.DataFrame\n",
    "            Training data. Categories are mapped back to strings.\n",
    "\n",
    "        X_test : pandas.DataFrame\n",
    "            Test data. Categories are mapped back to strings.\n",
    "\n",
    "        penalizer : float\n",
    "            Regularization strength.\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        dict\n",
    "            Dictionary containing the weights and concordance index (train and test).\n",
    "        \"\"\"\n",
    "\n",
    "        me = TargetEncoder(smooth = \"auto\", target_type = \"continuous\", random_state=3155135)\n",
    "        sc = StandardScaler()\n",
    "        cph = CoxPHSurvivalAnalysis(alpha=penalizer)\n",
    "        util = Surv()\n",
    "\n",
    "        if X_test is None:\n",
    "            # Prepare X and y. This is the complete data.\n",
    "            # Define proper encodings\n",
    "            y = X_train[[self.event_col, self.duration_col]]\n",
    "            X = X_train.drop([self.event_col, self.duration_col], axis=1)\n",
    "            X = pd.get_dummies(X, columns = self.one_hot_cols, drop_first=True)\n",
    "            X[self.target_enc_cols] = me.fit_transform(X[self.target_enc_cols], y[self.duration_col])\n",
    "            y = y.to_records(index=False)\n",
    "            X_sc = sc.fit_transform(X)\n",
    "            X_sc = pd.DataFrame(X_sc, columns = sc.feature_names_in_)\n",
    "            cph.fit(X_sc, y)\n",
    "            weights = cph.coef_\n",
    "            weights = dict(zip(X_sc.columns, weights))\n",
    "            preds = cph.predict(X_sc)\n",
    "            conc = concordance_index_censored(event_indicator = y[self.event_col], event_time = y[self.duration_col], estimate = preds)[0]\n",
    "            conc = round(conc, 3)\n",
    "\n",
    "            # Lifelines summary of complete model\n",
    "            X = pd.get_dummies(X_train, columns = self.one_hot_cols, drop_first=True)\n",
    "            X[self.target_enc_cols] = me.fit_transform(X[self.target_enc_cols], y[self.duration_col])\n",
    "            # Temporarily remove y to not scale it\n",
    "            y = X[[self.event_col, self.duration_col]]\n",
    "            X = X.drop([self.event_col, self.duration_col], axis=1)\n",
    "            X_sc = sc.fit_transform(X)\n",
    "            X_sc = pd.DataFrame(X_sc, columns = sc.feature_names_in_)\n",
    "            X_sc[[self.event_col, self.duration_col]] = y\n",
    "\n",
    "            self.complete_model = CoxPHFitter(baseline_estimation_method='breslow', penalizer = penalizer)\n",
    "            self.complete_model.fit(X_sc, \n",
    "                               duration_col = self.duration_col, \n",
    "                               event_col= self.event_col,\n",
    "                               show_progress=False)\n",
    "\n",
    "            return {'weights': weights, \n",
    "                    'concordance': conc}\n",
    "\n",
    "        # One hot encode binary\n",
    "        X_train = pd.get_dummies(X_train, columns = self.one_hot_cols, drop_first = True)\n",
    "        X_test = pd.get_dummies(X_test, columns = self.one_hot_cols, drop_first = True)\n",
    "\n",
    "        # Ensure all category levels are present\n",
    "        for column in X_train.columns:\n",
    "            if column not in X_test.columns:\n",
    "                X_test[column] = 0\n",
    "        X_test  = X_test[X_train.columns]\n",
    "\n",
    "        # Technicality of converting to structured array\n",
    "        y_train = util.from_arrays(event = X_train[self.event_col], time = X_train[self.duration_col], name_event = self.event_col, name_time = self.duration_col)\n",
    "        y_test = util.from_arrays(event = X_test[self.event_col], time = X_test[self.duration_col], name_event = self.event_col, name_time = self.duration_col)\n",
    "        X_train = X_train.drop([self.event_col, self.duration_col], axis=1)\n",
    "        X_test = X_test.drop([self.event_col, self.duration_col], axis=1)\n",
    "\n",
    "        # Target encode train and transform test\n",
    "        X_train[self.target_enc_cols] = me.fit_transform(X_train[self.target_enc_cols], y_train[self.duration_col])\n",
    "        X_test[self.target_enc_cols] = me.transform(X_test[self.target_enc_cols])\n",
    "\n",
    "        # Scale X (not event and duration)\n",
    "        X_train_sc = sc.fit_transform(X_train)\n",
    "        X_train_sc = pd.DataFrame(X_train_sc, columns = sc.feature_names_in_)\n",
    "        X_test_sc = sc.transform(X_test)\n",
    "        X_test_sc = pd.DataFrame(X_test_sc, columns = sc.feature_names_in_)\n",
    "        \n",
    "        # For regularizing singular matrices\n",
    "        alpha_values = [0, 10, 100, 1000, 10000]\n",
    "        for alpha in alpha_values:\n",
    "            try:\n",
    "                cph = CoxPHSurvivalAnalysis(alpha = alpha)\n",
    "                cph.fit(X_train_sc, y_train)\n",
    "                preds_train = cph.predict(X_train_sc)\n",
    "                preds_test = cph.predict(X_test_sc)\n",
    "                \n",
    "                conc_train = concordance_index_censored(event_indicator = y_train[self.event_col], event_time = y_train[self.duration_col], estimate = preds_train)[0]\n",
    "                conc_test = concordance_index_censored(event_indicator = y_test[self.event_col], event_time = y_test[self.duration_col], estimate = preds_test)[0]\n",
    "                conc_train = round(conc_train, 3)\n",
    "                conc_test = round(conc_test, 3)\n",
    "                \n",
    "                weights = cph.coef_\n",
    "                weights = dict(zip(X_train.columns, weights))\n",
    "\n",
    "                return {'weights': weights, \n",
    "                'concordance_train': conc_train,\n",
    "                'concordance_test': conc_test}\n",
    "            \n",
    "            except Exception as e:\n",
    "                print(f\"Failed with alpha={alpha}: {e}\")\n",
    "\n",
    "    def complete_summary(self):\n",
    "        \"\"\" \n",
    "        Return a lifelines summary of cox ph of full data.\n",
    "        This is used because SciKit Survival does not have statistical information such as p-values etc.\n",
    "        \"\"\"\n",
    "\n",
    "        return self.complete_model.print_summary()\n",
    "    \n",
    "    def _map_categorical(self, X_cat):\n",
    "        \"\"\"\n",
    "        Temporarily map categorical variables to integers.\n",
    "        X_cat is assumed to be a numpy ndarray.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X_cat : np.ndarray\n",
    "            Categorical data matrix.\n",
    "    \n",
    "        Returns\n",
    "        -------\n",
    "        X_cat_enc : np.ndarray\n",
    "            Encoded categorical data matrix of integers.\n",
    "        \"\"\"\n",
    "        self.multi_le = MultiLabelEncoder()\n",
    "        \n",
    "        # Initialize an array of the same shape as X_cat to hold encoded data\n",
    "        X_cat_enc = np.empty(shape=X_cat.shape, dtype = \"object\")\n",
    "        # create a list of the columns that are not ordinal\n",
    "\n",
    "        encoded_columns = self.multi_le.fit_transform(X_cat[:, self.not_ordinal_idx])\n",
    "        X_cat_enc[:, self.not_ordinal_idx] = encoded_columns\n",
    "        X_cat_enc[:, self.ordinal_idx] = X_cat[:, self.ordinal_idx]\n",
    "        return X_cat_enc\n",
    "\n",
    "    def _decode_categorical(self, X_cat_enc):\n",
    "        \"\"\"\n",
    "        Decode categorical variables, excluding columns specified in self.ordinal_idx.\n",
    "        self.ordinal_idx contains indices of columns that should not be decoded because they are ordered.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X_cat_enc : np.ndarray\n",
    "            Encoded categorical data matrix of integers.\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        X_cat_dec : np.ndarray\n",
    "            Decoded categorical data matrix of strings.\n",
    "        \"\"\"\n",
    "\n",
    "        # Initialize an array of the same shape as X_cat_enc to hold decoded data\n",
    "        X_cat_dec = np.empty(shape=X_cat_enc.shape, dtype = \"object\")\n",
    "        decoded_columns = self.multi_le.inverse_transform(X_cat_enc[:, self.not_ordinal_idx])\n",
    "        X_cat_dec[:, self.not_ordinal_idx] = decoded_columns\n",
    "        X_cat_dec[:, self.ordinal_idx] = X_cat_enc[:, self.ordinal_idx]\n",
    "        return X_cat_dec\n",
    "     \n",
    "    def _simulate_single_na_dataset(self, p_miss, mecha = \"MCAR\", opt = None, p_obs = None, q = None,\n",
    "                                    sample_seed = 135135, column_seed = 115342):\n",
    "        \"\"\"\n",
    "        Generate missing values for specifics missing-data mechanism and proportion of missing values. \n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        p_miss : float\n",
    "            Proportion of missing values to generate for variables which will have missing values.\n",
    "\n",
    "        mecha : str, \n",
    "                Indicates the missing-data mechanism to be used. \"MCAR\" by default, \"MAR\" or \"MNAR\".\n",
    "\n",
    "        opt: str, \n",
    "             For mecha = \"MNAR\", it indicates how the missing-data mechanism is generated: using a logistic regression (\"logistic\"), \n",
    "             quantile censorship (\"quantile\") or logistic regression for generating a self-masked MNAR mechanism (\"selfmasked\").\n",
    "\n",
    "        p_obs : float\n",
    "                If mecha = \"MAR\", or mecha = \"MNAR\" with opt = \"logistic\" or \"quanti\", proportion of variables with *no* \n",
    "                missing values that will be used for the logistic masking model.\n",
    "\n",
    "        q : float\n",
    "            If mecha = \"MNAR\" and opt = \"quanti\", quantile level at which the cuts should occur.\n",
    "\n",
    "        sample_seed : int\n",
    "            Seed for the random number generator used to generate the missing samples.\n",
    "            Also used in train test split.\n",
    "\n",
    "        column_seed : int\n",
    "            Seed for the random number generator used to generate the missing columns.\n",
    "\n",
    "        Returns\n",
    "        ----------\n",
    "        A dictionary containing:\n",
    "        'X_init': the initial data matrix.\n",
    "        'X_na': a tensor (n, d) containing the dataset with missing values.\n",
    "        'mask': a tensor (n, d) containing the mask.\n",
    "        'event_and_duration': a tensor (n, 2) containing the event and duration columns.\n",
    "        \"\"\"\n",
    "\n",
    "        set_sample_seed(sample_seed)\n",
    "        set_column_seed(column_seed)\n",
    "\n",
    "        to_torch = torch.is_tensor(self.X) ## output a pytorch tensor, or a numpy array\n",
    "        if not to_torch:\n",
    "            X = self.X.astype(np.float32)\n",
    "            X = torch.from_numpy(X)\n",
    "        \n",
    "        if mecha == \"MAR\":\n",
    "            mask = MAR_mask(X, p_miss, p_obs).double()\n",
    "        elif mecha == \"MNAR\" and opt == \"logistic\":\n",
    "            mask = MNAR_mask_logistic(X, p_miss, p_obs).double()\n",
    "        elif mecha == \"MNAR\" and opt == \"quantile\":\n",
    "            mask = MNAR_mask_quantiles(X, p_miss, q, 1-p_obs).double()\n",
    "        elif mecha == \"MNAR\" and opt == \"selfmasked\":\n",
    "            mask = MNAR_self_mask_logistic(X, p_miss).double()\n",
    "        else:\n",
    "            mask = MCAR_mask(X, p_miss).double()\n",
    "        \n",
    "        X_nas = X.clone()\n",
    "        X_nas[mask.bool()] = np.nan\n",
    "\n",
    "        # Perform a train/test split on X_init, and use the same indices to split X_na and mask\n",
    "        duration_and_event = self.X_original[[self.duration_col, self.event_col]].to_numpy()\n",
    "\n",
    "        X_init_train, X_init_test, X_nas_train, X_nas_test, mask_train, mask_test, \\\n",
    "        event_and_duration_train, event_and_duration_test = train_test_split(X, X_nas, mask, duration_and_event, test_size=0.3, random_state = sample_seed)\n",
    "\n",
    "        X_init = {'train': X_init_train.double(), 'test': X_init_test.double()}\n",
    "        X_nas = {'train': X_nas_train.double(), 'test': X_nas_test.double()}\n",
    "        mask = {'train': mask_train.bool(), 'test': mask_test.bool()}\n",
    "        duration_and_event = {'train': event_and_duration_train, 'test': event_and_duration_test}\n",
    "        \n",
    "        return {'X_init': X_init, 'X_na': X_nas, 'mask': mask, 'event_and_duration': duration_and_event}\n",
    "    \n",
    "    def _simulate_M_na_datasets(self, M, p_miss, mecha = \"MCAR\", opt = None, p_obs = None, q = None,\n",
    "                               vary_cols = True, save = False, sample_seed = 135135, column_seed = 115342):\n",
    "        \"\"\"\n",
    "        Function to generate M datasets with missing values.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        M : int\n",
    "            Number of datasets to generate.\n",
    "        \n",
    "        vary_cols: bool\n",
    "            If True, the column_seed will vary.\n",
    "            This yields the most variation when simulating MAR because retained columns will vary. \n",
    "\n",
    "        See other params above\n",
    "\n",
    "        Returns\n",
    "        ----------\n",
    "        A dictionary containing:\n",
    "        'X_init': dictionary with train and test of shape (n_train, d, M), (n_test, d, M) respectively. \n",
    "            Contains the initial data matrix, but split into train and test.\n",
    "        'X_na': dictionary with train and test of shape (n_train, d, M), (n_test, d, M) respectively.\n",
    "            Contains the dataset with simulated missing values, but split into train and test.\n",
    "        'mask': dictionary with train and test of shape (n_train, d, M), (n_test, d, M)\n",
    "            Contains the mask, but split into train and test.\n",
    "        'event_and_duration': dictionary with train and test of shape (n_train, 2, M), (n_test, 2, M)\n",
    "        \"\"\"\n",
    "\n",
    "        to_torch = torch.is_tensor(self.X) ## output a pytorch tensor, or a numpy array\n",
    "        if not to_torch:\n",
    "            X = self.X.astype(np.float32)\n",
    "            X = torch.from_numpy(X)\n",
    "\n",
    "        # Set the dimensions used for initiating tensors\n",
    "        temporary_train, temporary_test = train_test_split(X, test_size=0.3, random_state = sample_seed)\n",
    "        n_train = temporary_train.shape[0]\n",
    "        n_test = temporary_test.shape[0]\n",
    "        n = X.shape[0]\n",
    "        d = X.shape[1]\n",
    "\n",
    "        # Initialize empty tensor for the data with missing values and the mask\n",
    "        event_and_duration_train = np.empty((n_train, 2, M), dtype = object)\n",
    "        event_and_duration_test = np.empty((n_test, 2, M), dtype = object)\n",
    "        \n",
    "        X_init_tensor_train = torch.empty((n_train, d,  M))\n",
    "        X_init_tensor_test = torch.empty((n_test, d, M))\n",
    "\n",
    "        X_na_tensor_train = torch.empty((n_train, d, M))\n",
    "        X_na_tensor_test = torch.empty((n_test, d, M))\n",
    "\n",
    "        X_mask_tensor_train = torch.empty((n_train, d, M), dtype = torch.bool)\n",
    "        X_mask_tensor_test = torch.empty((n_test, d, M), dtype = torch.bool)\n",
    "\n",
    "        for i in range(M):\n",
    "            X_init, X_na, X_mask, event_and_duration = self._simulate_single_na_dataset(p_miss = p_miss,\n",
    "                                                                    mecha = mecha,\n",
    "                                                                    p_obs = p_obs,\n",
    "                                                                    sample_seed = sample_seed,\n",
    "                                                                    column_seed = column_seed).values()\n",
    "            \n",
    "            event_and_duration_train[:, :, i] = event_and_duration['train']\n",
    "            event_and_duration_test[:, :, i] = event_and_duration['test']\n",
    "            \n",
    "            X_init_tensor_train[:, :, i] = X_init['train'].double()\n",
    "            X_init_tensor_test[:, :, i] = X_init['test'].double()\n",
    "\n",
    "            X_na_tensor_train[:, :, i] = X_na['train'].double()\n",
    "            X_na_tensor_test[:, :, i] = X_na['test'].double()\n",
    "\n",
    "            X_mask_tensor_train[:, :, i] = X_mask['train'].bool()\n",
    "            X_mask_tensor_test[:, :, i] = X_mask['test'].bool()\n",
    "\n",
    "            # Change seeds\n",
    "            sample_seed += 1\n",
    "            if vary_cols:\n",
    "                column_seed += 1\n",
    "\n",
    "        X_init = {'train': X_init_tensor_train, 'test': X_init_tensor_test}\n",
    "        X_na = {'train': X_na_tensor_train, 'test': X_na_tensor_test}\n",
    "        mask = {'train': X_mask_tensor_train, 'test': X_mask_tensor_test}\n",
    "        event_and_duration = {'train': event_and_duration_train, 'test': event_and_duration_test}\n",
    "\n",
    "        return {'X_init': X_init, 'X_na': X_na, 'mask': mask, 'event_and_duration': event_and_duration}\n",
    "    \n",
    "    def _impute_baselines(self, X_na_train, X_na_test, multivariate = False, n_neighbors = 2):\n",
    "        \"\"\"\n",
    "        Impute missing values using the baseline approaches.\n",
    "        Numerical and categorical variables are imputed separately for univariate.\n",
    "\n",
    "        Parameters:\n",
    "        -----------\n",
    "        X_na_train : torch.DoubleTensor, shape (n_train, d, M)\n",
    "            Train data tensor with missing values.\n",
    "\n",
    "        X_na_test : torch.DoubleTensor, shape (n_test, d, M)\n",
    "            Test data tensor with missing values.\n",
    "\n",
    "        multivariate : bool\n",
    "            If True, use KNN imputation.\n",
    "\n",
    "        n_neighbors : int\n",
    "            Number of neighbors to use for KNN imputation.\n",
    "\n",
    "        Returns:\n",
    "        --------\n",
    "        A dictionary containing:\n",
    "        If univariate:\n",
    "            'train': dictionary with numerical and categorical imputations for train.\n",
    "                Methods are mean, zero and mode.\n",
    "            'test': dictionary with numerical and categorical imputations for test.\n",
    "                Methods are mean, zero and mode.    \n",
    "        If multivariate:\n",
    "            'train': dictionary with KNN imputations for train.\n",
    "            'test': dictionary with KNN imputations for test.\n",
    "        \"\"\"\n",
    "\n",
    "        # separate continous and categorical columns\n",
    "        X_na_cat_train = X_na_train \n",
    "        X_na_cat_test = X_na_test\n",
    " \n",
    "        if not multivariate:\n",
    "            X_mode_train = X_na_cat_train.clone()\n",
    "            X_mode_test = X_na_cat_test.clone()\n",
    "\n",
    "            for i in range(X_na_train.shape[2]):\n",
    "                imputed_train, imputed_test = mode_imputation(X_train = X_na_cat_train[:, :, i], X_test = X_na_cat_test[:, :, i]).values()\n",
    "                X_mode_train[:, :, i] = imputed_train\n",
    "                X_mode_test[:, :, i] = imputed_test\n",
    "\n",
    "            train = {\"categorical\": {\"mode\": X_mode_train}}\n",
    "            test = {\"categorical\": {\"mode\": X_mode_test}}\n",
    "            \n",
    "            return {\"train\": train, \"test\": test}\n",
    "        \n",
    "        else:\n",
    "            X_knn_train = X_na_train.clone()\n",
    "            X_knn_test = X_na_test.clone()\n",
    "\n",
    "            for i in range(X_na_train.shape[2]):\n",
    "                imputed_train, imputed_test = knn_imputation(X_train = X_na_train[:, :, i], X_test = X_na_test[:, :, i], n_neighbors = n_neighbors).values()\n",
    "                X_knn_train[:, :, i] = imputed_train\n",
    "                X_knn_test[:, :, i] = imputed_test\n",
    "            \n",
    "            train = {\"knn\": X_knn_train}\n",
    "            test = {\"knn\": X_knn_test}\n",
    "            return {\"train\": train, \"test\": test}\n",
    "\n",
    "    def _evaluate_univariate_acc(self, X_imp, X_true, X_mask, column_wise = False):\n",
    "        \"\"\"\n",
    "        Function to evaluate the univariate imputation accuracy.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        X_imp : dict\n",
    "            Dictionary containing the imputed datasets. The imputations are numpy tensors.\n",
    "\n",
    "        X_true: torch.DoubleTensor, shape (n_train, d, M) or (n_test, d, M)\n",
    "            Groundtruth data tensor.\n",
    "\n",
    "        X_mask: torch.BoolTensor, shape (n_train, d, M) or (n_test, d, M)\n",
    "            Missing value mask tensor.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        acc : dict\n",
    "            Dictionary containing the pooled accuracy for the imputed datasets.\n",
    "        \"\"\"\n",
    "        # fetch the imputations\n",
    "        X_imp_mode = X_imp[\"categorical\"][\"mode\"]\n",
    "\n",
    "        # separate the true data into numerical and categorical\n",
    "        X_true_cat = X_true\n",
    "        X_mask_cat = X_mask\n",
    "\n",
    "        acc = {\"categorical\": {\"mode\": []}}\n",
    "\n",
    "        M = X_mask.shape[2]\n",
    "\n",
    "        # Get accuracy\n",
    "        for i in range(M):\n",
    "            # evaluate accuracy for categorical features\n",
    "            acc[\"categorical\"][\"mode\"].append(accuracy(X_imp = X_imp_mode[:, :, i], \n",
    "                                                      X_true = X_true_cat[:, :, i],\n",
    "                                                      mask = X_mask_cat[:, :, i],\n",
    "                                                      column_wise = column_wise))\n",
    "        # Take the average and std\n",
    "        for key, value in acc.items():\n",
    "            for k, v in value.items():\n",
    "                acc[key][k] = np.matrix(v)\n",
    "                if column_wise:\n",
    "                    acc[key][k] = (acc[key][k].mean(axis = 0).round(4), acc[key][k].std(axis = 0).round(4))\n",
    "                else:\n",
    "                    acc[key][k] = (acc[key][k].mean().round(4), acc[key][k].std().round(4))\n",
    "        return acc\n",
    "        \n",
    "    def _evaluate_multivariate_acc(self, X_imp, X_true, X_mask):\n",
    "        \"\"\"\n",
    "        Function to evaluate the KNN imputation approach.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X_imp : dict\n",
    "            Dictionary containing the imputed datasets, shape (n_train, d, M) or (n_test, d, M).\n",
    "    \n",
    "        X_true: torch.DoubleTensor, shape (n_train, d, M) or (n_test, d, M)\n",
    "            Groundtruth data tensor.\n",
    "        \n",
    "        X_mask: torch.BoolTensor, shape (n_train, d, M) or (n_test, d, M)\n",
    "            Missing value mask tensor.\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        acc : dict\n",
    "            Dictionary containing the pooled accuracy for the imputed datasets.\n",
    "        \"\"\"\n",
    "\n",
    "        X_imp_knn = X_imp[\"knn\"]\n",
    "        X_imp_cat = X_imp_knn\n",
    "        X_true_cat = X_true\n",
    "        X_mask_cat = X_mask\n",
    "\n",
    "        acc = {\"categorical\": {\"knn\": []}}\n",
    "\n",
    "        M = X_mask.shape[2]\n",
    "        for i in range(M):\n",
    "            acc[\"categorical\"][\"knn\"].append(accuracy(X_imp = X_imp_cat[:, :, i],\n",
    "                                                    X_true = X_true_cat[:, :, i],\n",
    "                                                    mask = X_mask_cat[:, :, i]))\n",
    "        # Take the average and std\n",
    "        for key, value in acc.items():\n",
    "            for k, v in value.items():\n",
    "                acc[key][k] = np.matrix(v)\n",
    "                acc[key][k] = (acc[key][k].mean().round(4), acc[key][k].std().round(4))\n",
    "        return acc\n",
    "        \n",
    "    def _evaluate_univariate_coxPH(self, decoded_imps_train, decoded_imps_test, event_and_duration_train, event_and_duration_test):\n",
    "        \"\"\" \n",
    "        Evaluate the univariate cox PH model on the imputed datasets.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        decoded_imps_train : dict\n",
    "            Dictionary containing the imputed datasets. The imputations are numpy tensors of shape (n_train, d, M).\n",
    "        \n",
    "        decoded_imps_test : dict\n",
    "            Dictionary containing the imputed datasets. The imputations are numpy tensors of shape (n_test, d, M).\n",
    "        \n",
    "        event_and_duration_train : torch.DoubleTensor, shape (n_train, 2, M)\n",
    "            Tensor containing the event and duration columns for the train set.\n",
    "\n",
    "        event_and_duration_test : torch.DoubleTensor, shape (n_test, 2, M)\n",
    "            Tensor containing the event and duration columns for the test set.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        dict\n",
    "            Dictionary containing the concordance index (train and test) and bias for the imputed datasets.\n",
    "        \"\"\"\n",
    "\n",
    "\n",
    "        X_imp_mode_train = decoded_imps_train[\"categorical\"][\"mode\"]\n",
    "        X_imp_mode_test = decoded_imps_test[\"categorical\"][\"mode\"]\n",
    "    \n",
    "        M = X_imp_mode_train.shape[2]\n",
    "        c_index_train = {\"mode\": []}\n",
    "        c_index_test = {\"mode\": []}\n",
    "        bias = {\"mode\": []}\n",
    "        \n",
    "        for i in range(M):\n",
    "            df_mode_train = pd.DataFrame(X_imp_mode_train[:, :, i], columns = self.categorical_colnames)\n",
    "            df_mode_test = pd.DataFrame(X_imp_mode_test[:, :, i], columns = self.categorical_colnames)\n",
    "\n",
    "            df_mode_train = self._add_event_and_duration(df_mode_train, event_and_duration_train[:, 0, i], event_and_duration_train[:, 1, i])\n",
    "            df_mode_test = self._add_event_and_duration(df_mode_test, event_and_duration_test[:, 0, i], event_and_duration_test[:, 1, i])\n",
    "\n",
    "            weights_mode, mode_conc_train, mode_conc_test = self._fit_single_cox_PH(df_mode_train, df_mode_test).values()\n",
    "\n",
    "            c_index_train['mode'].append(mode_conc_train)\n",
    "            c_index_test['mode'].append(mode_conc_test)\n",
    "\n",
    "            abs_bias_mode = self._evaluate_bias_weights(weights_mode)\n",
    "            bias['mode'].append(abs_bias_mode)\n",
    "\n",
    "        \n",
    "        # Take the average and std\n",
    "        for key,value in c_index_train.items():\n",
    "            c_index_train[key] = (np.mean(value).round(4), np.std(value).round(4))\n",
    "        for key,value in c_index_test.items():\n",
    "            c_index_test[key] = (np.mean(value).round(4), np.std(value).round(4))\n",
    "\n",
    "        for key, value in bias.items():\n",
    "            column = pd.DataFrame(value)\n",
    "            mean_and_std = dict(zip(column.columns, zip(round(column.mean(),4), round(column.std(), 4))))\n",
    "            bias[key] = mean_and_std\n",
    "        return {'c_index_train': c_index_train, 'c_index_test': c_index_test, 'bias': bias}\n",
    "    \n",
    "    def _evaluate_multivariate_coxPH(self, decoded_imps_train, decoded_imps_test, event_and_duration_train, event_and_duration_test):\n",
    "        \"\"\"\n",
    "        Evaluate the multivariate cox PH model on the imputed datasets.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        decoded_imps_train : dict\n",
    "            Dictionary containing the imputed datasets. The imputations are numpy tensors of shape (n_train, d, M).\n",
    "\n",
    "        decoded_imps_test : dict\n",
    "            Dictionary containing the imputed datasets. The imputations are numpy tensors of shape (n_test, d, M).\n",
    "\n",
    "        event_and_duration_train : torch.DoubleTensor, shape (n_train, 2, M)\n",
    "            Tensor containing the event and duration columns for the train set.\n",
    "\n",
    "        event_and_duration_test : torch.DoubleTensor, shape (n_test, 2, M)\n",
    "            Tensor containing the event and duration columns for the test set.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        dict\n",
    "            Dictionary containing the concordance index (train and test) and bias for the imputed datasets.\n",
    "        \"\"\"\n",
    "\n",
    "        X_imp_cat_train = decoded_imps_train[\"categorical\"][\"knn\"]\n",
    "        X_imp_cat_test = decoded_imps_test[\"categorical\"][\"knn\"]\n",
    "\n",
    "        M = X_imp_cat_train.shape[2]\n",
    "\n",
    "        c_index_train = {\"knn\": []}\n",
    "        c_index_test = {\"knn\": []}\n",
    "        bias = {\"knn\": []}\n",
    "        \n",
    "        for i in range(M):\n",
    "            df_cat_train = pd.DataFrame(X_imp_cat_train[:, :, i], columns = self.categorical_colnames)\n",
    "            df_joined_train = self._add_event_and_duration(df_cat_train, event_and_duration_train[:, 0, i], event_and_duration_train[:, 1, i])\n",
    "\n",
    "            df_cat_test = pd.DataFrame(X_imp_cat_test[:, :, i], columns = self.categorical_colnames)\n",
    "            df_joined_test = self._add_event_and_duration(df_cat_test, event_and_duration_test[:, 0, i], event_and_duration_test[:, 1, i])\n",
    "\n",
    "            weights, conc_train, conc_test = self._fit_single_cox_PH(df_joined_train, df_joined_test).values()\n",
    "\n",
    "            c_index_train['knn'].append(conc_train)\n",
    "            c_index_test['knn'].append(conc_test)\n",
    "            bias_weights = self._evaluate_bias_weights(weights)\n",
    "            bias['knn'].append(bias_weights)\n",
    "\n",
    "        # Take the average and std\n",
    "        for key,value in c_index_train.items():\n",
    "            c_index_train[key] = (np.mean(value).round(4), np.std(value).round(4))\n",
    "        for key,value in c_index_test.items():\n",
    "            c_index_test[key] = (np.mean(value).round(4), np.std(value).round(4))\n",
    "\n",
    "        for key, value in bias.items():\n",
    "            column = pd.DataFrame(value)\n",
    "            mean_and_std = dict(zip(column.columns, zip(round(column.mean(),4), round(column.std(), 4))))\n",
    "            bias[key] = mean_and_std\n",
    "        return {'c_index_train': c_index_train, 'c_index_test': c_index_test, 'bias': bias}\n",
    "\n",
    "    def _evaluate_bias_weights(self, weights):\n",
    "        \"\"\"\n",
    "        Takes in weights of the ith dataset in M and calculates the absolute bias for each feature.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        weights : dict\n",
    "            Dictionary containing the weights of the cox PH model.\n",
    "            Key is the feature name and value is the weight.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        abs_bias : dict\n",
    "            Dictionary containing the absolute bias for each feature.\n",
    "        \"\"\"\n",
    "\n",
    "        GT_weights = self.GT_weights\n",
    "        abs_bias = {}\n",
    "        for key, value in weights.items():\n",
    "            abs_bias[key] = (value - GT_weights[key])/abs(GT_weights[key])\n",
    "        return abs_bias\n",
    "    \n",
    "    def _CCA(self, X_na_train, X_na_test, event_and_duration_train, event_and_duration_test):\n",
    "        \"\"\"\n",
    "        Function to run CCA.\n",
    "\n",
    "        Parameters:\n",
    "        -----------\n",
    "        X_na_train : np.ndarray, shape (n_train, d, M)\n",
    "            Train data tensor with missing values.\n",
    "\n",
    "        X_na_test : np.ndarray, shape (n_test, d, M)\n",
    "            Test data tensor with missing values.\n",
    "\n",
    "        event_and_duration_train : torch.DoubleTensor, shape (n_train, 2, M)\n",
    "            Tensor containing the event and duration columns for the train set.\n",
    "\n",
    "        event_and_duration_test : torch.DoubleTensor, shape (n_test, 2, M)\n",
    "            Tensor containing the event and duration columns for the test set.\n",
    "\n",
    "        Returns:\n",
    "        --------\n",
    "        dict\n",
    "            Dictionary containing the concordance index (train and test) and bias for the imputed datasets.\n",
    "        \"\"\"\n",
    "        \n",
    "        M = X_na_train.shape[2]\n",
    "        X_na_cat_train = X_na_train\n",
    "        X_na_cat_test = X_na_test\n",
    "\n",
    "        conc_train = []\n",
    "        conc_test = []\n",
    "        bias = []\n",
    "\n",
    "        for i in range(M):\n",
    "            X_na_cat_train[:, :, i] = self._decode_categorical(X_na_cat_train[:, :, i])\n",
    "            cat_train = pd.DataFrame(X_na_cat_train[:, :, i], columns = self.categorical_colnames)\n",
    "            df_train = cat_train\n",
    "            df_train = self._add_event_and_duration(df_train, event_and_duration_train[:, 0, i], event_and_duration_train[:, 1, i])\n",
    "            df_train = df_train.dropna()\n",
    "\n",
    "            X_na_cat_test[:, :, i] = self._decode_categorical(X_na_cat_test[:, :, i])\n",
    "            cat_test = pd.DataFrame(X_na_cat_test[:, :, i], columns = self.categorical_colnames)\n",
    "            df_test = cat_test\n",
    "            df_test = self._add_event_and_duration(df_test, event_and_duration_test[:, 0, i], event_and_duration_test[:, 1, i])\n",
    "            df_test = df_test.dropna()\n",
    "\n",
    "            weights, c_index_train, c_index_test = self._fit_single_cox_PH(df_train, df_test).values()\n",
    "            abs_bias = self._evaluate_bias_weights(weights)\n",
    "\n",
    "            conc_train.append(c_index_train)\n",
    "            conc_test.append(c_index_test)\n",
    "            bias.append(abs_bias)\n",
    "\n",
    "        # Take the average and std\n",
    "        conc_avg_train = np.mean(conc_train)\n",
    "        conc_std_train = np.std(conc_train)\n",
    "        c_index_train = (conc_avg_train.round(4), conc_std_train.round(4))\n",
    "\n",
    "        conc_avg_test = np.mean(conc_test)\n",
    "        conc_std_test = np.std(conc_test)\n",
    "        c_index_test = (conc_avg_test.round(4), conc_std_test.round(4))\n",
    "        \n",
    "        bias_column = pd.DataFrame(bias)\n",
    "        mean_and_std = dict(zip(bias_column.columns, zip(round(bias_column.mean(),4), round(bias_column.std(), 4))))\n",
    "        bias = mean_and_std\n",
    "\n",
    "        return {'c_index_train': c_index_train, 'c_index_test': c_index_test, 'bias': bias}\n",
    "    \n",
    "    def _run_CCA(self, M, p_miss, mecha, p_obs = None):\n",
    "        \"\"\" \n",
    "        Run CCA on the imputed datasets.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        M : int\n",
    "            Number of datasets to generate.\n",
    "\n",
    "        p_miss : float\n",
    "            Proportion of missing values to generate for variables which will have missing values.\n",
    "\n",
    "        mecha : str,\n",
    "                Indicates the missing-data mechanism to be used. \"MCAR\" by default, \"MAR\" or \"MNAR\".\n",
    "        \n",
    "        p_obs : float\n",
    "            Proportion of variables to retain if the mechanism is MAR.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        dict\n",
    "            Dictionary containing the concordance index (train and test) and bias for the imputed datasets.\n",
    "        \"\"\"\n",
    "\n",
    "        X_init, X_na, X_mask, event_and_duration = self._simulate_M_na_datasets(M = M,\n",
    "                                                            p_miss = p_miss,\n",
    "                                                            p_obs = p_obs,\n",
    "                                                            mecha = mecha).values()\n",
    "\n",
    "        X_na_train = X_na['train']        \n",
    "        X_na_train = X_na_train.numpy()\n",
    "        X_na_train = X_na_train.astype(object)\n",
    "        event_and_duration_train = event_and_duration['train']\n",
    "\n",
    "        X_na_test = X_na['test']\n",
    "        X_na_test = X_na_test.numpy()\n",
    "        X_na_test = X_na_test.astype(object)\n",
    "        event_and_duration_test = event_and_duration['test']\n",
    "        \n",
    "        c_index_train, c_index_test, bias = self._CCA(X_na_train, X_na_test, event_and_duration_train, event_and_duration_test).values()\n",
    "        train = {'bias': bias, 'c_index': c_index_train}\n",
    "        test = {'bias': bias, 'c_index': c_index_test}\n",
    "        return {'train': train, 'test': test}\n",
    "           \n",
    "class SimulationMCAR(SimulationMV):\n",
    "    \"\"\"\n",
    "    Subclass of SimulationMV to simulate MCAR missing values.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, X, duration_col, event_col, cat_colnames):\n",
    "        super().__init__(X, duration_col, event_col, cat_colnames)\n",
    "    \n",
    "    def _simulate_MCAR_dataset(self, M, p_miss, column_wise = False,\n",
    "                        sample_seed = 135135, column_seed = 115342):\n",
    "            \"\"\"\n",
    "            Simulate M datasets with MCAR missing values. See parent class for more details.\n",
    "            \"\"\"\n",
    "\n",
    "            X_init, X_na, X_mask, event_and_duration = self._simulate_M_na_datasets(M = M,\n",
    "                                                            p_miss = p_miss,\n",
    "                                                            mecha = \"MCAR\",\n",
    "                                                            sample_seed = sample_seed,\n",
    "                                                            column_seed = column_seed).values()\n",
    "            \n",
    "            \n",
    "            return {'X_init': X_init, 'X_na': X_na, 'mask': X_mask, 'event_and_duration': event_and_duration}\n",
    "    \n",
    "    def _run_univariate_imputations(self, M, p_miss):\n",
    "            \"\"\" \n",
    "            Simulate M datasets of missing values and impute them using univariate imputation methods.\n",
    "\n",
    "            Parameters\n",
    "            ----------\n",
    "            M : int\n",
    "                Number of datasets to generate.\n",
    "            \n",
    "            p_miss : float\n",
    "                Proportion of missing values to generate for variables which will have missing values.\n",
    "\n",
    "            Returns\n",
    "            -------\n",
    "            dict\n",
    "                Dictionary containing train and test accuacy, concordance index and bias of the imputed datasets.\n",
    "            \"\"\"\n",
    "            \n",
    "            X_init, X_na, X_mask, event_and_duration = self._simulate_M_na_datasets(M = M,\n",
    "                                                            p_miss = p_miss,\n",
    "                                                            mecha = 'MCAR').values()\n",
    "            \n",
    "            X_init_train = X_init['train']\n",
    "            X_na_train = X_na['train']\n",
    "            X_mask_train = X_mask['train']\n",
    "            event_and_duration_train = event_and_duration['train']           \n",
    "\n",
    "            X_init_test = X_init['test']\n",
    "            X_na_test = X_na['test']\n",
    "            X_mask_test = X_mask['test']\n",
    "            event_and_duration_test = event_and_duration['test']\n",
    "\n",
    "\n",
    "            # impute data\n",
    "            imputations_train, imputation_test = self._impute_baselines(X_na_train, X_na_test).values()\n",
    "\n",
    "            # Get accuracy\n",
    "            acc_dict_train = self._evaluate_univariate_acc(X_imp = imputations_train,\n",
    "                                        X_true = X_init_train,\n",
    "                                        X_mask = X_mask_train) \n",
    "            acc_dict_test = self._evaluate_univariate_acc(X_imp = imputation_test,\n",
    "                                        X_true = X_init_test,\n",
    "                                        X_mask = X_mask_test)      \n",
    "\n",
    "            # Map integer representation back to categories\n",
    "            mode_imp_train = imputations_train[\"categorical\"][\"mode\"]\n",
    "            mode_imp_train = mode_imp_train.numpy()\n",
    "            mode_imp_train = mode_imp_train.astype(\"object\")\n",
    "\n",
    "            mode_imp_test = imputation_test[\"categorical\"][\"mode\"]\n",
    "            mode_imp_test = mode_imp_test.numpy()\n",
    "            mode_imp_test = mode_imp_test.astype(\"object\")\n",
    "\n",
    "            for i in range(M):\n",
    "                mode_imp_train[:, :, i] = self._decode_categorical(mode_imp_train[:, :, i])\n",
    "                mode_imp_test[:, :, i] = self._decode_categorical(mode_imp_test[:, :, i])\n",
    "            \n",
    "\n",
    "            decoded_imps_train = {\"categorical\": {\"mode\": mode_imp_train}}\n",
    "            decoded_imps_test = {\"categorical\": {\"mode\": mode_imp_test}}\n",
    "            \n",
    "            c_index_train, c_index_test, bias = self._evaluate_univariate_coxPH(decoded_imps_train, decoded_imps_test, event_and_duration_train, event_and_duration_test).values()\n",
    "            \n",
    "            train = {'acc': acc_dict_train, 'c_index': c_index_train, 'bias': bias}\n",
    "            test = {'acc': acc_dict_test, 'c_index': c_index_test, 'bias': bias}\n",
    "            \n",
    "            return {'train': train, 'test': test}\n",
    "    \n",
    "    def _run_multivariate_imputations(self, M, p_miss, n_neighbors = 2):\n",
    "        \"\"\"\n",
    "        Simulate M datasets of missing values and impute them using multivariate imputation methods.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        M : int\n",
    "            Number of datasets to generate.\n",
    "        \n",
    "        p_miss : float\n",
    "            Proportion of missing values to generate for variables which will have missing values.\n",
    "        \n",
    "        n_neighbors : int\n",
    "            Number of neighbors to use for KNN imputation.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        dict\n",
    "            Dictionary containing train and test accuracy, concordance index and bias of the imputed datasets.\n",
    "        \"\"\"\n",
    "        X_init, X_na, X_mask, event_and_duration = self._simulate_M_na_datasets(M = M,\n",
    "                                                            p_miss = p_miss,\n",
    "                                                            mecha = 'MCAR').values()\n",
    "\n",
    "        X_init_train = X_init['train']\n",
    "        X_na_train = X_na['train']\n",
    "        X_mask_train = X_mask['train']\n",
    "        event_and_duration_train = event_and_duration['train']           \n",
    "\n",
    "        X_init_test = X_init['test']\n",
    "        X_na_test = X_na['test']\n",
    "        X_mask_test = X_mask['test']\n",
    "        event_and_duration_test = event_and_duration['test']\n",
    "\n",
    "        # impute data\n",
    "        imputations_train, imputation_test = self._impute_baselines(X_na_train, X_na_test, n_neighbors=n_neighbors, multivariate = True).values()\n",
    "\n",
    "        # Get accuracy\n",
    "        acc_dict_train = self._evaluate_multivariate_acc(X_imp = imputations_train,\n",
    "                                    X_true = X_init_train,\n",
    "                                    X_mask = X_mask_train) \n",
    "        acc_dict_test = self._evaluate_multivariate_acc(X_imp = imputation_test,\n",
    "                                    X_true = X_init_test,\n",
    "                                    X_mask = X_mask_test)    \n",
    "        \n",
    "        # decode the categorical variables\n",
    "        X_imp_train = imputations_train[\"knn\"]\n",
    "        X_imp_cat_train = X_imp_train\n",
    "        X_imp_cat_train = X_imp_cat_train.numpy()\n",
    "        X_imp_cat_train = X_imp_cat_train.astype(object)\n",
    "\n",
    "        X_imp_test = imputation_test[\"knn\"]\n",
    "        X_imp_cat_test = X_imp_test\n",
    "        X_imp_cat_test = X_imp_cat_test.numpy()\n",
    "        X_imp_cat_test = X_imp_cat_test.astype(object)\n",
    "\n",
    "        for i in range(M):\n",
    "            X_imp_cat_train[:, :, i] = self._decode_categorical(X_imp_cat_train[:, :, i])\n",
    "            X_imp_cat_test[:, :, i] = self._decode_categorical(X_imp_cat_test[:, :, i])\n",
    "                    \n",
    "        decoded_imps_train = {\"categorical\": {\"knn\": X_imp_cat_train}}\n",
    "        decoded_imps_test = {\"categorical\": {\"knn\": X_imp_cat_test}}\n",
    "\n",
    "        c_index_train, c_index_test, bias = self._evaluate_multivariate_coxPH(decoded_imps_train, decoded_imps_test, event_and_duration_train, event_and_duration_test).values()\n",
    "    \n",
    "        train = {'acc': acc_dict_train, 'c_index': c_index_train, 'bias': bias}\n",
    "        test = {'acc': acc_dict_test, 'c_index': c_index_test, 'bias': bias}\n",
    "        return {'train': train, 'test': test}                                                  \n",
    "        \n",
    "    def _get_univariate_acc(self, acc_in):\n",
    "        \"\"\" \n",
    "        Get the univariate accuracy for the imputed datasets.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        acc_in : dict\n",
    "            Dictionary containing the accuracy for the imputed datasets.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        pd.DataFrame\n",
    "            DataFrame containing the accuracy for the imputed datasets of each p_miss.\n",
    "        \"\"\"\n",
    "\n",
    "        acc_cols = pd.MultiIndex.from_tuples([\n",
    "                        ('Categorical', 'Mode')\n",
    "                        ])\n",
    "        acc_row = []\n",
    "\n",
    "        for miss in self.p_miss:\n",
    "            acc = acc_in[self.p_miss.index(miss)]\n",
    "\n",
    "            mode_mean = acc['categorical']['mode'][0]\n",
    "            mode_std = acc['categorical']['mode'][1]\n",
    "\n",
    "            row = [f\"{mode_mean:.4f}  {mode_std:.4f}\"]\n",
    "            acc_row.append(row)\n",
    "        \n",
    "        acc_out = pd.DataFrame(acc_row, index = self.p_miss , columns = acc_cols)\n",
    "        acc_out.index.name = 'p_miss'\n",
    "        return acc_out\n",
    "    \n",
    "    def _get_multivariate_acc(self, acc_in):\n",
    "        \"\"\"\n",
    "        Get the multivariate accuracy for the imputed datasets.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        acc_in : dict\n",
    "            Dictionary containing the accuracy for the imputed datasets.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        pd.DataFrame\n",
    "            DataFrame containing the accuracy for the imputed datasets of each p_miss.\n",
    "        \"\"\"\n",
    "\n",
    "        acc_cols = pd.MultiIndex.from_tuples([\n",
    "                        ('Categorical', 'KNN')\n",
    "                        ])\n",
    "        acc_row = []\n",
    "\n",
    "        for miss in self.p_miss:\n",
    "            acc = acc_in[self.p_miss.index(miss)]\n",
    "                    \n",
    "            knn_mean_cat = acc['categorical']['knn'][0]\n",
    "            knn_std_cat = acc['categorical']['knn'][1]\n",
    "\n",
    "            row = [f\"{knn_mean_cat:.4f}  {knn_std_cat:.4f}\"]\n",
    "            acc_row.append(row)\n",
    "        \n",
    "        acc_out = pd.DataFrame(acc_row, index = self.p_miss , columns = acc_cols)\n",
    "        acc_out.index.name = 'p_miss'\n",
    "        return acc_out\n",
    "\n",
    "    def _get_univariate_conc(self, conc_in):\n",
    "        \"\"\"\n",
    "        Get the univariate concordance index for the imputed datasets.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        conc_in : dict\n",
    "            Dictionary containing the concordance index for the imputed datasets.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        pd.DataFrame\n",
    "            DataFrame containing the concordance index for the imputed datasets of each p_miss.\n",
    "        \"\"\"\n",
    "\n",
    "        conc_cols = ['mode']\n",
    "        conc_row = []\n",
    "\n",
    "        for miss in self.p_miss:\n",
    "            conc = conc_in[self.p_miss.index(miss)]\n",
    " \n",
    "            mode_mean = conc['mode'][0]\n",
    "            mode_std = conc['mode'][1]\n",
    "\n",
    "            row = [f\"{mode_mean:.4f}  {mode_std:.4f}\"]\n",
    "            conc_row.append(row)\n",
    "\n",
    "        conc_out = pd.DataFrame(conc_row, index = self.p_miss , columns = conc_cols)\n",
    "        conc_out.index.name = 'p_miss'\n",
    "        return conc_out\n",
    "\n",
    "    def _get_multivariate_conc(self, conc_in):\n",
    "        \"\"\"\n",
    "        Get the multivariate concordance index for the imputed datasets.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        conc_in : dict\n",
    "            Dictionary containing the concordance index for the imputed datasets.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        pd.DataFrame\n",
    "            DataFrame containing the concordance index for the imputed datasets of each p_miss.\n",
    "        \"\"\"\n",
    "\n",
    "        column = ['KNN']\n",
    "        conc_row = []\n",
    "\n",
    "        for miss in self.p_miss:\n",
    "            conc = conc_in[self.p_miss.index(miss)]\n",
    "            knn_mean = conc['knn'][0]\n",
    "            knn_std = conc['knn'][1]\n",
    "            row = [f\"{knn_mean}  {knn_std}\"]\n",
    "            conc_row.append(row)\n",
    "\n",
    "        conc_out = pd.DataFrame(conc_row, index = self.p_miss , columns = column)\n",
    "        conc_out.index.name = 'p_miss'\n",
    "        return conc_out\n",
    "\n",
    "    def _get_univariate_bias(self):\n",
    "        \"\"\"\n",
    "        Get the univariate bias for the imputed datasets.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        pd.DataFrame\n",
    "            DataFrame containing the bias for the imputed datasets of each p_miss.\n",
    "        \"\"\"\n",
    "\n",
    "        feature_cols = list(self.bias[0]['mode'].keys())\n",
    "        bias_cols = pd.MultiIndex.from_tuples([\n",
    "            ('mode', col) for col in feature_cols\n",
    "        ])\n",
    "        bias_row = []\n",
    "    \n",
    "        for miss in self.p_miss:\n",
    "            mode_bias = list(self.bias[self.p_miss.index(miss)]['mode'].values())\n",
    "\n",
    "            mode_row = []\n",
    "\n",
    "            for column_index in range(len(mode_bias)):\n",
    "                mode_bias_mean = mode_bias[column_index][0]\n",
    "                mode_bias_std = mode_bias[column_index][1]\n",
    "\n",
    "                mode_row.append(f\"{mode_bias_mean:.4f}  {mode_bias_std:.4f}\")\n",
    "                \n",
    "            row = mode_row\n",
    "            \n",
    "            bias_row.append(row)\n",
    "\n",
    "        bias = pd.DataFrame(bias_row, index = self.p_miss , columns=bias_cols)\n",
    "        bias.index.name = 'p_miss'\n",
    "\n",
    "        return bias\n",
    "\n",
    "    def _get_multivariate_bias(self):\n",
    "        \"\"\"\n",
    "        Get the multivariate bias for the imputed datasets.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        pd.DataFrame\n",
    "            DataFrame containing the bias for the imputed datasets of each p_miss.\n",
    "        \"\"\"\n",
    "\n",
    "        feature_cols = list(self.bias[0]['knn'].keys())\n",
    "        bias_row = []\n",
    "\n",
    "        for miss in self.p_miss:\n",
    "            knn_bias = list(self.bias[self.p_miss.index(miss)]['knn'].values())\n",
    "\n",
    "            knn_row = []\n",
    "            for column_index in range(len(knn_bias)):\n",
    "                knn_bias_mean = knn_bias[column_index][0]\n",
    "                knn_bias_std = knn_bias[column_index][1]\n",
    "                knn_row.append(f\"{knn_bias_mean}  {knn_bias_std}\")\n",
    "\n",
    "            row = knn_row\n",
    "            bias_row.append(row)\n",
    "\n",
    "        bias = pd.DataFrame(bias_row, index = self.p_miss , columns = feature_cols)\n",
    "        bias.index.name = 'p_miss'\n",
    "        return bias\n",
    "    \n",
    "    def _get_cca_conc(self, conc_in):\n",
    "        \"\"\"\n",
    "        Get the concordance index for the CCA datasets.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        conc_in : dict\n",
    "            Dictionary containing the concordance index for the CCA datasets.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        pd.DataFrame\n",
    "            DataFrame containing the concordance index for the CCA datasets of each p_miss.\n",
    "        \"\"\"\n",
    "\n",
    "        column = ['CCA']\n",
    "        conc_row = []\n",
    "\n",
    "        for miss in self.p_miss:\n",
    "            conc = conc_in[self.p_miss.index(miss)]\n",
    "            mean = conc[0]\n",
    "            std = conc[1]\n",
    "\n",
    "            row = [f\"{mean:.4f}  {std:.4f}\"]\n",
    "            conc_row.append(row)\n",
    "\n",
    "        conc_out = pd.DataFrame(conc_row, index = self.p_miss , columns = column)\n",
    "        conc_out.index.name = 'p_miss'\n",
    "        return conc_out\n",
    "\n",
    "    def _get_cca_bias(self):\n",
    "        \"\"\"\n",
    "        Get the bias for the CCA datasets.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        pd.DataFrame\n",
    "            DataFrame containing the bias for the CCA datasets of each p_miss.\n",
    "        \"\"\"\n",
    "\n",
    "        feature_cols = list(self.bias[0].keys())\n",
    "        complete_row = []\n",
    "\n",
    "        for miss in self.p_miss:\n",
    "            bias = list(self.bias[self.p_miss.index(miss)].values())\n",
    "            column_row = []\n",
    "\n",
    "            for column_index in range(len(bias)):\n",
    "                mean = bias[column_index][0]\n",
    "                std = bias[column_index][1]\n",
    "                bias_row = f\"{mean}  {std}\"\n",
    "                column_row.append(bias_row)\n",
    "            complete_row.append(column_row)\n",
    "            \n",
    "        bias = pd.DataFrame(complete_row, index = self.p_miss , columns = feature_cols)\n",
    "        bias.index.name = 'p_miss'\n",
    "        return bias\n",
    "\n",
    "    def simulate_univariate_imputations(self, M, p_miss = [0.1, 0.2, 0.3, 0.4, 0.5]):\n",
    "        \"\"\"\n",
    "        Run a simulation with univariate imputations.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        M : int\n",
    "            Number of datasets to generate.\n",
    "\n",
    "        p_miss : list\n",
    "            List of proportions of missing values to generate for variables which will have missing values.\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        self.p_miss = p_miss\n",
    "\n",
    "        self.acc_train = []\n",
    "        self.acc_test = []\n",
    "        self.conc_train = []\n",
    "        self.conc_test = []\n",
    "        self.bias = []\n",
    "\n",
    "        # Get mean and std of each p_miss\n",
    "        for miss in p_miss:\n",
    "            print(f\"Currently simulating for p_miss: {miss}\")\n",
    "            train, test = self._run_univariate_imputations(M = M, p_miss = miss).values()\n",
    "\n",
    "            acc_train = train['acc']\n",
    "            conc_train = train['c_index']\n",
    "            acc_test = test['acc']\n",
    "            conc_test = test['c_index']\n",
    "            bias = train['bias']\n",
    "                    \n",
    "            self.acc_train.append(acc_train)\n",
    "            self.acc_test.append(acc_test)\n",
    "            self.conc_train.append(conc_train)\n",
    "            self.conc_test.append(conc_test)\n",
    "            self.bias.append(bias)\n",
    "\n",
    "    def get_univariate_imputed_results(self):\n",
    "        \"\"\"\n",
    "        Get the results of the univariate imputation simulation.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        dict\n",
    "            A dictionary containing the accuracy, concordance index and bias for the univariate imputation simulation.\n",
    "            The dictionary values are dataframes.\n",
    "        \"\"\"\n",
    "\n",
    "        acc_train, acc_test = self._get_univariate_acc(self.acc_train), self._get_univariate_acc(self.acc_test)\n",
    "        conc_train, conc_test = self._get_univariate_conc(self.conc_train), self._get_univariate_conc(self.conc_test)\n",
    "        bias = self._get_univariate_bias()\n",
    "\n",
    "        return {'acc_train': acc_train, 'acc_test': acc_test, 'c_index_train': conc_train, 'c_index_test': conc_test, 'bias': bias}\n",
    "    \n",
    "    def simulate_multivariate_imputations(self, M, p_miss = [0.1, 0.2, 0.3, 0.4, 0.5], n_neighbors = 2):\n",
    "        \"\"\"\n",
    "        Run a simulation with multivariate imputations.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        M : int\n",
    "            Number of datasets to generate.\n",
    "\n",
    "        p_miss : list\n",
    "            List of proportions of missing values to generate for variables which will have missing values.\n",
    "\n",
    "        n_neighbors : int\n",
    "            Number of neighbors to use for KNN imputation.\n",
    "        \"\"\"\n",
    "\n",
    "        self.p_miss = p_miss\n",
    "\n",
    "        self.acc_train = []\n",
    "        self.acc_test = []\n",
    "        self.conc_train = []\n",
    "        self.conc_test = []\n",
    "        self.bias = []\n",
    "\n",
    "        for miss in p_miss:\n",
    "            print(f\"Currently simulating for p_miss: {miss}\")\n",
    "            train, test = self._run_multivariate_imputations(M = M, p_miss = miss, n_neighbors = n_neighbors).values()\n",
    "\n",
    "            acc_train = train['acc']\n",
    "            conc_train = train['c_index']\n",
    "            acc_test = test['acc']\n",
    "            conc_test = test['c_index']\n",
    "            bias = train['bias']\n",
    "                    \n",
    "            self.acc_train.append(acc_train)\n",
    "            self.acc_test.append(acc_test)\n",
    "            self.conc_train.append(conc_train)\n",
    "            self.conc_test.append(conc_test)\n",
    "            self.bias.append(bias)\n",
    "\n",
    "    def get_multivariate_imputed_results(self):\n",
    "        \"\"\" \n",
    "        Get the results of the multivariate imputation simulation.\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        dict\n",
    "            A dictionary containing the accuracy, concordance index and bias for the multivariate imputation simulation.\n",
    "            The dictionary values are dataframes.\n",
    "        \"\"\"\n",
    "        acc_train, acc_test = self._get_multivariate_acc(self.acc_train), self._get_multivariate_acc(self.acc_test)\n",
    "        conc_train, conc_test = self._get_multivariate_conc(self.conc_train), self._get_multivariate_conc(self.conc_test)\n",
    "        bias = self._get_multivariate_bias()\n",
    "\n",
    "        return {'acc_train': acc_train, 'acc_test': acc_test, 'c_index_train': conc_train, 'c_index_test': conc_test, 'bias': bias}\n",
    "            \n",
    "    def simulate_cca(self, M, p_miss = [0.1, 0.2, 0.3]):\n",
    "        \"\"\"\n",
    "        Run a simulation with CCA.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        M : int\n",
    "            Number of datasets to generate.\n",
    "\n",
    "        p_miss : list\n",
    "            List of proportions of missing values to generate for variables which will have missing values.\n",
    "        \"\"\"\n",
    "\n",
    "        self.p_miss = p_miss\n",
    "\n",
    "        self.conc_train = []\n",
    "        self.conc_test = []\n",
    "        self.bias = []\n",
    "\n",
    "        for miss in p_miss:\n",
    "            print(f\"Currently simulating for p_miss: {miss}\")\n",
    "            train, test = self._run_CCA(M = M, p_miss = miss, mecha = 'MCAR').values()\n",
    "            conc_train = train['c_index']\n",
    "            conc_test = test['c_index']\n",
    "            bias = train['bias']\n",
    "            \n",
    "            self.conc_train.append(conc_train)\n",
    "            self.conc_test.append(conc_test)\n",
    "            self.bias.append(bias)\n",
    "\n",
    "    def get_cca_results(self):\n",
    "        \"\"\"\n",
    "        Get the results of the CCA simulation.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        dict\n",
    "            A dictionary containing the concordance index and the bias for the CCA simulation.\n",
    "            The dictionary values are dataframes.        \n",
    "        \"\"\"\n",
    "        \n",
    "        conc_train, conc_test = self._get_cca_conc(self.conc_train), self._get_cca_conc(self.conc_test)\n",
    "        bias = self._get_cca_bias()\n",
    "\n",
    "        return {'c_index_train': conc_train, 'c_index_test': conc_test, 'bias': bias}\n",
    "\n",
    "class SimulationMAR(SimulationMV):\n",
    "    \"\"\"\n",
    "    Subclass of SimulationMV to simulate MAR missing values.\n",
    "    \"\"\"\n",
    "    def __init__(self, X, duration_col, event_col, cat_colnames):\n",
    "        super().__init__(X, duration_col, event_col, cat_colnames)\n",
    "    \n",
    "    def _simulate_MAR_dataset(self, M, p_miss, p_obs, column_wise = False,\n",
    "                        sample_seed = 135135, column_seed = 115342):\n",
    "            \"\"\"\n",
    "            Simulate M datasets with MAR missing values.\n",
    "            See parent class for more details.\n",
    "            \"\"\"\n",
    "\n",
    "            X_init, X_na, X_mask, event_and_duration = self._simulate_M_na_datasets(M = M,\n",
    "                                                            p_miss = p_miss,\n",
    "                                                            mecha = \"MAR\",\n",
    "                                                            p_obs = p_obs,\n",
    "                                                            sample_seed = sample_seed,\n",
    "                                                            column_seed = column_seed).values()           \n",
    "            \n",
    "            return {'X_init': X_init, 'X_na': X_na, 'mask': X_mask, 'event_and_duration': event_and_duration}\n",
    "    \n",
    "    def _run_univariate_imputations(self, M, p_miss, p_obs):\n",
    "            \"\"\" \n",
    "            Simulate M datasets of missing values and impute them using univariate imputation methods.\n",
    "\n",
    "            Parameters\n",
    "            ----------\n",
    "            M : int\n",
    "                Number of datasets to generate.\n",
    "            \n",
    "            p_miss : float\n",
    "                Proportion of missing values to generate for variables which will have missing values.\n",
    "\n",
    "            p_obs : float\n",
    "                Proportion of variables to not contain missing values.\n",
    "\n",
    "            Returns\n",
    "            -------\n",
    "            dict\n",
    "                Dictionary containing train and test accuracy, concordance index and bias of the imputed datasets.\n",
    "            \"\"\"\n",
    "\n",
    "            X_init, X_na, X_mask, event_and_duration = self._simulate_M_na_datasets(M = M,\n",
    "                                                            p_miss = p_miss,\n",
    "                                                            mecha = 'MAR',\n",
    "                                                            p_obs = p_obs).values()\n",
    "\n",
    "            X_init_train = X_init['train']\n",
    "            X_na_train = X_na['train']\n",
    "            X_mask_train = X_mask['train']\n",
    "            event_and_duration_train = event_and_duration['train']           \n",
    "\n",
    "            X_init_test = X_init['test']\n",
    "            X_na_test = X_na['test']\n",
    "            X_mask_test = X_mask['test']\n",
    "            event_and_duration_test = event_and_duration['test']\n",
    "\n",
    "            # impute data\n",
    "            imputations_train, imputation_test = self._impute_baselines(X_na_train, X_na_test).values()\n",
    "\n",
    "            # Get accuracy\n",
    "            acc_dict_train = self._evaluate_univariate_acc(X_imp = imputations_train,\n",
    "                                        X_true = X_init_train,\n",
    "                                        X_mask = X_mask_train) \n",
    "            acc_dict_test = self._evaluate_univariate_acc(X_imp = imputation_test,\n",
    "                                        X_true = X_init_test,\n",
    "                                        X_mask = X_mask_test)            \n",
    "\n",
    "\n",
    "            # Map integer representation back to categories\n",
    "            mode_imp_train = imputations_train[\"categorical\"][\"mode\"]\n",
    "            mode_imp_train = mode_imp_train.numpy()\n",
    "            mode_imp_train = mode_imp_train.astype(\"object\")\n",
    "\n",
    "            mode_imp_test = imputation_test[\"categorical\"][\"mode\"]\n",
    "            mode_imp_test = mode_imp_test.numpy()\n",
    "            mode_imp_test = mode_imp_test.astype(\"object\")\n",
    "\n",
    "            for i in range(M):\n",
    "                mode_imp_train[:, :, i] = self._decode_categorical(mode_imp_train[:, :, i])\n",
    "                mode_imp_test[:, :, i] = self._decode_categorical(mode_imp_test[:, :, i])\n",
    "            \n",
    "\n",
    "            decoded_imps_train = {\"categorical\": {\"mode\": mode_imp_train}}\n",
    "            decoded_imps_test = {\"categorical\": {\"mode\": mode_imp_test}}\n",
    "            \n",
    "            c_index_train, c_index_test, bias = self._evaluate_univariate_coxPH(decoded_imps_train, decoded_imps_test, event_and_duration_train, event_and_duration_test).values()\n",
    "            \n",
    "            train = {'acc': acc_dict_train, 'c_index': c_index_train, 'bias': bias}\n",
    "            test = {'acc': acc_dict_test, 'c_index': c_index_test, 'bias': bias}\n",
    "            \n",
    "            return {'train': train, 'test': test}\n",
    "    \n",
    "    def _run_multivariate_imputations(self, M, p_miss, p_obs, n_neighbors = 2):\n",
    "        \"\"\"\n",
    "        Simulate M datasets of missing values and impute them using multivariate imputation methods.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        M : int\n",
    "            Number of datasets to generate.\n",
    "\n",
    "        p_miss : float\n",
    "            Proportion of missing values to generate for variables which will have missing values.\n",
    "\n",
    "        p_obs : float\n",
    "            Proportion of variables to retain if the mechanism is MAR.\n",
    "\n",
    "        n_neighbors : int\n",
    "            Number of neighbors to use for KNN imputation.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        dict\n",
    "            Dictionary containing train and test accuracy, concordance index and bias of the imputed datasets.\n",
    "        \"\"\"\n",
    "\n",
    "        X_init, X_na, X_mask, event_and_duration = self._simulate_M_na_datasets(M = M,\n",
    "                                                            p_miss = p_miss,\n",
    "                                                            mecha = 'MAR',\n",
    "                                                            p_obs = p_obs).values()\n",
    "        X_init_train = X_init['train']\n",
    "        X_na_train = X_na['train']\n",
    "        X_mask_train = X_mask['train']\n",
    "        event_and_duration_train = event_and_duration['train']           \n",
    "\n",
    "        X_init_test = X_init['test']\n",
    "        X_na_test = X_na['test']\n",
    "        X_mask_test = X_mask['test']\n",
    "        event_and_duration_test = event_and_duration['test']\n",
    "\n",
    "        # impute data\n",
    "        imputations_train, imputation_test = self._impute_baselines(X_na_train, X_na_test, n_neighbors=n_neighbors, multivariate = True).values()\n",
    "\n",
    "        # Get accuracy\n",
    "        acc_dict_train = self._evaluate_multivariate_acc(X_imp = imputations_train,\n",
    "                                    X_true = X_init_train,\n",
    "                                    X_mask = X_mask_train) \n",
    "        acc_dict_test = self._evaluate_multivariate_acc(X_imp = imputation_test,\n",
    "                                    X_true = X_init_test,\n",
    "                                    X_mask = X_mask_test)    \n",
    "        \n",
    "        # decode the categorical variables\n",
    "        X_imp_train = imputations_train[\"knn\"]\n",
    "        X_imp_cat_train = X_imp_train\n",
    "        X_imp_cat_train = X_imp_cat_train.numpy()\n",
    "        X_imp_cat_train = X_imp_cat_train.astype(object)\n",
    "\n",
    "        X_imp_test = imputation_test[\"knn\"]\n",
    "        X_imp_cat_test = X_imp_test\n",
    "        X_imp_cat_test = X_imp_cat_test.numpy()\n",
    "        X_imp_cat_test = X_imp_cat_test.astype(object)\n",
    "\n",
    "        for i in range(M):\n",
    "            X_imp_cat_train[:, :, i] = self._decode_categorical(X_imp_cat_train[:, :, i])\n",
    "            X_imp_cat_test[:, :, i] = self._decode_categorical(X_imp_cat_test[:, :, i])\n",
    "                    \n",
    "        decoded_imps_train = {\"categorical\": {\"knn\": X_imp_cat_train}}\n",
    "        decoded_imps_test = {\"categorical\": {\"knn\": X_imp_cat_test}}\n",
    "\n",
    "        c_index_train, c_index_test, bias = self._evaluate_multivariate_coxPH(decoded_imps_train, decoded_imps_test, event_and_duration_train, event_and_duration_test).values()\n",
    "    \n",
    "        train = {'acc': acc_dict_train, 'c_index': c_index_train, 'bias': bias}\n",
    "        test = {'acc': acc_dict_test, 'c_index': c_index_test, 'bias': bias}\n",
    "        return {'train': train, 'test': test}     \n",
    "    \n",
    "\n",
    "    def _get_univariate_acc(self, acc_in):\n",
    "        \"\"\"\n",
    "        Get the univariate accuracy for the imputed datasets.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        acc_in : dict\n",
    "            Dictionary containing the accuracy for the imputed datasets.\n",
    "            \n",
    "        Returns\n",
    "        -------\n",
    "        pd.DataFrame\n",
    "            DataFrame containing the accuracy for the imputed datasets for each p_miss and p_obs.\n",
    "        \"\"\"\n",
    "\n",
    "        acc_cols = pd.MultiIndex.from_tuples([\n",
    "                ('Categorical', 'Mode')\n",
    "                ])\n",
    "        index = ((miss, obs) for miss in self.p_miss for obs in self.p_obs)\n",
    "        row_index = pd.MultiIndex.from_tuples(index, names=['p_miss', 'p_obs'])\n",
    "        acc_row = []\n",
    "\n",
    "        for miss in self.p_miss:\n",
    "            for obs in self.p_obs:\n",
    "                acc = acc_in[self.p_miss.index(miss), self.p_obs.index(obs)]\n",
    "\n",
    "                mode_mean = acc['categorical']['mode'][0]\n",
    "                mode_std = acc['categorical']['mode'][1]\n",
    "                row = [f\"{mode_mean}  {mode_std}\"]\n",
    "\n",
    "                acc_row.append(row)\n",
    "\n",
    "        return pd.DataFrame(acc_row, index = row_index, columns = acc_cols)\n",
    "    \n",
    "    def _get_multivariate_acc(self, acc_in):\n",
    "        \"\"\"\n",
    "        Get the multivariate accuracy for the imputed datasets.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        acc_in : dict\n",
    "            Dictionary containing the accuracy for the imputed datasets.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        pd.DataFrame\n",
    "            DataFrame containing the accuracy for the imputed datasets for each p_miss and p_obs.\n",
    "        \"\"\"\n",
    "\n",
    "        acc_cols = pd.MultiIndex.from_tuples([\n",
    "                ('Categorical', 'KNN')\n",
    "                ])\n",
    "        index = ((miss, obs) for miss in self.p_miss for obs in self.p_obs)\n",
    "        row_index = pd.MultiIndex.from_tuples(index, names=['p_miss', 'p_obs'])\n",
    "        acc_row = []\n",
    "\n",
    "        for miss in self.p_miss:\n",
    "            for obs in self.p_obs:\n",
    "                acc = acc_in[self.p_miss.index(miss), self.p_obs.index(obs)]\n",
    "\n",
    "                knn_mean_cat = acc['categorical']['knn'][0]\n",
    "                knn_std_cat = acc['categorical']['knn'][1]\n",
    "\n",
    "                row = [f\"{knn_mean_cat}  {knn_std_cat}\"]\n",
    "                acc_row.append(row)\n",
    "\n",
    "        return pd.DataFrame(acc_row, index = row_index, columns = acc_cols)\n",
    "\n",
    "    def _get_univariate_conc(self, conc_in):\n",
    "        \"\"\"\n",
    "        Get the univariate concordance index for the imputed datasets.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        conc_in : dict\n",
    "            Dictionary containing the concordance index for the imputed datasets.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        pd.DataFrame\n",
    "            DataFrame containing the concordance index for the imputed datasets for each p_miss and p_obs.\n",
    "        \"\"\"\n",
    "\n",
    "        conc_cols = ['mode']\n",
    "        index = ((miss, obs) for miss in self.p_miss for obs in self.p_obs)\n",
    "        row_index = pd.MultiIndex.from_tuples(index, names=['p_miss', 'p_obs'])\n",
    "        conc_row = []\n",
    "\n",
    "        for miss in self.p_miss:\n",
    "            for obs in self.p_obs:\n",
    "                conc = conc_in[self.p_miss.index(miss), self.p_obs.index(obs)]\n",
    "\n",
    "                mode_mean = conc['mode'][0]\n",
    "                mode_std = conc['mode'][1]\n",
    "\n",
    "                row = [f\"{mode_mean}  {mode_std}\"]\n",
    "                conc_row.append(row)\n",
    "        return pd.DataFrame(conc_row, index = row_index, columns = conc_cols)\n",
    "\n",
    "    def _get_multivariate_conc(self, conc_in):\n",
    "        \"\"\"\n",
    "        Get the multivariate concordance index for the imputed datasets.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        conc_in : dict\n",
    "            Dictionary containing the concordance index for the imputed datasets.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        pd.DataFrame\n",
    "            DataFrame containing the concordance index for the imputed datasets for each p_miss and p_obs.\n",
    "        \"\"\"\n",
    "\n",
    "        column = ['KNN']\n",
    "        index = ((miss, obs) for miss in self.p_miss for obs in self.p_obs)\n",
    "        row_index = pd.MultiIndex.from_tuples(index, names=['p_miss', 'p_obs'])\n",
    "        conc_row = []\n",
    "\n",
    "        for miss in self.p_miss:\n",
    "            for obs in self.p_obs:\n",
    "                conc = conc_in[self.p_miss.index(miss), self.p_obs.index(obs)]\n",
    "                knn_mean = conc['knn'][0]\n",
    "                knn_std = conc['knn'][1]\n",
    "                row = [f\"{knn_mean}  {knn_std}\"]\n",
    "                conc_row.append(row)\n",
    "\n",
    "        return pd.DataFrame(conc_row, index = row_index, columns = column)\n",
    "\n",
    "    def _get_univariate_bias(self):\n",
    "        \"\"\"\n",
    "        Get the univariate bias for the imputed datasets.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        pd.DataFrame\n",
    "            DataFrame containing the bias for the imputed datasets for each p_miss and p_obs.\n",
    "        \"\"\"\n",
    "\n",
    "        feature_cols = list(self.bias[0][0]['mode'].keys())\n",
    "        bias_cols = pd.MultiIndex.from_tuples([\n",
    "            ('mode', col) for col in feature_cols\n",
    "        ])\n",
    "        index = ((miss, obs) for miss in self.p_miss for obs in self.p_obs)\n",
    "        row_index = pd.MultiIndex.from_tuples(index, names=['p_miss', 'p_obs'])\n",
    "        bias_row = []\n",
    "\n",
    "        for miss in self.p_miss:\n",
    "            for obs in self.p_obs:\n",
    "                mode_bias = list(self.bias[self.p_miss.index(miss), self.p_obs.index(obs)]['mode'].values())\n",
    "                mode_row = []\n",
    "                for column_index in range(len(mode_bias)):\n",
    "                    mode_bias_mean = mode_bias[column_index][0]\n",
    "                    mode_bias_std = mode_bias[column_index][1]\n",
    "\n",
    "                    mode_row.append(f\"{mode_bias_mean}  {mode_bias_std}\")\n",
    "                row = mode_row\n",
    "                bias_row.append(row)\n",
    "    \n",
    "        return pd.DataFrame(bias_row, index = row_index, columns = bias_cols)\n",
    "    \n",
    "    def _get_multivariate_bias(self):\n",
    "        \"\"\"\n",
    "        Get the multivariate bias for the imputed datasets.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        pd.DataFrame\n",
    "            DataFrame containing the bias for the imputed datasets for each p_miss and p_obs.\n",
    "        \"\"\"\n",
    "\n",
    "        feature_cols = list(self.bias[0][0]['knn'].keys())\n",
    "        index = ((miss, obs) for miss in self.p_miss for obs in self.p_obs)\n",
    "        row_index = pd.MultiIndex.from_tuples(index, names=['p_miss', 'p_obs'])\n",
    "        bias_row = []\n",
    "\n",
    "        for miss in self.p_miss:\n",
    "            for obs in self.p_obs:\n",
    "                knn_bias = list(self.bias[self.p_miss.index(miss), self.p_obs.index(obs)]['knn'].values())\n",
    "                knn_row = []\n",
    "                for column_index in range(len(knn_bias)):\n",
    "                    knn_bias_mean = knn_bias[column_index][0]\n",
    "                    knn_bias_std = knn_bias[column_index][1]\n",
    "                    knn_row.append(f\"{knn_bias_mean}  {knn_bias_std}\")\n",
    "                bias_row.append(knn_row)\n",
    "        return pd.DataFrame(bias_row, index = row_index, columns = feature_cols)\n",
    "    \n",
    "\n",
    "    def _get_cca_conc(self, conc_in):\n",
    "        \"\"\"\n",
    "        Get the concordance index for the CCA datasets.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        conc_in : dict\n",
    "            Dictionary containing the concordance index for the CCA datasets.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        pd.DataFrame\n",
    "            DataFrame containing the concordance index for the CCA datasets of each p_miss and p_obs.\n",
    "        \"\"\"\n",
    "\n",
    "        column = ['CCA']\n",
    "        index = ((miss, obs) for miss in self.p_miss for obs in self.p_obs)\n",
    "        row_index = pd.MultiIndex.from_tuples(index, names=['p_miss', 'p_obs'])\n",
    "        conc_row = []\n",
    "\n",
    "        for miss in self.p_miss:\n",
    "            for obs in self.p_obs:\n",
    "                conc = conc_in[self.p_miss.index(miss), self.p_obs.index(obs)]\n",
    "                mean = conc[0]\n",
    "                std = conc[1]\n",
    "\n",
    "                row = [f\"{mean}  {std}\"]\n",
    "                conc_row.append(row)\n",
    "\n",
    "        conc_out = pd.DataFrame(conc_row, index = row_index, columns = column)\n",
    "        conc_out.index.name = 'p_miss'\n",
    "        return conc_out\n",
    "\n",
    "    def _get_cca_bias(self):\n",
    "        \"\"\"\n",
    "        Get the bias for the CCA datasets.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        pd.DataFrame\n",
    "            DataFrame containing the bias for the CCA datasets of each p_miss and p_obs.\n",
    "        \"\"\"\n",
    "\n",
    "        feature_cols = list(self.bias[0][0].keys())\n",
    "        index = ((miss, obs) for miss in self.p_miss for obs in self.p_obs)\n",
    "        row_index = pd.MultiIndex.from_tuples(index, names=['p_miss', 'p_obs'])\n",
    "        complete_row = []\n",
    "\n",
    "        for miss in self.p_miss:\n",
    "            for obs in self.p_obs:\n",
    "                bias = list(self.bias[self.p_miss.index(miss), self.p_obs.index(obs)].values())\n",
    "\n",
    "                column_row = []  \n",
    "                for column_index in range(len(bias)):                \n",
    "                    mean = bias[column_index][0]\n",
    "                    std = bias[column_index][1]\n",
    "                    bias_row = f\"{mean}  {std}\"\n",
    "                    column_row.append(bias_row)\n",
    "\n",
    "                complete_row.append(column_row)\n",
    "\n",
    "        bias = pd.DataFrame(complete_row, index = row_index, columns = feature_cols)\n",
    "        bias.index.name = 'p_miss'\n",
    "        return bias\n",
    "    \n",
    "    def simulate_univariate_imputations(self, M, p_miss = [0.1, 0.2, 0.3, 0.4, 0.5], p_obs = [8/9, 7/9, 6/9, 5/9, 4/9]):\n",
    "        \"\"\"\n",
    "        Run a simulation with univariate imputations.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        M : int\n",
    "            Number of datasets to generate.\n",
    "\n",
    "        p_miss : list\n",
    "            List of proportions of missing values to generate for variables which will have missing values.\n",
    "\n",
    "        p_obs : list\n",
    "            List of proportions of variables to retain.\n",
    "        \"\"\"\n",
    "\n",
    "        self.p_miss = p_miss\n",
    "        self.p_obs = p_obs\n",
    "\n",
    "        self.acc_train = np.empty((len(p_miss), len(p_obs)), dtype=object)\n",
    "        self.acc_test = np.empty((len(p_miss), len(p_obs)), dtype=object)\n",
    "        self.conc_train = np.empty((len(p_miss), len(p_obs)), dtype=object)\n",
    "        self.conc_test = np.empty((len(p_miss), len(p_obs)), dtype=object)\n",
    "        self.bias = np.empty((len(p_miss), len(p_obs)), dtype=object)\n",
    "\n",
    "        for miss in p_miss:\n",
    "            for obs in p_obs:\n",
    "                print(f\"Currently running for p_miss: {miss} and p_obs: {obs}\")\n",
    "                train, test = self._run_univariate_imputations(M = M, p_miss = miss, p_obs = obs).values()\n",
    "                acc_train = train['acc']\n",
    "                conc_train = train['c_index']\n",
    "                acc_test = test['acc']\n",
    "                conc_test = test['c_index']\n",
    "                bias = train['bias']\n",
    "\n",
    "                self.acc_train[p_miss.index(miss), p_obs.index(obs)] = acc_train\n",
    "                self.acc_test[p_miss.index(miss), p_obs.index(obs)] = acc_test\n",
    "                self.conc_train[p_miss.index(miss), p_obs.index(obs)] = conc_train\n",
    "                self.conc_test[p_miss.index(miss), p_obs.index(obs)] = conc_test\n",
    "                self.bias[p_miss.index(miss), p_obs.index(obs)] = bias\n",
    "\n",
    "    def get_univariate_imputed_results(self):\n",
    "        \"\"\"\n",
    "        Get the results of the univariate imputation simulation.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        dict\n",
    "            A dictionary containing the accuracy, concordance index and bias for the univariate imputation simulation.\n",
    "            The dictionary values are dataframes.\n",
    "        \"\"\"\n",
    "\n",
    "        acc_train, acc_test = self._get_univariate_acc(self.acc_train), self._get_univariate_acc(self.acc_test)\n",
    "        conc_train, conc_test = self._get_univariate_conc(self.conc_train), self._get_univariate_conc(self.conc_test)\n",
    "        bias = self._get_univariate_bias()\n",
    "\n",
    "        return {'acc_train': acc_train, 'acc_test': acc_test, 'c_index_train': conc_train, 'c_index_test': conc_test, 'bias': bias}\n",
    "    \n",
    "    def simulate_multivariate_imputations(self, M, p_miss = [0.1, 0.2, 0.3, 0.4, 0.5], p_obs = [8/9, 7/9, 6/9, 5/9, 4/9], n_neighbors = 2):\n",
    "        \"\"\"\n",
    "        Run a simulation with multivariate imputations.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        M : int\n",
    "            Number of datasets to generate.\n",
    "        p_miss : list\n",
    "            List of proportions of missing values to generate for variables which will have missing values.\n",
    "\n",
    "        p_obs : list\n",
    "            List of proportions of variables to retain.\n",
    "\n",
    "        n_neighbors : int\n",
    "            Number of neighbors to use for KNN imputation.\n",
    "        \"\"\"\n",
    "\n",
    "        self.p_miss = p_miss\n",
    "        self.p_obs = p_obs\n",
    "\n",
    "        self.acc_train = np.empty((len(p_miss), len(p_obs)), dtype=object)\n",
    "        self.acc_test = np.empty((len(p_miss), len(p_obs)), dtype=object)\n",
    "        self.conc_train = np.empty((len(p_miss), len(p_obs)), dtype=object)\n",
    "        self.conc_test = np.empty((len(p_miss), len(p_obs)), dtype=object)\n",
    "        self.bias = np.empty((len(p_miss), len(p_obs)), dtype=object)\n",
    "\n",
    "        for miss in p_miss:\n",
    "            for obs in p_obs:\n",
    "                print(f\"Currently running for p_miss: {miss} and p_obs: {obs}\")\n",
    "                train, test = self._run_multivariate_imputations(M = M, p_miss = miss, p_obs = obs, n_neighbors = n_neighbors).values()\n",
    "                acc_train = train['acc']\n",
    "                conc_train = train['c_index']\n",
    "                acc_test = test['acc']\n",
    "                conc_test = test['c_index']\n",
    "                bias = train['bias']\n",
    "\n",
    "                self.acc_train[p_miss.index(miss), p_obs.index(obs)] = acc_train\n",
    "                self.acc_test[p_miss.index(miss), p_obs.index(obs)] = acc_test\n",
    "                self.conc_train[p_miss.index(miss), p_obs.index(obs)] = conc_train\n",
    "                self.conc_test[p_miss.index(miss), p_obs.index(obs)] = conc_test\n",
    "                self.bias[p_miss.index(miss), p_obs.index(obs)] = bias\n",
    "\n",
    "    def get_multivariate_imputed_results(self):\n",
    "        \"\"\"\n",
    "        Get the results of the multivariate imputation simulation.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        dict\n",
    "            A dictionary containing the accuracy, concordance index and bias for the multivariate imputation simulation.\n",
    "            The dictionary values are dataframes.\n",
    "        \"\"\"\n",
    "\n",
    "        acc_train, acc_test = self._get_multivariate_acc(self.acc_train), self._get_multivariate_acc(self.acc_test)\n",
    "        conc_train, conc_test = self._get_multivariate_conc(self.conc_train), self._get_multivariate_conc(self.conc_test)\n",
    "        bias = self._get_multivariate_bias()\n",
    "\n",
    "        return {'acc_train': acc_train, 'acc_test': acc_test, 'c_index_train': conc_train, 'c_index_test': conc_test, 'bias': bias}\n",
    "\n",
    "    def simulate_cca(self, M, p_miss = [0.1, 0.2, 0.3, 0.4, 0.5], p_obs = [8/9, 7/9, 6/9, 5/9, 4/9]):\n",
    "        \"\"\"\n",
    "        Run a simulation with CCA.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        M : int\n",
    "            Number of datasets to generate.\n",
    "\n",
    "        p_miss : list\n",
    "            List of proportions of missing values to generate for variables which will have missing values.\n",
    "\n",
    "        p_obs : list\n",
    "            List of proportions of variables to retain.\n",
    "        \"\"\"\n",
    "\n",
    "        self.p_miss = p_miss\n",
    "        self.p_obs = p_obs\n",
    "\n",
    "        self.conc_train = np.empty((len(p_miss), len(p_obs)), dtype=object)\n",
    "        self.conc_test  = np.empty((len(p_miss), len(p_obs)), dtype=object)\n",
    "        self.bias = np.empty((len(p_miss), len(p_obs)), dtype=object)\n",
    "\n",
    "        for miss in p_miss:\n",
    "            for obs in p_obs:\n",
    "                print(f\"Currently running for p_miss: {miss} and p_obs: {obs}\")\n",
    "                train, test = self._run_CCA(M = M, p_miss = miss, p_obs = obs, mecha = 'MAR').values()\n",
    "                conc_train = train['c_index']\n",
    "                conc_test = test['c_index']\n",
    "                bias = train['bias']\n",
    "            \n",
    "                self.conc_train[p_miss.index(miss), p_obs.index(obs)] = conc_train\n",
    "                self.conc_test[p_miss.index(miss), p_obs.index(obs)] = conc_test\n",
    "                self.bias[p_miss.index(miss), p_obs.index(obs)] = bias\n",
    "\n",
    "    def get_cca_results(self):\n",
    "        \"\"\"\n",
    "        Get the results of the CCA simulation.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        dict\n",
    "            A dictionary containing the concordance index and the bias for the CCA simulation.\n",
    "            The dictionary values are dataframes.        \n",
    "        \"\"\"\n",
    "\n",
    "        conc_train, conc_test = self._get_cca_conc(self.conc_train), self._get_cca_conc(self.conc_test)\n",
    "        bias = self._get_cca_bias()\n",
    "\n",
    "        return {'c_index_train': conc_train, 'c_index_test': conc_test, 'bias': bias}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rx</th>\n",
       "      <th>sex</th>\n",
       "      <th>obstruct</th>\n",
       "      <th>perfor</th>\n",
       "      <th>adhere</th>\n",
       "      <th>nodes</th>\n",
       "      <th>status</th>\n",
       "      <th>differ</th>\n",
       "      <th>extent</th>\n",
       "      <th>surg</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Lev+5FU</td>\n",
       "      <td>M</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>3.0</td>\n",
       "      <td>True</td>\n",
       "      <td>2.0</td>\n",
       "      <td>Serosa</td>\n",
       "      <td>S</td>\n",
       "      <td>1521</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Lev+5FU</td>\n",
       "      <td>M</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>1.0</td>\n",
       "      <td>False</td>\n",
       "      <td>2.0</td>\n",
       "      <td>Serosa</td>\n",
       "      <td>S</td>\n",
       "      <td>3087</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Obs</td>\n",
       "      <td>F</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>Y</td>\n",
       "      <td>3.0</td>\n",
       "      <td>True</td>\n",
       "      <td>2.0</td>\n",
       "      <td>Muscule</td>\n",
       "      <td>S</td>\n",
       "      <td>963</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Lev+5FU</td>\n",
       "      <td>F</td>\n",
       "      <td>Y</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>3.0</td>\n",
       "      <td>True</td>\n",
       "      <td>2.0</td>\n",
       "      <td>Serosa</td>\n",
       "      <td>L</td>\n",
       "      <td>293</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Obs</td>\n",
       "      <td>M</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>4.0</td>\n",
       "      <td>True</td>\n",
       "      <td>2.0</td>\n",
       "      <td>Serosa</td>\n",
       "      <td>L</td>\n",
       "      <td>659</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        rx sex obstruct perfor adhere nodes  status  differ   extent surg  \\\n",
       "0  Lev+5FU   M        N      N      N   3.0    True     2.0   Serosa    S   \n",
       "1  Lev+5FU   M        N      N      N   1.0   False     2.0   Serosa    S   \n",
       "2      Obs   F        N      N      Y   3.0    True     2.0  Muscule    S   \n",
       "3  Lev+5FU   F        Y      N      N   3.0    True     2.0   Serosa    L   \n",
       "4      Obs   M        N      N      N   4.0    True     2.0   Serosa    L   \n",
       "\n",
       "   time  \n",
       "0  1521  \n",
       "1  3087  \n",
       "2   963  \n",
       "3   293  \n",
       "4   659  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('../../data/colon/colon.csv', sep=',', index_col=0)\n",
    "# Start by subetting data where etype == 2\n",
    "df = df[df['etype'] == 2]\n",
    "# drop node4 column \n",
    "df = df.drop('node4', axis=1)\n",
    "\n",
    "df.dropna(axis=0, inplace=True)\n",
    "cox_ph_cols = df.columns.drop(['id', 'study', 'age', 'etype'])\n",
    "\n",
    "to_keep = [col for col in df.columns if col not in ['etype', 'study', 'id', 'age']]\n",
    "df = df[to_keep]\n",
    "df = df.reset_index(drop=True)\n",
    "df['sex'] = df['sex'].map({1: 'M', 0: 'F'})\n",
    "df['obstruct'] = df['obstruct'].map({1: 'Y', 0: 'N'})\n",
    "df['perfor'] = df['perfor'].map({1: 'Y', 0: 'N'})\n",
    "df['adhere'] = df['adhere'].map({1: 'Y', 0: 'N'})\n",
    "df['extent'] = df['extent'].map({1: 'Submucosa', 2: 'Muscule', 3: 'Serosa', 4: 'Contiguous_structures'})\n",
    "df['surg'] = df['surg'].map({1: 'L', 0: 'S'})\n",
    "df['status'] = df['status'].map({1: True, 0: False})\n",
    "df[\"nodes\"] = pd.cut(df[\"nodes\"], bins = [-1, 1, 3, 7, 100], labels = [1.0, 2.0, 3.0, 4.0])\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_colnames = ['rx', 'sex', 'obstruct', 'perfor', 'adhere', 'nodes', 'differ', 'extent', 'surg']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running simulations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initial summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>model</th>\n",
       "      <td>lifelines.CoxPHFitter</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>duration col</th>\n",
       "      <td>'time'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>event col</th>\n",
       "      <td>'status'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>baseline estimation</th>\n",
       "      <td>breslow</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>number of observations</th>\n",
       "      <td>888</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>number of events observed</th>\n",
       "      <td>430</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>partial log-likelihood</th>\n",
       "      <td>-2707.79</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>time fit was run</th>\n",
       "      <td>2024-05-10 11:46:22 UTC</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div><table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th style=\"min-width: 12px;\"></th>\n",
       "      <th style=\"min-width: 12px;\">coef</th>\n",
       "      <th style=\"min-width: 12px;\">exp(coef)</th>\n",
       "      <th style=\"min-width: 12px;\">se(coef)</th>\n",
       "      <th style=\"min-width: 12px;\">coef lower 95%</th>\n",
       "      <th style=\"min-width: 12px;\">coef upper 95%</th>\n",
       "      <th style=\"min-width: 12px;\">exp(coef) lower 95%</th>\n",
       "      <th style=\"min-width: 12px;\">exp(coef) upper 95%</th>\n",
       "      <th style=\"min-width: 12px;\">cmp to</th>\n",
       "      <th style=\"min-width: 12px;\">z</th>\n",
       "      <th style=\"min-width: 12px;\">p</th>\n",
       "      <th style=\"min-width: 12px;\">-log2(p)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>rx</th>\n",
       "      <td>-0.08</td>\n",
       "      <td>0.92</td>\n",
       "      <td>0.05</td>\n",
       "      <td>-0.18</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.83</td>\n",
       "      <td>1.02</td>\n",
       "      <td>0.00</td>\n",
       "      <td>-1.60</td>\n",
       "      <td>0.11</td>\n",
       "      <td>3.19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nodes</th>\n",
       "      <td>0.44</td>\n",
       "      <td>1.55</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.34</td>\n",
       "      <td>0.53</td>\n",
       "      <td>1.41</td>\n",
       "      <td>1.71</td>\n",
       "      <td>0.00</td>\n",
       "      <td>9.06</td>\n",
       "      <td>&lt;0.005</td>\n",
       "      <td>62.73</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>differ</th>\n",
       "      <td>0.09</td>\n",
       "      <td>1.09</td>\n",
       "      <td>0.05</td>\n",
       "      <td>-0.02</td>\n",
       "      <td>0.19</td>\n",
       "      <td>0.98</td>\n",
       "      <td>1.20</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.66</td>\n",
       "      <td>0.10</td>\n",
       "      <td>3.37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>extent</th>\n",
       "      <td>-0.16</td>\n",
       "      <td>0.85</td>\n",
       "      <td>0.05</td>\n",
       "      <td>-0.27</td>\n",
       "      <td>-0.05</td>\n",
       "      <td>0.77</td>\n",
       "      <td>0.95</td>\n",
       "      <td>0.00</td>\n",
       "      <td>-2.94</td>\n",
       "      <td>&lt;0.005</td>\n",
       "      <td>8.26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sex_M</th>\n",
       "      <td>-0.01</td>\n",
       "      <td>0.99</td>\n",
       "      <td>0.05</td>\n",
       "      <td>-0.11</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.90</td>\n",
       "      <td>1.08</td>\n",
       "      <td>0.00</td>\n",
       "      <td>-0.31</td>\n",
       "      <td>0.76</td>\n",
       "      <td>0.40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>obstruct_Y</th>\n",
       "      <td>0.09</td>\n",
       "      <td>1.09</td>\n",
       "      <td>0.05</td>\n",
       "      <td>-0.00</td>\n",
       "      <td>0.18</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.20</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.89</td>\n",
       "      <td>0.06</td>\n",
       "      <td>4.10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>perfor_Y</th>\n",
       "      <td>-0.03</td>\n",
       "      <td>0.98</td>\n",
       "      <td>0.05</td>\n",
       "      <td>-0.12</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.89</td>\n",
       "      <td>1.07</td>\n",
       "      <td>0.00</td>\n",
       "      <td>-0.54</td>\n",
       "      <td>0.59</td>\n",
       "      <td>0.76</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>adhere_Y</th>\n",
       "      <td>0.09</td>\n",
       "      <td>1.09</td>\n",
       "      <td>0.05</td>\n",
       "      <td>-0.00</td>\n",
       "      <td>0.18</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.19</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.86</td>\n",
       "      <td>0.06</td>\n",
       "      <td>4.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>surg_S</th>\n",
       "      <td>-0.11</td>\n",
       "      <td>0.90</td>\n",
       "      <td>0.05</td>\n",
       "      <td>-0.20</td>\n",
       "      <td>-0.02</td>\n",
       "      <td>0.82</td>\n",
       "      <td>0.98</td>\n",
       "      <td>0.00</td>\n",
       "      <td>-2.32</td>\n",
       "      <td>0.02</td>\n",
       "      <td>5.61</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><br><div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Concordance</th>\n",
       "      <td>0.66</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Partial AIC</th>\n",
       "      <td>5433.59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>log-likelihood ratio test</th>\n",
       "      <td>120.23 on 9 df</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>-log2(p) of ll-ratio test</th>\n",
       "      <td>69.50</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/latex": [
       "\\begin{tabular}{lrrrrrrrrrrr}\n",
       " & coef & exp(coef) & se(coef) & coef lower 95% & coef upper 95% & exp(coef) lower 95% & exp(coef) upper 95% & cmp to & z & p & -log2(p) \\\\\n",
       "covariate &  &  &  &  &  &  &  &  &  &  &  \\\\\n",
       "rx & -0.08 & 0.92 & 0.05 & -0.18 & 0.02 & 0.83 & 1.02 & 0.00 & -1.60 & 0.11 & 3.19 \\\\\n",
       "nodes & 0.44 & 1.55 & 0.05 & 0.34 & 0.53 & 1.41 & 1.71 & 0.00 & 9.06 & 0.00 & 62.73 \\\\\n",
       "differ & 0.09 & 1.09 & 0.05 & -0.02 & 0.19 & 0.98 & 1.20 & 0.00 & 1.66 & 0.10 & 3.37 \\\\\n",
       "extent & -0.16 & 0.85 & 0.05 & -0.27 & -0.05 & 0.77 & 0.95 & 0.00 & -2.94 & 0.00 & 8.26 \\\\\n",
       "sex_M & -0.01 & 0.99 & 0.05 & -0.11 & 0.08 & 0.90 & 1.08 & 0.00 & -0.31 & 0.76 & 0.40 \\\\\n",
       "obstruct_Y & 0.09 & 1.09 & 0.05 & -0.00 & 0.18 & 1.00 & 1.20 & 0.00 & 1.89 & 0.06 & 4.10 \\\\\n",
       "perfor_Y & -0.03 & 0.98 & 0.05 & -0.12 & 0.07 & 0.89 & 1.07 & 0.00 & -0.54 & 0.59 & 0.76 \\\\\n",
       "adhere_Y & 0.09 & 1.09 & 0.05 & -0.00 & 0.18 & 1.00 & 1.19 & 0.00 & 1.86 & 0.06 & 4.00 \\\\\n",
       "surg_S & -0.11 & 0.90 & 0.05 & -0.20 & -0.02 & 0.82 & 0.98 & 0.00 & -2.32 & 0.02 & 5.61 \\\\\n",
       "\\end{tabular}\n"
      ],
      "text/plain": [
       "<lifelines.CoxPHFitter: fitted with 888 total observations, 458 right-censored observations>\n",
       "             duration col = 'time'\n",
       "                event col = 'status'\n",
       "      baseline estimation = breslow\n",
       "   number of observations = 888\n",
       "number of events observed = 430\n",
       "   partial log-likelihood = -2707.79\n",
       "         time fit was run = 2024-05-10 11:46:22 UTC\n",
       "\n",
       "---\n",
       "             coef  exp(coef)   se(coef)   coef lower 95%   coef upper 95%  exp(coef) lower 95%  exp(coef) upper 95%\n",
       "covariate                                                                                                          \n",
       "rx          -0.08       0.92       0.05            -0.18             0.02                 0.83                 1.02\n",
       "nodes        0.44       1.55       0.05             0.34             0.53                 1.41                 1.71\n",
       "differ       0.09       1.09       0.05            -0.02             0.19                 0.98                 1.20\n",
       "extent      -0.16       0.85       0.05            -0.27            -0.05                 0.77                 0.95\n",
       "sex_M       -0.01       0.99       0.05            -0.11             0.08                 0.90                 1.08\n",
       "obstruct_Y   0.09       1.09       0.05            -0.00             0.18                 1.00                 1.20\n",
       "perfor_Y    -0.03       0.98       0.05            -0.12             0.07                 0.89                 1.07\n",
       "adhere_Y     0.09       1.09       0.05            -0.00             0.18                 1.00                 1.19\n",
       "surg_S      -0.11       0.90       0.05            -0.20            -0.02                 0.82                 0.98\n",
       "\n",
       "             cmp to     z      p   -log2(p)\n",
       "covariate                                  \n",
       "rx             0.00 -1.60   0.11       3.19\n",
       "nodes          0.00  9.06 <0.005      62.73\n",
       "differ         0.00  1.66   0.10       3.37\n",
       "extent         0.00 -2.94 <0.005       8.26\n",
       "sex_M          0.00 -0.31   0.76       0.40\n",
       "obstruct_Y     0.00  1.89   0.06       4.10\n",
       "perfor_Y       0.00 -0.54   0.59       0.76\n",
       "adhere_Y       0.00  1.86   0.06       4.00\n",
       "surg_S         0.00 -2.32   0.02       5.61\n",
       "---\n",
       "Concordance = 0.66\n",
       "Partial AIC = 5433.59\n",
       "log-likelihood ratio test = 120.23 on 9 df\n",
       "-log2(p) of ll-ratio test = 69.50"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "init = SimulationMCAR(df, duration_col = 'time', event_col = 'status', cat_colnames = cat_colnames)\n",
    "init.complete_summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MCAR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Univariate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Currently simulating for p_miss: 0.05\n",
      "Currently simulating for p_miss: 0.1\n",
      "Currently simulating for p_miss: 0.2\n",
      "Currently simulating for p_miss: 0.3\n",
      "Currently simulating for p_miss: 0.4\n",
      "Currently simulating for p_miss: 0.5\n"
     ]
    }
   ],
   "source": [
    "M = 50\n",
    "mcar_uni = SimulationMCAR(df, duration_col = 'time', event_col = 'status', cat_colnames = cat_colnames)\n",
    "mcar_uni.simulate_univariate_imputations(M = M, p_miss = [0.05, 0.1, 0.2, 0.3, 0.4, 0.5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_train, acc_test, conc_train, conc_test, bias = mcar_uni.get_univariate_imputed_results().values()\n",
    "acc_train.to_csv(f'../results/acc_train_MCAR{M}_univariate.csv')\n",
    "acc_test.to_csv(f'../results/acc_test_MCAR{M}_univariate.csv')\n",
    "conc_train.to_csv(f'../results/c_index_train_MCAR{M}_univariate.csv')\n",
    "conc_test.to_csv(f'../results/c_index_test_MCAR{M}_univariate.csv')\n",
    "bias.to_csv(f'../results/bias_MCAR{M}_univariate.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Multivariate (kNN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Currently simulating for p_miss: 0.05\n",
      "Currently simulating for p_miss: 0.1\n",
      "Currently simulating for p_miss: 0.2\n",
      "Currently simulating for p_miss: 0.3\n",
      "Currently simulating for p_miss: 0.4\n",
      "Currently simulating for p_miss: 0.5\n"
     ]
    }
   ],
   "source": [
    "n_neighbors = 10\n",
    "M = 50\n",
    "\n",
    "mcar_knn = SimulationMCAR(df, duration_col = 'time', event_col = 'status', cat_colnames = cat_colnames)\n",
    "mcar_knn.simulate_multivariate_imputations(M = M, p_miss = [0.05, 0.1, 0.2, 0.3, 0.4, 0.5], n_neighbors = n_neighbors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_train, acc_test, conc_train, conc_test, bias = mcar_knn.get_multivariate_imputed_results().values()\n",
    "acc_train.to_csv(f'../results/acc_train_MCAR{M}_knn_N{n_neighbors}.csv')\n",
    "acc_test.to_csv(f'../results/acc_test_MCAR{M}_knn_N{n_neighbors}.csv')\n",
    "conc_train.to_csv(f'../results/c_index_train_MCAR{M}_knn_N{n_neighbors}.csv')\n",
    "conc_test.to_csv(f'../results/c_index_test_MCAR{M}_knn_N{n_neighbors}.csv')\n",
    "bias.to_csv(f'../results/bias_MCAR{M}_knn_N{n_neighbors}.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Currently simulating for p_miss: 0.05\n",
      "Currently simulating for p_miss: 0.1\n",
      "Currently simulating for p_miss: 0.15\n",
      "Currently simulating for p_miss: 0.2\n",
      "Currently simulating for p_miss: 0.25\n"
     ]
    }
   ],
   "source": [
    "M = 50\n",
    "\n",
    "mcar_cca = SimulationMCAR(df, duration_col = 'time', event_col = 'status', cat_colnames = cat_colnames)\n",
    "mcar_cca.simulate_cca(M = M, p_miss = [0.05, 0.1, 0.15, 0.2, 0.25])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "conc_train, conc_test, bias = mcar_cca.get_cca_results().values()\n",
    "conc_train.to_csv(f'../results/c_index_train_MCAR{M}_cca.csv')\n",
    "conc_test.to_csv(f'../results/c_index_test_MCAR{M}_cca.csv')\n",
    "bias.to_csv(f'../results/bias_MCAR{M}_cca.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MAR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Univariate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Currently running for p_miss: 0.05 and p_obs: 0.8888888888888888\n",
      "Currently running for p_miss: 0.05 and p_obs: 0.7777777777777778\n",
      "Currently running for p_miss: 0.05 and p_obs: 0.6666666666666666\n",
      "Currently running for p_miss: 0.05 and p_obs: 0.5555555555555556\n",
      "Currently running for p_miss: 0.05 and p_obs: 0.4444444444444444\n",
      "Currently running for p_miss: 0.05 and p_obs: 0.3333333333333333\n",
      "Currently running for p_miss: 0.1 and p_obs: 0.8888888888888888\n",
      "Currently running for p_miss: 0.1 and p_obs: 0.7777777777777778\n",
      "Currently running for p_miss: 0.1 and p_obs: 0.6666666666666666\n",
      "Currently running for p_miss: 0.1 and p_obs: 0.5555555555555556\n",
      "Currently running for p_miss: 0.1 and p_obs: 0.4444444444444444\n",
      "Currently running for p_miss: 0.1 and p_obs: 0.3333333333333333\n",
      "Currently running for p_miss: 0.2 and p_obs: 0.8888888888888888\n",
      "Currently running for p_miss: 0.2 and p_obs: 0.7777777777777778\n",
      "Currently running for p_miss: 0.2 and p_obs: 0.6666666666666666\n",
      "Currently running for p_miss: 0.2 and p_obs: 0.5555555555555556\n",
      "Currently running for p_miss: 0.2 and p_obs: 0.4444444444444444\n",
      "Currently running for p_miss: 0.2 and p_obs: 0.3333333333333333\n",
      "Currently running for p_miss: 0.3 and p_obs: 0.8888888888888888\n",
      "Currently running for p_miss: 0.3 and p_obs: 0.7777777777777778\n",
      "Currently running for p_miss: 0.3 and p_obs: 0.6666666666666666\n",
      "Currently running for p_miss: 0.3 and p_obs: 0.5555555555555556\n",
      "Currently running for p_miss: 0.3 and p_obs: 0.4444444444444444\n",
      "Currently running for p_miss: 0.3 and p_obs: 0.3333333333333333\n",
      "Currently running for p_miss: 0.4 and p_obs: 0.8888888888888888\n",
      "Currently running for p_miss: 0.4 and p_obs: 0.7777777777777778\n",
      "Currently running for p_miss: 0.4 and p_obs: 0.6666666666666666\n",
      "Currently running for p_miss: 0.4 and p_obs: 0.5555555555555556\n",
      "Currently running for p_miss: 0.4 and p_obs: 0.4444444444444444\n",
      "Currently running for p_miss: 0.4 and p_obs: 0.3333333333333333\n",
      "Currently running for p_miss: 0.5 and p_obs: 0.8888888888888888\n",
      "Currently running for p_miss: 0.5 and p_obs: 0.7777777777777778\n",
      "Currently running for p_miss: 0.5 and p_obs: 0.6666666666666666\n",
      "Currently running for p_miss: 0.5 and p_obs: 0.5555555555555556\n",
      "Currently running for p_miss: 0.5 and p_obs: 0.4444444444444444\n",
      "Currently running for p_miss: 0.5 and p_obs: 0.3333333333333333\n"
     ]
    }
   ],
   "source": [
    "M = 50\n",
    "\n",
    "mar_uni = SimulationMAR(df, duration_col = 'time', event_col = 'status', cat_colnames = cat_colnames)\n",
    "mar_uni.simulate_univariate_imputations(M = M, p_miss = [0.05, 0.1, 0.2, 0.3, 0.4, 0.5], p_obs = [8/9, 7/9, 6/9, 5/9, 4/9, 3/9])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_train, acc_test, conc_train, conc_test, bias = mar_uni.get_univariate_imputed_results().values()\n",
    "acc_train.to_csv(f'../results/acc_train_MAR{M}_univariate.csv')\n",
    "acc_test.to_csv(f'../results/acc_test_MAR{M}_univariate.csv')\n",
    "conc_train.to_csv(f'../results/c_index_train_MAR{M}_univariate.csv')\n",
    "conc_test.to_csv(f'../results/c_index_test_MAR{M}_univariate.csv')\n",
    "bias.to_csv(f'../results/bias_MAR{M}_univariate.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Multivariate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Currently running for p_miss: 0.05 and p_obs: 0.8888888888888888\n",
      "Currently running for p_miss: 0.05 and p_obs: 0.7777777777777778\n",
      "Currently running for p_miss: 0.05 and p_obs: 0.6666666666666666\n",
      "Currently running for p_miss: 0.05 and p_obs: 0.5555555555555556\n",
      "Currently running for p_miss: 0.05 and p_obs: 0.4444444444444444\n",
      "Currently running for p_miss: 0.05 and p_obs: 0.3333333333333333\n",
      "Currently running for p_miss: 0.1 and p_obs: 0.8888888888888888\n",
      "Currently running for p_miss: 0.1 and p_obs: 0.7777777777777778\n",
      "Currently running for p_miss: 0.1 and p_obs: 0.6666666666666666\n",
      "Currently running for p_miss: 0.1 and p_obs: 0.5555555555555556\n",
      "Currently running for p_miss: 0.1 and p_obs: 0.4444444444444444\n",
      "Currently running for p_miss: 0.1 and p_obs: 0.3333333333333333\n",
      "Currently running for p_miss: 0.2 and p_obs: 0.8888888888888888\n",
      "Currently running for p_miss: 0.2 and p_obs: 0.7777777777777778\n",
      "Currently running for p_miss: 0.2 and p_obs: 0.6666666666666666\n",
      "Currently running for p_miss: 0.2 and p_obs: 0.5555555555555556\n",
      "Currently running for p_miss: 0.2 and p_obs: 0.4444444444444444\n",
      "Currently running for p_miss: 0.2 and p_obs: 0.3333333333333333\n",
      "Currently running for p_miss: 0.3 and p_obs: 0.8888888888888888\n",
      "Currently running for p_miss: 0.3 and p_obs: 0.7777777777777778\n",
      "Currently running for p_miss: 0.3 and p_obs: 0.6666666666666666\n",
      "Currently running for p_miss: 0.3 and p_obs: 0.5555555555555556\n",
      "Currently running for p_miss: 0.3 and p_obs: 0.4444444444444444\n",
      "Currently running for p_miss: 0.3 and p_obs: 0.3333333333333333\n",
      "Currently running for p_miss: 0.4 and p_obs: 0.8888888888888888\n",
      "Currently running for p_miss: 0.4 and p_obs: 0.7777777777777778\n",
      "Currently running for p_miss: 0.4 and p_obs: 0.6666666666666666\n",
      "Currently running for p_miss: 0.4 and p_obs: 0.5555555555555556\n",
      "Currently running for p_miss: 0.4 and p_obs: 0.4444444444444444\n",
      "Currently running for p_miss: 0.4 and p_obs: 0.3333333333333333\n",
      "Currently running for p_miss: 0.5 and p_obs: 0.8888888888888888\n",
      "Currently running for p_miss: 0.5 and p_obs: 0.7777777777777778\n",
      "Currently running for p_miss: 0.5 and p_obs: 0.6666666666666666\n",
      "Currently running for p_miss: 0.5 and p_obs: 0.5555555555555556\n",
      "Currently running for p_miss: 0.5 and p_obs: 0.4444444444444444\n",
      "Currently running for p_miss: 0.5 and p_obs: 0.3333333333333333\n"
     ]
    }
   ],
   "source": [
    "M = 50\n",
    "n_neighbors = 10\n",
    "\n",
    "mar_multi = SimulationMAR(df, duration_col = 'time', event_col = 'status', cat_colnames = cat_colnames)\n",
    "mar_multi.simulate_multivariate_imputations(M = M, p_miss = [0.05, 0.1, 0.2, 0.3, 0.4, 0.5], p_obs = [8/9, 7/9, 6/9, 5/9, 4/9, 3/9], n_neighbors = n_neighbors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_train, acc_test, conc_train, conc_test, bias = mar_multi.get_multivariate_imputed_results().values()\n",
    "acc_train.to_csv(f'../results/acc_train_MAR{M}_knn_N{n_neighbors}.csv')\n",
    "acc_test.to_csv(f'../results/acc_test_MAR{M}_knn_N{n_neighbors}.csv')\n",
    "conc_train.to_csv(f'../results/c_index_train_MAR{M}_knn_N{n_neighbors}.csv')\n",
    "conc_test.to_csv(f'../results/c_index_test_MAR{M}_knn_N{n_neighbors}.csv')\n",
    "bias.to_csv(f'../results/bias_MAR{M}_knn_N{n_neighbors}.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Currently running for p_miss: 0.05 and p_obs: 0.8888888888888888\n",
      "Currently running for p_miss: 0.05 and p_obs: 0.7777777777777778\n",
      "Currently running for p_miss: 0.05 and p_obs: 0.6666666666666666\n",
      "Currently running for p_miss: 0.05 and p_obs: 0.5555555555555556\n",
      "Currently running for p_miss: 0.05 and p_obs: 0.4444444444444444\n",
      "Currently running for p_miss: 0.05 and p_obs: 0.3333333333333333\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\erlen\\.conda\\envs\\masters\\Lib\\site-packages\\sksurv\\linear_model\\coxph.py:170: RuntimeWarning: overflow encountered in exp\n",
      "  risk_set2 += np.exp(xw[k])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Currently running for p_miss: 0.1 and p_obs: 0.8888888888888888\n",
      "Currently running for p_miss: 0.1 and p_obs: 0.7777777777777778\n",
      "Currently running for p_miss: 0.1 and p_obs: 0.6666666666666666\n",
      "Currently running for p_miss: 0.1 and p_obs: 0.5555555555555556\n",
      "Currently running for p_miss: 0.1 and p_obs: 0.4444444444444444\n",
      "Currently running for p_miss: 0.1 and p_obs: 0.3333333333333333\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\erlen\\.conda\\envs\\masters\\Lib\\site-packages\\sksurv\\linear_model\\coxph.py:449: LinAlgWarning: Ill-conditioned matrix (rcond=1.78565e-17): result may not be accurate.\n",
      "  delta = solve(\n",
      "c:\\Users\\erlen\\.conda\\envs\\masters\\Lib\\site-packages\\sksurv\\linear_model\\coxph.py:179: RuntimeWarning: divide by zero encountered in log\n",
      "  loss -= (numerator - n_events * np.log(risk_set)) / n_samples\n",
      "c:\\Users\\erlen\\.conda\\envs\\masters\\Lib\\site-packages\\sksurv\\linear_model\\coxph.py:170: RuntimeWarning: overflow encountered in exp\n",
      "  risk_set2 += np.exp(xw[k])\n",
      "c:\\Users\\erlen\\.conda\\envs\\masters\\Lib\\site-packages\\sksurv\\linear_model\\coxph.py:179: RuntimeWarning: invalid value encountered in scalar subtract\n",
      "  loss -= (numerator - n_events * np.log(risk_set)) / n_samples\n",
      "c:\\Users\\erlen\\.conda\\envs\\masters\\Lib\\site-packages\\sksurv\\linear_model\\coxph.py:194: RuntimeWarning: overflow encountered in exp\n",
      "  exp_xw = np.exp(offset + np.dot(x, w))\n",
      "c:\\Users\\erlen\\.conda\\envs\\masters\\Lib\\site-packages\\sksurv\\linear_model\\coxph.py:238: RuntimeWarning: invalid value encountered in divide\n",
      "  z = risk_set_x / risk_set\n",
      "c:\\Users\\erlen\\.conda\\envs\\masters\\Lib\\site-packages\\sksurv\\linear_model\\coxph.py:241: RuntimeWarning: invalid value encountered in divide\n",
      "  a = risk_set_xx / risk_set\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed with alpha=0: search direction contains NaN or infinite values\n",
      "Currently running for p_miss: 0.15 and p_obs: 0.8888888888888888\n",
      "Currently running for p_miss: 0.15 and p_obs: 0.7777777777777778\n",
      "Currently running for p_miss: 0.15 and p_obs: 0.6666666666666666\n",
      "Currently running for p_miss: 0.15 and p_obs: 0.5555555555555556\n",
      "Currently running for p_miss: 0.15 and p_obs: 0.4444444444444444\n",
      "Currently running for p_miss: 0.15 and p_obs: 0.3333333333333333\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\erlen\\.conda\\envs\\masters\\Lib\\site-packages\\sksurv\\linear_model\\coxph.py:170: RuntimeWarning: overflow encountered in exp\n",
      "  risk_set2 += np.exp(xw[k])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Currently running for p_miss: 0.2 and p_obs: 0.8888888888888888\n",
      "Currently running for p_miss: 0.2 and p_obs: 0.7777777777777778\n",
      "Currently running for p_miss: 0.2 and p_obs: 0.6666666666666666\n",
      "Currently running for p_miss: 0.2 and p_obs: 0.5555555555555556\n",
      "Currently running for p_miss: 0.2 and p_obs: 0.4444444444444444\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\erlen\\.conda\\envs\\masters\\Lib\\site-packages\\sksurv\\linear_model\\coxph.py:170: RuntimeWarning: overflow encountered in exp\n",
      "  risk_set2 += np.exp(xw[k])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Currently running for p_miss: 0.2 and p_obs: 0.3333333333333333\n",
      "Currently running for p_miss: 0.25 and p_obs: 0.8888888888888888\n",
      "Currently running for p_miss: 0.25 and p_obs: 0.7777777777777778\n",
      "Currently running for p_miss: 0.25 and p_obs: 0.6666666666666666\n",
      "Currently running for p_miss: 0.25 and p_obs: 0.5555555555555556\n",
      "Currently running for p_miss: 0.25 and p_obs: 0.4444444444444444\n",
      "Currently running for p_miss: 0.25 and p_obs: 0.3333333333333333\n"
     ]
    }
   ],
   "source": [
    "M = 50\n",
    "mar_cca = SimulationMAR(df, duration_col = 'time', event_col = 'status', cat_colnames = cat_colnames)\n",
    "mar_cca.simulate_cca(M = M, p_miss = [0.05, 0.1, 0.15, 0.2, 0.25], p_obs = [8/9, 7/9, 6/9, 5/9, 4/9, 3/9])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "conc_train, conc_test, bias = mar_cca.get_cca_results().values()\n",
    "conc_train.to_csv(f'../results/c_index_train_MAR{M}_cca.csv')\n",
    "conc_test.to_csv(f'../results/c_index_test_MAR{M}_cca.csv')\n",
    "bias.to_csv(f'../results/bias_MAR{M}_cca.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "masters",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
