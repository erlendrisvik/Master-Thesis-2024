{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Disclaimer\n",
    "The experiment was initially conducted with both nunmerical and categorical variables. After discussions with supervisors, we found it reasonable to stick to only categorical variables due to the problem of RMSE under MAR. This made changes to the code and a lot of variables may have redundant namings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Missing value experiment MI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import TargetEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from lifelines import CoxPHFitter\n",
    "from sksurv.linear_model import CoxPHSurvivalAnalysis\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sksurv.metrics import (\n",
    "    concordance_index_censored\n",
    ")\n",
    "from sksurv.util import Surv\n",
    "\n",
    "try:\n",
    "    from utils.utils import *\n",
    "    from utils.encoders import MultiLabelEncoder\n",
    "except:\n",
    "    import sys\n",
    "    sys.path.append('../')\n",
    "    from utils.utils import *\n",
    "    from utils.encoders import MultiLabelEncoder    \n",
    "    \n",
    "\n",
    "def accuracy(X_imp, X_true, mask, column_wise = False):\n",
    "    \"\"\"\n",
    "    Accuracy between imputed variables and ground truth.\n",
    "    Pytorch/Numpy agnostic\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    X_imp : torch.DoubleTensor or np.ndarray, shape (n, d)\n",
    "        Data with imputed variables.\n",
    "    \n",
    "    X_true : torch.DoubleTensor or np.ndarray, shape (n, d)\n",
    "        Ground truth.\n",
    "    \n",
    "    mask : torch.BoolTensor or np.ndarray of booleans, shape (n, d)\n",
    "        Missing value mask (missing if True)\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    acc : torch.DoubleTensor or np.ndarray of floats, shape (d,) if column_wise = True, else float\n",
    "        accuracy between imputed variables and ground truth.\n",
    "    \"\"\"\n",
    "\n",
    "    if not mask.any():\n",
    "        return torch.tensor(1, dtype=torch.float64)   \n",
    "    \n",
    "    if torch.is_tensor(X_imp):\n",
    "        if column_wise:\n",
    "            acc = []\n",
    "            for col in range(X_imp.shape[1]):\n",
    "                if not mask[:, col].any():\n",
    "                    acc.append(torch.tensor(0, dtype=torch.float64))\n",
    "                else:\n",
    "                    diff = (X_imp[:, col] == X_true[:, col]).double()\n",
    "                    valid_diff = diff[mask[:, col]]\n",
    "                    acc.append(valid_diff.mean())\n",
    "            return torch.stack(acc)\n",
    "        else:\n",
    "            return ((X_imp[mask] == X_true[mask]).double().mean())\n",
    "    else:  # NumPy array\n",
    "        if column_wise:\n",
    "            acc = []\n",
    "            for col in range(X_imp.shape[1]):\n",
    "                if not mask[:, col].any():\n",
    "                    acc.append(0)\n",
    "                else:\n",
    "                    diff = (X_imp[:, col] == X_true[:, col]).double()\n",
    "                    valid_diff = diff[mask[:, col]]\n",
    "                    acc.append(valid_diff.mean())\n",
    "            return np.array(acc)\n",
    "        else:\n",
    "            return ((X_imp[mask] == X_true[mask]).mean())\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MiceSimulationMV():\n",
    "    def __init__(self, X, duration_col, event_col, cat_colnames):\n",
    "        \"\"\"\n",
    "        Initialize the SimulationMV class. \n",
    "        Event and duration is removed for X before simulation.\n",
    "        Encoding types used for modelling are specified for each categorical column.\n",
    "\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : pandas.DataFrame\n",
    "            Complete data matrix\n",
    "        duration_col : str\n",
    "            Name of the column containing the durations.\n",
    "        event_col : str\n",
    "            Name of the column containing the events.\n",
    "        cat_colnames : list of str\n",
    "            List of categorical column names.        \n",
    "        \"\"\"\n",
    "        # Assign parameters to instance\n",
    "        self.duration_col = duration_col\n",
    "        self.event_col = event_col\n",
    "\n",
    "        self.categorical_colnames = cat_colnames\n",
    "\n",
    "        # Reorder the columns \n",
    "        new_order = cat_colnames+[duration_col, event_col]\n",
    "        X = X[new_order]\n",
    "\n",
    "        # Store original X for baseline evaluation\n",
    "        self.X_original = X.copy()\n",
    "\n",
    "        # Remove duration and event col. We don't want to impute and evaluate on these\n",
    "        X_to_sim = self._remove_event_and_duration(X)\n",
    "        X_to_sim = X_to_sim.to_numpy()\n",
    "\n",
    "        # split into numerical and categorical\n",
    "        X_cat_init = X_to_sim\n",
    "        \n",
    "        # Define ordinal, one-hot and target encoding columns\n",
    "        self.ordinal_idx = []\n",
    "        self.one_hot_idx = []\n",
    "        self.target_idx = []\n",
    "\n",
    "        for i in range(X_cat_init.shape[1]):\n",
    "            if not isinstance(X_cat_init[0, i], str):\n",
    "                self.ordinal_idx.append(i)\n",
    "            else:\n",
    "                if len(np.unique(X_cat_init[:, i])) == 2:\n",
    "                    self.one_hot_idx.append(i)\n",
    "                else:\n",
    "                    self.target_idx.append(i)\n",
    "        \n",
    "        self.not_ordinal_idx = [i for i in range(X_cat_init.shape[1]) if i not in self.ordinal_idx] \n",
    "        self.one_hot_cols = [self.categorical_colnames[i] for i in self.one_hot_idx]\n",
    "        self.target_enc_cols = [self.categorical_colnames[i] for i in self.target_idx]\n",
    "  \n",
    "        # (Temporarily) encode map categorical variables to integers for simulating missing values\n",
    "        # Torch is unable to handle strings.\n",
    "        X_cat_enc = self._map_categorical(X_cat_init)\n",
    "        \n",
    "        self.X = X_cat_enc\n",
    "\n",
    "        # Evaluate groundtruth coxPH\n",
    "        self.GT_weights, self.GT_concordance = self._fit_single_cox_PH(self.X_original, None).values()\n",
    "        \n",
    "    def _remove_event_and_duration(self, X):\n",
    "        \"\"\"\n",
    "        Remove event and duration columns from the data.\n",
    "        Re-set the indices for the categorical and numerical columns.\n",
    "        This is to avoid simulating missing values with these columns.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : pandas.DataFrame\n",
    "            Data matrix.\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        pandas.DataFrame\n",
    "            Data matrix with event and duration columns removed.\n",
    "        \"\"\"\n",
    "\n",
    "        X = X.drop(columns = [self.duration_col, self.event_col])\n",
    "\n",
    "        cat_idx = [X.columns.get_loc(col) for col in self.categorical_colnames]\n",
    "        self.categorical_cols = cat_idx\n",
    "        return X\n",
    "    \n",
    "    \n",
    "    def _add_event_and_duration(self, X, duration_col, event_col):\n",
    "        \"\"\"\n",
    "        Add event and duration columns to the data.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : pandas.DataFrame or np.ndarray\n",
    "            Data matrix.\n",
    "\n",
    "        duration_col : pandas.Series\n",
    "            Duration column.\n",
    "\n",
    "        event_col : pandas.Series\n",
    "            Event column.\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        pandas.DataFrame\n",
    "            Data matrix with event and duration columns added.\n",
    "        \"\"\"\n",
    "\n",
    "        if isinstance(X, pd.DataFrame):\n",
    "            # Add columns\n",
    "            X[self.duration_col] = duration_col\n",
    "            X[self.event_col] = event_col\n",
    "        else:\n",
    "            X = np.concatenate(X, [duration_col, event_col], axis = 1)\n",
    "        return X\n",
    "\n",
    "    def _fit_single_cox_PH(self, X_train, X_test, penalizer = 0):\n",
    "        \"\"\" \n",
    "        Fit cox PH model to the training data. Evaluates concordance on both train and test data.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X_train : pandas.DataFrame\n",
    "            Training data. Categories are mapped back to strings.\n",
    "\n",
    "        X_test : pandas.DataFrame\n",
    "            Test data. Categories are mapped back to strings.\n",
    "\n",
    "        penalizer : float\n",
    "            Regularization strength.\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        dict\n",
    "            Dictionary containing the weights and concordance index (train and test).\n",
    "\n",
    "        complete model\n",
    "            if fitting on the full dataset.\n",
    "        \"\"\"\n",
    "\n",
    "        me = TargetEncoder(smooth = \"auto\", target_type = \"continuous\", random_state=3155135)\n",
    "        sc = StandardScaler()\n",
    "        cph = CoxPHSurvivalAnalysis(alpha=penalizer)\n",
    "        util = Surv()\n",
    "\n",
    "        if X_test is None:\n",
    "            # Prepare X and y. This is the complete data.\n",
    "            # Define proper encodings\n",
    "            y = X_train[[self.event_col, self.duration_col]]\n",
    "            X = X_train.drop([self.event_col, self.duration_col], axis=1)\n",
    "            X = pd.get_dummies(X, columns = self.one_hot_cols, drop_first=True)\n",
    "            X[self.target_enc_cols] = me.fit_transform(X[self.target_enc_cols], y[self.duration_col])\n",
    "            y = y.to_records(index=False)\n",
    "            X_sc = sc.fit_transform(X)\n",
    "            X_sc = pd.DataFrame(X_sc, columns = sc.feature_names_in_)\n",
    "            cph.fit(X_sc, y)\n",
    "            weights = cph.coef_\n",
    "            weights = dict(zip(X_sc.columns, weights))\n",
    "            preds = cph.predict(X_sc)\n",
    "            conc = concordance_index_censored(event_indicator = y[self.event_col], event_time = y[self.duration_col], estimate = preds)[0]\n",
    "            conc = round(conc, 3)\n",
    "\n",
    "            # Lifelines summary of complete model\n",
    "            X = pd.get_dummies(X_train, columns = self.one_hot_cols, drop_first=True)\n",
    "            X[self.target_enc_cols] = me.fit_transform(X[self.target_enc_cols], y[self.duration_col])\n",
    "            # Temporarily remove y to not scale it\n",
    "            y = X[[self.event_col, self.duration_col]]\n",
    "            X = X.drop([self.event_col, self.duration_col], axis=1)\n",
    "            X_sc = sc.fit_transform(X)\n",
    "            X_sc = pd.DataFrame(X_sc, columns = sc.feature_names_in_)\n",
    "            X_sc[[self.event_col, self.duration_col]] = y\n",
    "\n",
    "            self.complete_model = CoxPHFitter(baseline_estimation_method='breslow', penalizer = penalizer)\n",
    "            self.complete_model.fit(X_sc, \n",
    "                               duration_col = self.duration_col, \n",
    "                               event_col= self.event_col,\n",
    "                               show_progress=False)\n",
    "\n",
    "            return {'weights': weights, \n",
    "                    'concordance': conc}\n",
    "\n",
    "        # One hot encode binary\n",
    "        X_train = pd.get_dummies(X_train, columns = self.one_hot_cols, drop_first = True)\n",
    "        X_test = pd.get_dummies(X_test, columns = self.one_hot_cols, drop_first = True)\n",
    "\n",
    "        # Ensure all category levels are present\n",
    "        for column in X_train.columns:\n",
    "            if column not in X_test.columns:\n",
    "                X_test[column] = 0\n",
    "        X_test  = X_test[X_train.columns]\n",
    "\n",
    "        # Technicality of converting to structured array\n",
    "        y_train = util.from_arrays(event = X_train[self.event_col], time = X_train[self.duration_col], name_event = self.event_col, name_time = self.duration_col)\n",
    "        y_test = util.from_arrays(event = X_test[self.event_col], time = X_test[self.duration_col], name_event = self.event_col, name_time = self.duration_col)\n",
    "        X_train = X_train.drop([self.event_col, self.duration_col], axis=1)\n",
    "        X_test = X_test.drop([self.event_col, self.duration_col], axis=1)\n",
    "\n",
    "        # Target encode train and transform test\n",
    "        X_train[self.target_enc_cols] = me.fit_transform(X_train[self.target_enc_cols], y_train[self.duration_col])\n",
    "        X_test[self.target_enc_cols] = me.transform(X_test[self.target_enc_cols])\n",
    "\n",
    "        # Scale X (not event and duration)\n",
    "        X_train_sc = sc.fit_transform(X_train)\n",
    "        X_train_sc = pd.DataFrame(X_train_sc, columns = sc.feature_names_in_)\n",
    "        X_test_sc = sc.transform(X_test)\n",
    "        X_test_sc = pd.DataFrame(X_test_sc, columns = sc.feature_names_in_)\n",
    "        \n",
    "        # For regularizing singular matrices\n",
    "        alpha_values = [0, 10, 100, 1000, 10000]\n",
    "        for alpha in alpha_values:\n",
    "            try:\n",
    "                cph = CoxPHSurvivalAnalysis(alpha = alpha)\n",
    "                cph.fit(X_train_sc, y_train)\n",
    "                preds_train = cph.predict(X_train_sc)\n",
    "                preds_test = cph.predict(X_test_sc)\n",
    "                \n",
    "                conc_train = concordance_index_censored(event_indicator = y_train[self.event_col], event_time = y_train[self.duration_col], estimate = preds_train)[0]\n",
    "                conc_test = concordance_index_censored(event_indicator = y_test[self.event_col], event_time = y_test[self.duration_col], estimate = preds_test)[0]\n",
    "                conc_train = round(conc_train, 3)\n",
    "                conc_test = round(conc_test, 3)\n",
    "                \n",
    "                weights = cph.coef_\n",
    "                weights = dict(zip(X_train.columns, weights))\n",
    "\n",
    "                # tbd add if regularising\n",
    "\n",
    "                return {'weights': weights, \n",
    "                'concordance_train': conc_train,\n",
    "                'concordance_test': conc_test}\n",
    "            \n",
    "            except Exception as e:\n",
    "                print(f\"Failed with alpha={alpha}: {e}\")\n",
    "\n",
    "    def complete_summary(self):\n",
    "        \"\"\" \n",
    "        Return a lifelines summary of cox ph of full data.\n",
    "        This is used because SciKit Survival does not have statistical information such as p-values etc.\n",
    "        \"\"\"\n",
    "\n",
    "        return self.complete_model.print_summary()\n",
    "    \n",
    "    def _map_categorical(self, X_cat):\n",
    "        \"\"\"\n",
    "        Temporarily map categorical variables to integers.\n",
    "        X_cat is assumed to be a numpy ndarray.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X_cat : np.ndarray\n",
    "            Categorical data matrix.\n",
    "    \n",
    "        Returns\n",
    "        -------\n",
    "        X_cat_enc : np.ndarray\n",
    "            Encoded categorical data matrix of integers.\n",
    "        \"\"\"\n",
    "        self.multi_le = MultiLabelEncoder()\n",
    "        \n",
    "        # Initialize an array of the same shape as X_cat to hold encoded data\n",
    "        X_cat_enc = np.empty(shape=X_cat.shape, dtype = \"object\")\n",
    "\n",
    "        # create a list of the columns that are not ordinal\n",
    "        encoded_columns = self.multi_le.fit_transform(X_cat[:, self.not_ordinal_idx])\n",
    "        X_cat_enc[:, self.not_ordinal_idx] = encoded_columns\n",
    "        X_cat_enc[:, self.ordinal_idx] = X_cat[:, self.ordinal_idx]\n",
    "        return X_cat_enc\n",
    "\n",
    "    def _decode_categorical(self, X_cat_enc):\n",
    "        \"\"\"\n",
    "        Decode categorical variables, excluding columns specified in self.ordinal_idx.\n",
    "        self.ordinal_idx contains indices of columns that should not be decoded because they are ordered.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X_cat_enc : np.ndarray\n",
    "            Encoded categorical data matrix of integers.\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        X_cat_dec : np.ndarray\n",
    "            Decoded categorical data matrix of strings.\n",
    "        \"\"\"\n",
    "\n",
    "        # Initialize an array of the same shape as X_cat_enc to hold decoded data\n",
    "        X_cat_dec = np.empty(shape=X_cat_enc.shape, dtype = \"object\")\n",
    "        decoded_columns = self.multi_le.inverse_transform(X_cat_enc[:, self.not_ordinal_idx])\n",
    "        X_cat_dec[:, self.not_ordinal_idx] = decoded_columns\n",
    "        X_cat_dec[:, self.ordinal_idx] = X_cat_enc[:, self.ordinal_idx]\n",
    "        return X_cat_dec\n",
    "     \n",
    "    def _simulate_single_na_dataset(self, p_miss, mecha = \"MCAR\", opt = None, p_obs = None, q = None,\n",
    "                                    sample_seed = 135135, column_seed = 115342):\n",
    "        \"\"\"\n",
    "        Generate missing values for specifics missing-data mechanism and proportion of missing values. \n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        p_miss : float\n",
    "            Proportion of missing values to generate for variables which will have missing values.\n",
    "\n",
    "        mecha : str, \n",
    "                Indicates the missing-data mechanism to be used. \"MCAR\" by default, \"MAR\" or \"MNAR\".\n",
    "\n",
    "        opt: str, \n",
    "             For mecha = \"MNAR\", it indicates how the missing-data mechanism is generated: using a logistic regression (\"logistic\"), \n",
    "             quantile censorship (\"quantile\") or logistic regression for generating a self-masked MNAR mechanism (\"selfmasked\").\n",
    "\n",
    "        p_obs : float\n",
    "                If mecha = \"MAR\", or mecha = \"MNAR\" with opt = \"logistic\" or \"quanti\", proportion of variables with *no* \n",
    "                missing values that will be used for the logistic masking model.\n",
    "\n",
    "        q : float\n",
    "            If mecha = \"MNAR\" and opt = \"quanti\", quantile level at which the cuts should occur.\n",
    "\n",
    "        sample_seed : int\n",
    "            Seed for the random number generator used to generate the missing samples.\n",
    "            Also used in train test split.\n",
    "\n",
    "        column_seed : int\n",
    "            Seed for the random number generator used to generate the missing columns.\n",
    "\n",
    "        Returns\n",
    "        ----------\n",
    "        A dictionary containing:\n",
    "        'X_init': the initial data matrix.\n",
    "        'X_na': a tensor (n, d) containing the dataset with missing values.\n",
    "        'mask': a tensor (n, d) containing the mask.\n",
    "        'event_and_duration': a tensor (n, 2) containing the event and duration columns.\n",
    "        \"\"\"\n",
    "\n",
    "        set_sample_seed(sample_seed)\n",
    "        set_column_seed(column_seed)\n",
    "\n",
    "        to_torch = torch.is_tensor(self.X) ## output a pytorch tensor, or a numpy array\n",
    "        if not to_torch:\n",
    "            X = self.X.astype(np.float32)\n",
    "            X = torch.from_numpy(X)\n",
    "        \n",
    "        if mecha == \"MAR\":\n",
    "            mask = MAR_mask(X, p_miss, p_obs).double()\n",
    "        elif mecha == \"MNAR\" and opt == \"logistic\":\n",
    "            mask = MNAR_mask_logistic(X, p_miss, p_obs).double()\n",
    "        elif mecha == \"MNAR\" and opt == \"quantile\":\n",
    "            mask = MNAR_mask_quantiles(X, p_miss, q, 1-p_obs).double()\n",
    "        elif mecha == \"MNAR\" and opt == \"selfmasked\":\n",
    "            mask = MNAR_self_mask_logistic(X, p_miss).double()\n",
    "        else:\n",
    "            mask = MCAR_mask(X, p_miss).double()\n",
    "        \n",
    "        X_nas = X.clone()\n",
    "        X_nas[mask.bool()] = np.nan\n",
    "\n",
    "        # Perform a train/test split on X_init, and use the same indices to split X_na and mask\n",
    "        duration_and_event = self.X_original[[self.duration_col, self.event_col]].to_numpy()\n",
    "\n",
    "        X_init_train, X_init_test, X_nas_train, X_nas_test, mask_train, mask_test, \\\n",
    "        event_and_duration_train, event_and_duration_test = train_test_split(X, X_nas, mask, duration_and_event, test_size=0.3, random_state = sample_seed)\n",
    "\n",
    "        X_init = {'train': X_init_train.double(), 'test': X_init_test.double()}\n",
    "        X_nas = {'train': X_nas_train.double(), 'test': X_nas_test.double()}\n",
    "        mask = {'train': mask_train.bool(), 'test': mask_test.bool()}\n",
    "        duration_and_event = {'train': event_and_duration_train, 'test': event_and_duration_test}\n",
    "        \n",
    "        return {'X_init': X_init, 'X_na': X_nas, 'mask': mask, 'event_and_duration': duration_and_event}\n",
    "    \n",
    "    def _simulate_M_na_datasets(self, M, p_miss, mecha = \"MCAR\", opt = None, p_obs = None, q = None,\n",
    "                               vary_cols = True, save = False, sample_seed = 135135, column_seed = 115342):\n",
    "        \"\"\"\n",
    "        Function to generate M datasets with missing values.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        M : int\n",
    "            Number of datasets to generate.\n",
    "        \n",
    "        vary_cols: bool\n",
    "            If True, the column_seed will vary.\n",
    "            This yields the most variation when simulating MAR because retained columns will vary. \n",
    "\n",
    "        See other params above\n",
    "\n",
    "        Returns\n",
    "        ----------\n",
    "        A dictionary containing:\n",
    "        'X_init': dictionary with train and test of shape (n_train, d, M), (n_test, d, M) respectively. \n",
    "            Contains the initial data matrix, but split into train and test.\n",
    "        'X_na': dictionary with train and test of shape (n_train, d, M), (n_test, d, M) respectively.\n",
    "            Contains the dataset with simulated missing values, but split into train and test.\n",
    "        'mask': dictionary with train and test of shape (n_train, d, M), (n_test, d, M)\n",
    "            Contains the mask, but split into train and test.\n",
    "        'event_and_duration': dictionary with train and test of shape (n_train, 2, M), (n_test, 2, M)\n",
    "        \"\"\"\n",
    "\n",
    "        to_torch = torch.is_tensor(self.X) ## output a pytorch tensor, or a numpy array\n",
    "        if not to_torch:\n",
    "            X = self.X.astype(np.float32)\n",
    "            X = torch.from_numpy(X)\n",
    "\n",
    "        # Set the dimensions used for initiating tensors\n",
    "        temporary_train, temporary_test = train_test_split(X, test_size=0.3, random_state = sample_seed)\n",
    "        n_train = temporary_train.shape[0]\n",
    "        n_test = temporary_test.shape[0]\n",
    "        n = X.shape[0]\n",
    "        d = X.shape[1]\n",
    "\n",
    "        # Initialize empty tensor for the data with missing values and the mask\n",
    "        event_and_duration_train = np.empty((n_train, 2, M), dtype = object)\n",
    "        event_and_duration_test = np.empty((n_test, 2, M), dtype = object)\n",
    "        \n",
    "        X_init_tensor_train = torch.empty((n_train, d,  M))\n",
    "        X_init_tensor_test = torch.empty((n_test, d, M))\n",
    "\n",
    "        X_na_tensor_train = torch.empty((n_train, d, M))\n",
    "        X_na_tensor_test = torch.empty((n_test, d, M))\n",
    "\n",
    "        X_mask_tensor_train = torch.empty((n_train, d, M), dtype = torch.bool)\n",
    "        X_mask_tensor_test = torch.empty((n_test, d, M), dtype = torch.bool)\n",
    "\n",
    "        for i in range(M):\n",
    "            X_init, X_na, X_mask, event_and_duration = self._simulate_single_na_dataset(p_miss = p_miss,\n",
    "                                                                    mecha = mecha,\n",
    "                                                                    p_obs = p_obs,\n",
    "                                                                    sample_seed = sample_seed,\n",
    "                                                                    column_seed = column_seed).values()\n",
    "            \n",
    "            event_and_duration_train[:, :, i] = event_and_duration['train']\n",
    "            event_and_duration_test[:, :, i] = event_and_duration['test']\n",
    "            \n",
    "            X_init_tensor_train[:, :, i] = X_init['train'].double()\n",
    "            X_init_tensor_test[:, :, i] = X_init['test'].double()\n",
    "\n",
    "            X_na_tensor_train[:, :, i] = X_na['train'].double()\n",
    "            X_na_tensor_test[:, :, i] = X_na['test'].double()\n",
    "\n",
    "            X_mask_tensor_train[:, :, i] = X_mask['train'].bool()\n",
    "            X_mask_tensor_test[:, :, i] = X_mask['test'].bool()\n",
    "\n",
    "            # Change seeds\n",
    "            sample_seed += 1\n",
    "            if vary_cols:\n",
    "                column_seed += 1\n",
    "\n",
    "        if save:\n",
    "            colnames = np.array(self.categorical_colnames)\n",
    "\n",
    "            np.save(\"../../data/R/simulated_datasets/colnames\", colnames)\n",
    "\n",
    "            X_init_tensor_train = X_init_tensor_train.numpy()\n",
    "            X_init_tensor_train = X_init_tensor_train.astype(object)\n",
    "            X_init_tensor_test = X_init_tensor_test.numpy()\n",
    "            X_init_tensor_test = X_init_tensor_test.astype(object)\n",
    "\n",
    "            X_na_tensor_train = X_na_tensor_train.numpy()\n",
    "            X_na_tensor_test = X_na_tensor_test.numpy() \n",
    "\n",
    "            X_mask_tensor_train = X_mask_tensor_train.numpy()\n",
    "            X_mask_tensor_train = X_mask_tensor_train.astype(bool)\n",
    "            X_mask_tensor_test = X_mask_tensor_test.numpy()\n",
    "            X_mask_tensor_test = X_mask_tensor_test.astype(bool)\n",
    "\n",
    "            X_init_cat_train = X_init_tensor_train\n",
    "            X_init_cat_test = X_init_tensor_test\n",
    "\n",
    "            X_na_cat_train = X_na_tensor_train\n",
    "            X_na_cat_test = X_na_tensor_test\n",
    "\n",
    "            # replace all nan in X_na_cat with np.nan. Prevent overflows bug\n",
    "            X_na_cat_train = np.where(np.isnan(X_na_cat_train), np.nan, X_na_cat_train)\n",
    "            X_na_cat_test = np.where(np.isnan(X_na_cat_test), np.nan, X_na_cat_test)\n",
    "            X_na_cat_train = X_na_cat_train.astype(object)\n",
    "            X_na_cat_test = X_na_cat_test.astype(object)\n",
    "\n",
    "            for i in range(M):\n",
    "                X_init_cat_train[:, :, i] = self._decode_categorical(X_init_cat_train[:, :, i])\n",
    "                X_init_cat_test[:, :, i] = self._decode_categorical(X_init_cat_test[:, :, i])\n",
    "                X_na_cat_train[:, :, i] = self._decode_categorical(X_na_cat_train[:, :, i])\n",
    "                X_na_cat_test[:, :, i] = self._decode_categorical(X_na_cat_test[:, :, i])\n",
    "\n",
    "            X_init_train = X_init_cat_train\n",
    "            X_init_test = X_init_cat_test\n",
    "            X_na_train = X_na_cat_train\n",
    "            X_na_test = X_na_cat_test\n",
    "\n",
    "            if mecha == \"MCAR\":\n",
    "                np.save(f\"../../data/R/simulated_datasets/mcar/X_init_train_M{M}p_miss{p_miss}\", X_init_train)\n",
    "                np.save(f\"../../data/R/simulated_datasets/mcar/X_init_test_M{M}p_miss{p_miss}\", X_init_test)\n",
    "                np.save(f\"../../data/R/simulated_datasets/mcar/X_na_train_M{M}p_miss{p_miss}\", X_na_train)\n",
    "                np.save(f\"../../data/R/simulated_datasets/mcar/X_na_test_M{M}p_miss{p_miss}\", X_na_test)\n",
    "                np.save(f\"../../data/R/simulated_datasets/mcar/X_mask_train_M{M}p_miss{p_miss}\", X_mask_tensor_train)\n",
    "                np.save(f\"../../data/R/simulated_datasets/mcar/X_mask_test_M{M}p_miss{p_miss}\", X_mask_tensor_test)\n",
    "                np.save(f\"../../data/R/simulated_datasets/mcar/event_and_dur_train_M{M}p_miss{p_miss}\", event_and_duration_train)\n",
    "                np.save(f\"../../data/R/simulated_datasets/mcar/event_and_dur_test_M{M}p_miss{p_miss}\", event_and_duration_test)\n",
    "            elif mecha == \"MAR\":\n",
    "                p_obs = round(p_obs, 4)\n",
    "                np.save(f\"../../data/R/simulated_datasets/mar/X_init_train_M{M}p_miss{p_miss}p_obs{p_obs}\", X_init_train)\n",
    "                np.save(f\"../../data/R/simulated_datasets/mar/X_init_test_M{M}p_miss{p_miss}p_obs{p_obs}\", X_init_test)\n",
    "                np.save(f\"../../data/R/simulated_datasets/mar/X_na_train_M{M}p_miss{p_miss}p_obs{p_obs}\", X_na_train)\n",
    "                np.save(f\"../../data/R/simulated_datasets/mar/X_na_test_M{M}p_miss{p_miss}p_obs{p_obs}\", X_na_test)\n",
    "                np.save(f\"../../data/R/simulated_datasets/mar/X_mask_train_M{M}p_miss{p_miss}p_obs{p_obs}\", X_mask_tensor_train)\n",
    "                np.save(f\"../../data/R/simulated_datasets/mar/X_mask_test_M{M}p_miss{p_miss}p_obs{p_obs}\", X_mask_tensor_test)\n",
    "                np.save(f\"../../data/R/simulated_datasets/mar/event_and_dur_train_M{M}p_miss{p_miss}p_obs{p_obs}\", event_and_duration_train)\n",
    "                np.save(f\"../../data/R/simulated_datasets/mar/event_and_dur_test_M{M}p_miss{p_miss}p_obs{p_obs}\", event_and_duration_test)\n",
    "            return None\n",
    "\n",
    "        X_init = {'train': X_init_tensor_train, 'test': X_init_tensor_test}\n",
    "        X_na = {'train': X_na_tensor_train, 'test': X_na_tensor_test}\n",
    "        mask = {'train': X_mask_tensor_train, 'test': X_mask_tensor_test}\n",
    "        event_and_duration = {'train': event_and_duration_train, 'test': event_and_duration_test}\n",
    "\n",
    "        return {'X_init': X_init, 'X_na': X_na, 'mask': mask, 'event_and_duration': event_and_duration}\n",
    "\n",
    "    \n",
    "    def _evaluate_bias_weights(self, weights):\n",
    "        \"\"\"\n",
    "        Takes in weights of the ith dataset in M and calculates the absolute bias for each feature.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        weights : dict\n",
    "            Dictionary containing the weights of the cox PH model.\n",
    "            Key is the feature name and value is the weight.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        abs_bias : dict\n",
    "            Dictionary containing the absolute bias for each feature.\n",
    "        \"\"\"\n",
    "\n",
    "        GT_weights = self.GT_weights\n",
    "        abs_bias = {}\n",
    "        for key, value in weights.items():\n",
    "            abs_bias[key] = (value - GT_weights[key])/abs(GT_weights[key])\n",
    "        return abs_bias\n",
    "    \n",
    "    @staticmethod\n",
    "    def convert_datatypes(arr):\n",
    "        \"\"\"\n",
    "        Convert the data types of the numpy arrays to the correct types.\n",
    "        \"\"\"\n",
    "        shape = arr.shape\n",
    "        \n",
    "        # Flatten the array to iterate over it\n",
    "        flat_arr = arr.flatten()\n",
    "        \n",
    "        converted = []\n",
    "        for item in flat_arr:\n",
    "            try:\n",
    "                converted.append(float(item))\n",
    "            except ValueError:\n",
    "                converted.append(item)\n",
    "        return np.array(converted, dtype=object).reshape(shape)\n",
    "\n",
    "    @staticmethod\n",
    "    def _map_p_obs(p_obs):\n",
    "        map_obs = {'0.888889': '8/9',\n",
    "                '0.777778': '7/9',\n",
    "                '0.666667': '6/9',\n",
    "                '0.555556': '5/9',\n",
    "                '0.444444': '4/9'}\n",
    "\n",
    "        # Given p_obs is the float, convert to string representation from the dict\n",
    "        p_obs = map_obs[str(p_obs)]\n",
    "    \n",
    "    @staticmethod\n",
    "    def mean_and_std_list(input_list):\n",
    "        means = [x[0] for x in input_list]\n",
    "        stds = [x[1] for x in input_list]\n",
    "\n",
    "        mean_of_means = np.mean(means)\n",
    "        mean_of_stds = np.mean(stds)\n",
    "\n",
    "        result = (mean_of_means, mean_of_stds)\n",
    "        return result\n",
    "    \n",
    "    @staticmethod\n",
    "    def mean_and_std_df(column):\n",
    "        \"\"\"\n",
    "        Given a datframe with tuple pairs (mean, std) as rows, extract the expected\n",
    "        mean and std.\n",
    "        \"\"\"\n",
    "\n",
    "        means = [x[0] for x in column]\n",
    "        stds = [x[1] for x in column]\n",
    "        return (np.mean(means), np.mean(stds))\n",
    "    \n",
    "class MiceSimulationMCAR(MiceSimulationMV):\n",
    "    \"\"\"\n",
    "    Subclass of MicecSimulationMV to simulate MCAR missing values and impute with MICE.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, X, duration_col, event_col, cat_colnames):\n",
    "        super().__init__(X, duration_col, event_col, cat_colnames)\n",
    "\n",
    "    def _simulate_MCAR_dataset(self, M, p_miss, save, sample_seed = 135135, column_seed = 115342):\n",
    "        \"\"\"\n",
    "        Simulate M datasets with MCAR missing values. See parent class for more details.\n",
    "        \"\"\"\n",
    "\n",
    "        self._simulate_M_na_datasets(M = M,\n",
    "                                     p_miss = p_miss,\n",
    "                                     mecha = \"MCAR\",\n",
    "                                     sample_seed = sample_seed,\n",
    "                                     column_seed = column_seed,\n",
    "                                     save = save)\n",
    "        \n",
    "    def _generate_mice_datasets(self, M, p_miss = [0.1, 0.2, 0.3, 0.4, 0.5]):\n",
    "        \"\"\"\n",
    "        Generate datasets to save to disk for R to read.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        M : int\n",
    "            Number of datasets to generate.\n",
    "\n",
    "        p_miss : list\n",
    "            List of proportions of missing values to generate for variables which will have missing values.\n",
    "        \"\"\"\n",
    "\n",
    "        self.p_miss = p_miss\n",
    "        self.M = M\n",
    "\n",
    "        # Get mean and std of each p_miss\n",
    "        for miss in p_miss:\n",
    "            self._simulate_MCAR_dataset(M = M, p_miss = miss, save = True)\n",
    "\n",
    "    def _read_single_simulated_dataset(self, M, p_miss):\n",
    "        \"\"\" \n",
    "        Method to read from disk the simulated, unimputed, datasets used for imputing with MICE. \n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        M : int\n",
    "            Number of datasets to generate.\n",
    "        \n",
    "        p_miss : float\n",
    "            Proportion of missing values to generate for variables which will have missing values.\n",
    "        \"\"\"\n",
    "        X_init_train = np.load(f\"../../data/R/simulated_datasets/mcar/X_init_train_M{M}p_miss{p_miss}.npy\", allow_pickle=True)\n",
    "        X_init_test = np.load(f\"../../data/R/simulated_datasets/mcar/X_init_test_M{M}p_miss{p_miss}.npy\", allow_pickle=True)\n",
    "        X_mask_train = np.load(f\"../../data/R/simulated_datasets/mcar/X_mask_train_M{M}p_miss{p_miss}.npy\", allow_pickle=True)\n",
    "        X_mask_test = np.load(f\"../../data/R/simulated_datasets/mcar/X_mask_test_M{M}p_miss{p_miss}.npy\", allow_pickle=True)\n",
    "        event_and_duration_train = np.load(f\"../../data/R/simulated_datasets/mcar/event_and_dur_train_M{M}p_miss{p_miss}.npy\", allow_pickle=True)\n",
    "        event_and_duration_test = np.load(f\"../../data/R/simulated_datasets/mcar/event_and_dur_test_M{M}p_miss{p_miss}.npy\", allow_pickle=True)\n",
    "        \n",
    "        X_init = {'train': X_init_train, 'test': X_init_test}\n",
    "        X_mask = {'train': X_mask_train, 'test': X_mask_test}\n",
    "        event_and_duration = {'train': event_and_duration_train, 'test': event_and_duration_test}\n",
    "\n",
    "        return {'X_init': X_init, 'X_mask': X_mask, 'event_and_duration': event_and_duration}\n",
    "    \n",
    "    def _read_single_mice_tensor(self, p_miss):\n",
    "        \"\"\" \n",
    "        Read the train and test tensors representing mice imputations. \n",
    "        They are of shape (n_train, d, G, M) or (n_test, d, G, M) respectively.\n",
    "        G represents the number of imputed datasets by MICE, and M is the number of datasets generated with missing values.  \n",
    "        \"\"\"\n",
    "\n",
    "        X_imp_train = np.load(f\"../../data/R/imputed_datasets/mcar/X_mice_tensor_train_G{self.G}_p_miss{p_miss}.npy\", allow_pickle=True)\n",
    "        X_imp_test = np.load(f\"../../data/R/imputed_datasets/mcar/X_mice_tensor_test_G{self.G}_p_miss{p_miss}.npy\", allow_pickle=True)\n",
    "        X_imp_train = self.convert_datatypes(X_imp_train)\n",
    "        X_imp_test = self.convert_datatypes(X_imp_test)\n",
    "        return {\"train\": X_imp_train, \"test\": X_imp_test}\n",
    "    \n",
    "    def _evaluate_single_mice_acc(self, X_imp, X_true, X_mask):\n",
    "        \"\"\" \n",
    "        Given a jth simulated dataset, evaluate the accuracy of the MICE imputations. \n",
    "        \"\"\"\n",
    "\n",
    "        X_imp_cat = X_imp\n",
    "        X_true_cat = X_true\n",
    "        X_mask_cat = X_mask\n",
    "\n",
    "        acc = {\"categorical\": {\"mice\": []}}\n",
    "\n",
    "        for i in range(X_imp.shape[2]):\n",
    "            acc[\"categorical\"][\"mice\"].append(accuracy(X_imp = X_imp_cat[:, :, i],\n",
    "                                                    X_true = X_true_cat[:, :, i],\n",
    "                                                    mask = X_mask_cat[:, :, i]))\n",
    "        for key, value in acc.items():\n",
    "            for k, v in value.items():\n",
    "                acc[key][k] = np.matrix(v)\n",
    "                acc[key][k] = (acc[key][k].mean().round(4), acc[key][k].std().round(4))\n",
    "        return acc\n",
    "                \n",
    "    def _evaluate_mice_coxPH(self, X_imp_train, X_imp_test, event_and_duration_train, event_and_duration_test):\n",
    "        \"\"\"\n",
    "        Evaluate the multivariate cox PH model on the multiple imputed datasets.\n",
    "        Data is already decoded.\n",
    "        \"\"\"\n",
    "\n",
    "        c_index_train = []\n",
    "        c_index_test = []\n",
    "        bias = []\n",
    "\n",
    "        # concatinate X_train and event and duration\n",
    "        X_imp_train = np.concatenate((X_imp_train, event_and_duration_train), axis = 1)\n",
    "        X_imp_test = np.concatenate((X_imp_test, event_and_duration_test), axis = 1)\n",
    "        \n",
    "        for i in range(X_imp_train.shape[2]):\n",
    "            data_train = pd.DataFrame(X_imp_train[:, :, i], columns = self.categorical_colnames + [self.duration_col, self.event_col])\n",
    "            data_test = pd.DataFrame(X_imp_test[:, :, i], columns = self.categorical_colnames + [self.duration_col, self.event_col])\n",
    "            weights, conc_train, conc_test = self._fit_single_cox_PH(data_train, data_test).values()\n",
    "            c_index_train.append(conc_train)\n",
    "            c_index_test.append(conc_test)\n",
    "            \n",
    "            bias_weights = self._evaluate_bias_weights(weights)\n",
    "            bias.append(bias_weights)\n",
    "\n",
    "        c_index_train = (np.array(c_index_train).mean().round(3), np.array(c_index_train).std().round(3))\n",
    "        c_index_test = (np.array(c_index_test).mean().round(3), np.array(c_index_test).std().round(3))\n",
    "\n",
    "        bias_df = pd.DataFrame(bias)\n",
    "        bias = dict(zip(bias_df.columns, zip(round(bias_df.mean(),4), round(bias_df.std(), 4))))\n",
    "\n",
    "        return {'c_index_train': c_index_train, 'c_index_test': c_index_test, 'bias': bias}\n",
    "\n",
    "    def _run_mice_imputations(self, M, p_miss):\n",
    "        \"\"\"\n",
    "        Run MICE imputation on M datasets with MCAR missing values.\n",
    "        Assumes method self._generate_mice_datasets(.) has been ran.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        M : int\n",
    "            Number of datasets to generate.\n",
    "\n",
    "        p_miss : float\n",
    "            Proportion of missing values to generate for variables which will have missing values.\n",
    "\n",
    "        Returns\n",
    "        ----------\n",
    "        dict\n",
    "            Dictionary containing train and test accuracy, concordance index and bias of the imputed datasets.\n",
    "        \"\"\"\n",
    "\n",
    "        X_init = self._read_single_simulated_dataset(M, p_miss)[\"X_init\"]\n",
    "        X_mask = self._read_single_simulated_dataset(M, p_miss)[\"X_mask\"]\n",
    "        event_and_duration = self._read_single_simulated_dataset(M, p_miss)[\"event_and_duration\"]\n",
    "        \n",
    "        X_init_train = X_init['train']\n",
    "        X_init_test = X_init['test']\n",
    "        X_mask_train = X_mask['train']\n",
    "        X_mask_test = X_mask['test']\n",
    "        event_and_duration_train = event_and_duration['train']           \n",
    "        event_and_duration_test = event_and_duration['test']\n",
    "\n",
    "        # Recall imputations are tensors of dimensions (n, d, g, M)\n",
    "        imputed_train, imputed_test = self._read_single_mice_tensor(p_miss).values()\n",
    "        \n",
    "        M = imputed_train.shape[3]\n",
    "        g = imputed_train.shape[2]\n",
    "\n",
    "        # We need g copies of the initial data and the mask because we have g imputed datasets\n",
    "        # Thus, we reshape the initial data and the mask to (n, d, G, M)\n",
    "        X_init_train = np.repeat(X_init_train[:, :, np.newaxis], g, axis=2)\n",
    "        X_init_test = np.repeat(X_init_test[:, :, np.newaxis], g, axis=2)\n",
    "\n",
    "        X_mask_train = np.repeat(X_mask_train[:, :, np.newaxis], g, axis=2)\n",
    "        X_mask_test = np.repeat(X_mask_test[:, :, np.newaxis], g, axis=2)\n",
    "\n",
    "        event_and_duration_train = np.repeat(event_and_duration_train[:, :, np.newaxis], g, axis=2)\n",
    "        event_and_duration_test = np.repeat(event_and_duration_test[:, :, np.newaxis], g, axis=2)\n",
    "\n",
    "        acc_train = {\"categorical\": {\"mice\": []}}\n",
    "        \n",
    "        acc_test = {\"categorical\": {\"mice\": []}}\n",
    "\n",
    "        c_index_train = []\n",
    "        c_index_test = []\n",
    "        bias = []\n",
    "        \n",
    "        for i in range(M):\n",
    "            # accuracy\n",
    "            acc_dict_train = self._evaluate_single_mice_acc(imputed_train[:, :, :, i], X_init_train[:, :, :, i], X_mask_train[:, :, :, i])\n",
    "            acc_dict_test = self._evaluate_single_mice_acc(imputed_test[:, :, :, i], X_init_test[:, :, :, i], X_mask_test[:, :, :, i])\n",
    "\n",
    "            acc_train[\"categorical\"][\"mice\"].append((acc_dict_train[\"categorical\"][\"mice\"][0], acc_dict_train[\"categorical\"][\"mice\"][1]))\n",
    "            acc_test[\"categorical\"][\"mice\"].append((acc_dict_test[\"categorical\"][\"mice\"][0], acc_dict_test[\"categorical\"][\"mice\"][1]))\n",
    "\n",
    "            # Bias and c-index\n",
    "            conc_train, conc_test, weight_bias = self._evaluate_mice_coxPH(imputed_train[:, :, :, i], imputed_test[:, :, :, i], \\\n",
    "                                                                          event_and_duration_train[:, :, :, i], event_and_duration_test[:, :, :, i]).values()\n",
    "            c_index_train.append(conc_train)\n",
    "            c_index_test.append(conc_test)\n",
    "            bias.append(weight_bias)\n",
    "\n",
    "        # Take average of mean and std\n",
    "        for key, value in acc_train.items():\n",
    "            for k, v in value.items():\n",
    "                acc_train[key][k] = np.matrix(v)\n",
    "                acc_train[key][k] = acc_train[key][k].mean(axis = 0).round(4).flatten()\n",
    "\n",
    "        for key, value in acc_test.items():\n",
    "            for k, v in value.items():\n",
    "                acc_test[key][k] = np.matrix(v)\n",
    "                acc_test[key][k] = acc_test[key][k].mean(axis = 0).round(4).flatten()\n",
    "\n",
    "        c_index_train = self.mean_and_std_list(c_index_train)\n",
    "        c_index_test = self.mean_and_std_list(c_index_test)\n",
    "\n",
    "        bias_df = pd.DataFrame(bias)\n",
    "        bias = {col: self.mean_and_std_df(bias_df[col]) for col in bias_df.columns}\n",
    "\n",
    "        train = {\"acc\": acc_train, \"c_index\": c_index_train, \"bias\": bias}\n",
    "        test = {\"acc\": acc_test, \"c_index\": c_index_test, \"bias\": bias}\n",
    "        return {\"train:\": train, \"test\": test}\n",
    "\n",
    "    def _get_mice_acc(self, acc_in):\n",
    "        \"\"\"\n",
    "        Get the MICE accuracy for the imputed datasets.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        acc_in : dict\n",
    "            Dictionary containing the accuracy for the imputed datasets.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        pd.DataFrame\n",
    "            DataFrame containing the accuracy for the imputed datasets of each p_miss.\n",
    "        \"\"\"\n",
    "\n",
    "        acc_cols = pd.MultiIndex.from_tuples([\n",
    "                        ('Categorical', 'MICE')\n",
    "                        ])\n",
    "        acc_row = []\n",
    "\n",
    "        for miss in self.p_miss:\n",
    "            acc = acc_in[self.p_miss.index(miss)]\n",
    "        \n",
    "            mice_mean_cat = acc['categorical']['mice'][0]\n",
    "            mice_std_cat = acc['categorical']['mice'][1]\n",
    "\n",
    "            row = [f\"{mice_mean_cat:.4f} ± {mice_std_cat:.4f}\"]\n",
    "            acc_row.append(row)\n",
    "        \n",
    "        acc_out = pd.DataFrame(acc_row, index = self.p_miss , columns = acc_cols)\n",
    "        acc_out.index.name = 'p_miss'\n",
    "\n",
    "        return acc_out\n",
    "    \n",
    "    def _get_mice_c_index(self, c_index_in):\n",
    "        \"\"\"\n",
    "        Get the multivariate concordance index for the imputed datasets.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        c_index_in : list\n",
    "            List containing the concordance index for the imputed datasets.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        pd.DataFrame\n",
    "            DataFrame containing the concordance index for the imputed datasets of each p_miss.\n",
    "        \"\"\"\n",
    "\n",
    "        column = ['mice']\n",
    "        conc_row = []\n",
    "\n",
    "        for miss in self.p_miss:\n",
    "            conc = c_index_in[self.p_miss.index(miss)]\n",
    "            mice_mean = conc[0]\n",
    "            mice_std = conc[1]\n",
    "            row = [f\"{mice_mean:.4f} ± {mice_std:.4f}\"]\n",
    "            conc_row.append(row)\n",
    "\n",
    "        conc_out = pd.DataFrame(conc_row, index = self.p_miss , columns = column)\n",
    "        conc_out.index.name = 'p_miss'\n",
    "\n",
    "        return conc_out\n",
    "\n",
    "    def _get_mice_bias(self):\n",
    "        \"\"\"\n",
    "        Get the mice bias for the imputed datasets.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        pd.DataFrame\n",
    "            DataFrame containing the bias for the imputed datasets of each p_miss.\n",
    "        \"\"\"\n",
    "\n",
    "        feature_cols = list(self.bias[0].keys())\n",
    "        bias_row = []\n",
    "\n",
    "        for miss in self.p_miss:\n",
    "            mice_bias = list(self.bias[self.p_miss.index(miss)].values())\n",
    "\n",
    "            mice_row = []\n",
    "            for column_index in range(len(mice_bias)):\n",
    "                mice_bias_mean = mice_bias[column_index][0]\n",
    "                mice_bias_std = mice_bias[column_index][1]\n",
    "                mice_row.append(f\"{mice_bias_mean:.4f} ± {mice_bias_std:.4f}\")\n",
    "            row = mice_row\n",
    "            bias_row.append(row)\n",
    "\n",
    "        bias = pd.DataFrame(bias_row, index = self.p_miss , columns = feature_cols)\n",
    "        bias.index.name = 'p_miss'\n",
    "        return bias\n",
    "\n",
    "    def simulate_mice(self, M, G, p_miss = [0.1, 0.2, 0.3, 0.4, 0.5]):\n",
    "        \"\"\"\n",
    "        Run a simulation with mice imputations.\n",
    "        \"\"\"\n",
    "        self.p_miss = p_miss\n",
    "        self.G = G\n",
    "\n",
    "        self.acc_train = []\n",
    "        self.acc_test = []\n",
    "        self.conc_train = []\n",
    "        self.conc_test = []\n",
    "        self.bias = []\n",
    "\n",
    "        for miss in self.p_miss:\n",
    "            print(f\"Currently simulating for p_miss: {miss}\")\n",
    "            train, test = self._run_mice_imputations(M = M, p_miss = miss).values()\n",
    "\n",
    "            acc_train = train['acc']\n",
    "            conc_train = train['c_index']\n",
    "            acc_test = test['acc']\n",
    "            conc_test = test['c_index']\n",
    "            bias = train['bias']\n",
    "                    \n",
    "            self.acc_train.append(acc_train)\n",
    "            self.acc_test.append(acc_test)\n",
    "            self.conc_train.append(conc_train)\n",
    "            self.conc_test.append(conc_test)\n",
    "            self.bias.append(bias)\n",
    "\n",
    "    def get_mice_results(self):\n",
    "        \"\"\" \n",
    "        Get the results of the mice imputation simulation.\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        dict\n",
    "            A dictionary containing the accuracy, concordance index and bias for the multivariate imputation simulation.\n",
    "            The dictionary values are dataframes.\n",
    "        \"\"\"\n",
    "\n",
    "        acc_train, acc_test = self._get_mice_acc(self.acc_train), self._get_mice_acc(self.acc_test)\n",
    "        conc_train, conc_test = self._get_mice_c_index(self.conc_train), self._get_mice_c_index(self.conc_test)\n",
    "        bias = self._get_mice_bias()\n",
    "\n",
    "        return {'acc_train': acc_train, 'acc_test': acc_test, 'c_index_train': conc_train, 'c_index_test': conc_test, 'bias': bias}\n",
    "\n",
    "class MiceSimulationMAR(MiceSimulationMV):\n",
    "    \"\"\"\n",
    "    Subclass of MicecSimulationMV to simulate MAR missing values and impute with MICE.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, X, duration_col, event_col, cat_colnames):\n",
    "        super().__init__(X, duration_col, event_col, cat_colnames)\n",
    "\n",
    "    def _simulate_MAR_dataset(self, M, p_miss, p_obs, save, sample_seed = 135135, column_seed = 115342):\n",
    "        \"\"\"\n",
    "        Simulate M datasets with MAR missing values. See parent class for more details.\n",
    "        \"\"\"\n",
    "\n",
    "        self._simulate_M_na_datasets(M = M,\n",
    "                                     p_miss = p_miss,\n",
    "                                     p_obs = p_obs,\n",
    "                                     mecha = \"MAR\",\n",
    "                                     sample_seed = sample_seed,\n",
    "                                     column_seed = column_seed,\n",
    "                                     save = save)\n",
    "        \n",
    "    def _generate_mice_datasets(self, M, p_miss = [0.1, 0.2, 0.3, 0.4, 0.5], p_obs = [8/9, 7/9, 6/9, 5/9, 4/9]):\n",
    "        \"\"\"\n",
    "        Generate datasets to save to disk for R to read.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        M : int\n",
    "            Number of datasets to generate.\n",
    "\n",
    "        p_miss : list\n",
    "            List of proportions of missing values to generate for variables which will have missing values.\n",
    "\n",
    "        p_obs : list\n",
    "            List of proportions of missing values to retain.\n",
    "        \"\"\"\n",
    "\n",
    "        for miss in p_miss:\n",
    "            for obs in p_obs:\n",
    "                self._simulate_MAR_dataset(M = M, p_miss = miss, p_obs = obs, save = True)\n",
    "\n",
    "    def _read_single_simulated_dataset(self, M, p_miss, p_obs):\n",
    "        \"\"\" \n",
    "        Method to read from disk the simulated, unimputed, datasets used for imputing with MICE. \n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        M : int\n",
    "            Number of datasets to generate.\n",
    "        \n",
    "        p_miss : float\n",
    "            Proportion of missing values to generate for variables which will have missing values.\n",
    "\n",
    "        p_obs : float\n",
    "            Proportion of missing values to retain.\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        dict\n",
    "            Dictionary containing the train and test datasets, the mask and the event and duration.\n",
    "        \"\"\"\n",
    "\n",
    "        X_init_train = np.load(f\"../../data/R/simulated_datasets/mar/X_init_train_M{M}p_miss{p_miss}p_obs{p_obs}.npy\", allow_pickle=True)\n",
    "        X_init_test = np.load(f\"../../data/R/simulated_datasets/mar/X_init_test_M{M}p_miss{p_miss}p_obs{p_obs}.npy\", allow_pickle=True)\n",
    "        X_mask_train = np.load(f\"../../data/R/simulated_datasets/mar/X_mask_train_M{M}p_miss{p_miss}p_obs{p_obs}.npy\", allow_pickle=True)\n",
    "        X_mask_test = np.load(f\"../../data/R/simulated_datasets/mar/X_mask_test_M{M}p_miss{p_miss}p_obs{p_obs}.npy\", allow_pickle=True)\n",
    "        event_and_duration_train = np.load(f\"../../data/R/simulated_datasets/mar/event_and_dur_train_M{M}p_miss{p_miss}p_obs{p_obs}.npy\", allow_pickle=True)\n",
    "        event_and_duration_test = np.load(f\"../../data/R/simulated_datasets/mar/event_and_dur_test_M{M}p_miss{p_miss}p_obs{p_obs}.npy\", allow_pickle=True)\n",
    "        \n",
    "        X_init = {'train': X_init_train, 'test': X_init_test}\n",
    "        X_mask = {'train': X_mask_train, 'test': X_mask_test}\n",
    "        event_and_duration = {'train': event_and_duration_train, 'test': event_and_duration_test}\n",
    "\n",
    "        return {'X_init': X_init, 'X_mask': X_mask, 'event_and_duration': event_and_duration}\n",
    "    \n",
    "    def _read_single_mice_tensor(self, p_miss, p_obs):\n",
    "        \"\"\" \n",
    "        Read the train and test tensors representing mice imputations. \n",
    "        They are of shape (n_train, d, G, M) or (n_test, d, G, M) respectively.\n",
    "        G represents the number of imputed datasets by MICE, and M is the number of datasets generated with missing values.  \n",
    "        \"\"\"\n",
    "\n",
    "        X_imp_train = np.load(f\"../../data/R/imputed_datasets/mar/X_mice_tensor_train_G{self.G}_p_miss{p_miss}p_obs{p_obs}.npy\", allow_pickle=True)\n",
    "        X_imp_test = np.load(f\"../../data/R/imputed_datasets/mar/X_mice_tensor_test_G{self.G}_p_miss{p_miss}p_obs{p_obs}.npy\", allow_pickle=True)\n",
    "\n",
    "        X_imp_train = self.convert_datatypes(X_imp_train)\n",
    "        X_imp_test = self.convert_datatypes(X_imp_test)\n",
    "        return {\"train\": X_imp_train, \"test\": X_imp_test}\n",
    "    \n",
    "    def _evaluate_single_mice_acc(self, X_imp, X_true, X_mask):\n",
    "        \"\"\" \n",
    "        Given a jth simulated dataset, evaluate the accuracy of the MICE imputations. \n",
    "        \"\"\"\n",
    "\n",
    "        X_imp_cat = X_imp\n",
    "        X_true_cat = X_true\n",
    "        X_mask_cat = X_mask\n",
    "\n",
    "        acc = {\"categorical\": {\"mice\": []}}\n",
    "\n",
    "        for i in range(X_imp.shape[2]):\n",
    "            acc[\"categorical\"][\"mice\"].append(accuracy(X_imp = X_imp_cat[:, :, i],\n",
    "                                                    X_true = X_true_cat[:, :, i],\n",
    "                                                    mask = X_mask_cat[:, :, i]))\n",
    "        for key, value in acc.items():\n",
    "            for k, v in value.items():\n",
    "                acc[key][k] = np.matrix(v)\n",
    "                acc[key][k] = (acc[key][k].mean().round(4), acc[key][k].std().round(4))\n",
    "        return acc\n",
    "                \n",
    "    def _evaluate_mice_coxPH(self, X_imp_train, X_imp_test, event_and_duration_train, event_and_duration_test):\n",
    "        \"\"\"\n",
    "        Evaluate the multivariate cox PH model on the multiple imputed datasets.\n",
    "        Data is already decoded.\n",
    "        \"\"\"\n",
    "\n",
    "        c_index_train = []\n",
    "        c_index_test = []\n",
    "        bias = []\n",
    "\n",
    "        # concatinate X_train and event and duration\n",
    "        X_imp_train = np.concatenate((X_imp_train, event_and_duration_train), axis = 1)\n",
    "        X_imp_test = np.concatenate((X_imp_test, event_and_duration_test), axis = 1)\n",
    "\n",
    "        for i in range(X_imp_train.shape[2]):\n",
    "            data_train = pd.DataFrame(X_imp_train[:, :, i], columns = self.categorical_colnames + [self.duration_col, self.event_col])\n",
    "            data_test = pd.DataFrame(X_imp_test[:, :, i], columns = self.categorical_colnames + [self.duration_col, self.event_col])\n",
    "            weights, conc_train, conc_test = self._fit_single_cox_PH(data_train, data_test).values()\n",
    "\n",
    "            c_index_train.append(conc_train)\n",
    "            c_index_test.append(conc_test)\n",
    "            \n",
    "            bias_weights = self._evaluate_bias_weights(weights)\n",
    "            bias.append(bias_weights)\n",
    "\n",
    "        c_index_train = (np.array(c_index_train).mean().round(3), np.array(c_index_train).std().round(3))\n",
    "        c_index_test = (np.array(c_index_test).mean().round(3), np.array(c_index_test).std().round(3))\n",
    "\n",
    "        bias_df = pd.DataFrame(bias)\n",
    "        bias = dict(zip(bias_df.columns, zip(round(bias_df.mean(),4), round(bias_df.std(), 4))))\n",
    "\n",
    "        return {'c_index_train': c_index_train, 'c_index_test': c_index_test, 'bias': bias}\n",
    "\n",
    "    def _run_mice_imputations(self, M, p_miss, p_obs):\n",
    "        \"\"\"\n",
    "        Run MICE imputation on M datasets with MAR missing values.\n",
    "        Assumes method self._generate_mice_datasets(.) has been ran.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        M : int\n",
    "            Number of datasets to generate.\n",
    "\n",
    "        p_miss : float\n",
    "            Proportion of missing values to generate for variables which will have missing values.\n",
    "\n",
    "        Returns\n",
    "        ----------\n",
    "        dict\n",
    "            Dictionary containing train and test accuracy, concordance index and bias of the imputed datasets.\n",
    "        \"\"\"\n",
    "        p_obs = round(p_obs, 4)\n",
    "\n",
    "        X_init = self._read_single_simulated_dataset(M, p_miss, p_obs)[\"X_init\"]\n",
    "        X_mask = self._read_single_simulated_dataset(M, p_miss, p_obs)[\"X_mask\"]\n",
    "        event_and_duration = self._read_single_simulated_dataset(M, p_miss, p_obs)[\"event_and_duration\"]\n",
    "        \n",
    "        X_init_train = X_init['train']\n",
    "        X_init_test = X_init['test']\n",
    "        X_mask_train = X_mask['train']\n",
    "        X_mask_test = X_mask['test']\n",
    "        event_and_duration_train = event_and_duration['train']           \n",
    "        event_and_duration_test = event_and_duration['test']\n",
    "\n",
    "\n",
    "        # Recall imputations are tensors of dimensions (n, d, G, M)\n",
    "        imputed_train, imputed_test = self._read_single_mice_tensor(p_miss, p_obs).values()    \n",
    "        \n",
    "        M = imputed_train.shape[3]\n",
    "        G = imputed_train.shape[2]\n",
    "\n",
    "        # We need G copies of the initial data and the mask because we have G imputed datasets\n",
    "        # Thus, we reshape the initial data and the mask to (n, d, G, M)\n",
    "        X_init_train = np.repeat(X_init_train[:, :, np.newaxis], G, axis=2)\n",
    "        X_init_test = np.repeat(X_init_test[:, :, np.newaxis], G, axis=2)\n",
    "\n",
    "        X_mask_train = np.repeat(X_mask_train[:, :, np.newaxis], G, axis=2)\n",
    "        X_mask_test = np.repeat(X_mask_test[:, :, np.newaxis], G, axis=2)\n",
    "\n",
    "        event_and_duration_train = np.repeat(event_and_duration_train[:, :, np.newaxis], G, axis=2)\n",
    "        event_and_duration_test = np.repeat(event_and_duration_test[:, :, np.newaxis], G, axis=2)\n",
    "\n",
    "        acc_train = {\"categorical\": {\"mice\": []}}\n",
    "        \n",
    "        acc_test = {\"categorical\": {\"mice\": []}}\n",
    "\n",
    "        c_index_train = []\n",
    "        c_index_test = []\n",
    "        bias = []\n",
    "        \n",
    "        for i in range(M):\n",
    "            # accuracy\n",
    "            acc_dict_train = self._evaluate_single_mice_acc(imputed_train[:, :, :, i], X_init_train[:, :, :, i], X_mask_train[:, :, :, i])\n",
    "            acc_dict_test = self._evaluate_single_mice_acc(imputed_test[:, :, :, i], X_init_test[:, :, :, i], X_mask_test[:, :, :, i])\n",
    "\n",
    "            acc_train[\"categorical\"][\"mice\"].append((acc_dict_train[\"categorical\"][\"mice\"][0], acc_dict_train[\"categorical\"][\"mice\"][1]))\n",
    "            acc_test[\"categorical\"][\"mice\"].append((acc_dict_test[\"categorical\"][\"mice\"][0], acc_dict_test[\"categorical\"][\"mice\"][1]))\n",
    "\n",
    "            # Bias and c-index\n",
    "            conc_train, conc_test, weight_bias = self._evaluate_mice_coxPH(imputed_train[:, :, :, i], imputed_test[:, :, :, i], \\\n",
    "                                                                          event_and_duration_train[:, :, :, i], event_and_duration_test[:, :, :, i]).values()\n",
    "            c_index_train.append(conc_train)\n",
    "            c_index_test.append(conc_test)\n",
    "            bias.append(weight_bias)\n",
    "\n",
    "        # Take average of mean and std\n",
    "        for key, value in acc_train.items():\n",
    "            for k, v in value.items():\n",
    "                acc_train[key][k] = np.matrix(v)\n",
    "                acc_train[key][k] = acc_train[key][k].mean(axis = 0).round(4).flatten()\n",
    "\n",
    "        for key, value in acc_test.items():\n",
    "            for k, v in value.items():\n",
    "                acc_test[key][k] = np.matrix(v)\n",
    "                acc_test[key][k] = acc_test[key][k].mean(axis = 0).round(4).flatten()\n",
    "\n",
    "        c_index_train = self.mean_and_std_list(c_index_train)\n",
    "        c_index_test = self.mean_and_std_list(c_index_test)\n",
    "\n",
    "        bias_df = pd.DataFrame(bias)\n",
    "        bias = {col: self.mean_and_std_df(bias_df[col]) for col in bias_df.columns}\n",
    "\n",
    "        train = {\"acc\": acc_train, \"c_index\": c_index_train, \"bias\": bias}\n",
    "        test = {\"acc\": acc_test, \"c_index\": c_index_test, \"bias\": bias}\n",
    "        return {\"train:\": train, \"test\": test}\n",
    "\n",
    "    def _get_mice_acc(self, acc_in):\n",
    "        \"\"\"\n",
    "        Get the MICE accuracy for the imputed datasets.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        acc_in : dict\n",
    "            Dictionary containing the accuracy for the imputed datasets.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        pd.DataFrame\n",
    "            DataFrame containing the accuracy for the imputed datasets of each p_miss.\n",
    "        \"\"\"\n",
    "\n",
    "        acc_cols = pd.MultiIndex.from_tuples([\n",
    "                ('Categorical', 'mice')\n",
    "                ])\n",
    "        index = ((miss, obs) for miss in self.p_miss for obs in self.p_obs)\n",
    "        row_index = pd.MultiIndex.from_tuples(index, names=['p_miss', 'p_obs'])\n",
    "        acc_row = []\n",
    "\n",
    "        for miss in self.p_miss:\n",
    "            for obs in self.p_obs:\n",
    "                acc = acc_in[self.p_miss.index(miss), self.p_obs.index(obs)]        \n",
    "                mice_mean_cat = acc['categorical']['mice'][0]\n",
    "                mice_std_cat = acc['categorical']['mice'][1]\n",
    "\n",
    "                row = [f\"{mice_mean_cat:.4f} ± {mice_std_cat:.4f}\"]\n",
    "                acc_row.append(row)\n",
    "\n",
    "        return pd.DataFrame(acc_row, index = row_index, columns = acc_cols)\n",
    "    \n",
    "    def _get_mice_c_index(self, c_index_in):\n",
    "        \"\"\"\n",
    "        Get the multivariate concordance index for the imputed datasets.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        c_index_in : list\n",
    "            List containing the concordance index for the imputed datasets.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        pd.DataFrame\n",
    "            DataFrame containing the concordance index for the imputed datasets of each p_miss.\n",
    "        \"\"\"\n",
    "\n",
    "        column = ['mice']\n",
    "        index = ((miss, obs) for miss in self.p_miss for obs in self.p_obs)\n",
    "        row_index = pd.MultiIndex.from_tuples(index, names=['p_miss', 'p_obs'])\n",
    "        conc_row = []\n",
    "\n",
    "        for miss in self.p_miss:\n",
    "            for obs in self.p_obs:\n",
    "                conc = c_index_in[self.p_miss.index(miss), self.p_obs.index(obs)]\n",
    "                mice_mean = conc[0]\n",
    "                mice_std = conc[1]\n",
    "                row = [f\"{mice_mean:.4f} ± {mice_std:.4f}\"]\n",
    "                conc_row.append(row)\n",
    "\n",
    "        return pd.DataFrame(conc_row, index = row_index, columns = column)\n",
    "\n",
    "    def _get_mice_bias(self):\n",
    "        \"\"\"\n",
    "        Get the mice bias for the imputed datasets.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        pd.DataFrame\n",
    "            DataFrame containing the bias for the imputed datasets of each p_miss and p_obs.\n",
    "        \"\"\"\n",
    "\n",
    "        feature_cols = list(self.bias[0][0].keys())\n",
    "        index = ((miss, obs) for miss in self.p_miss for obs in self.p_obs)\n",
    "        row_index = pd.MultiIndex.from_tuples(index, names=['p_miss', 'p_obs'])\n",
    "        bias_row = []\n",
    "\n",
    "        for miss in self.p_miss:\n",
    "            for obs in self.p_obs:\n",
    "                mice_bias = list(self.bias[self.p_miss.index(miss), self.p_obs.index(obs)].values())\n",
    "                mice_row = []\n",
    "                for column_index in range(len(mice_bias)):\n",
    "                    mice_bias_mean = mice_bias[column_index][0]\n",
    "                    mice_bias_std = mice_bias[column_index][1]\n",
    "                    mice_row.append(f\"{mice_bias_mean:.4f} ± {mice_bias_std:.4f}\")\n",
    "                bias_row.append(mice_row)\n",
    "\n",
    "        return pd.DataFrame(bias_row, index = row_index, columns = feature_cols)\n",
    "\n",
    "    def simulate_mice(self, M, G, p_miss = [0.1, 0.2, 0.3, 0.4, 0.5], p_obs = [8/9, 7/9, 6/9, 5/9, 4/9]):\n",
    "        \"\"\"\n",
    "        Run a simulation with mice imputations.\n",
    "        \"\"\"\n",
    "\n",
    "        self.p_miss = p_miss\n",
    "        self.p_obs = p_obs\n",
    "        self.G = G\n",
    "\n",
    "        self.acc_train = np.empty((len(p_miss), len(p_obs)), dtype=object)\n",
    "        self.acc_test = np.empty((len(p_miss), len(p_obs)), dtype=object)\n",
    "        self.conc_train = np.empty((len(p_miss), len(p_obs)), dtype=object)\n",
    "        self.conc_test = np.empty((len(p_miss), len(p_obs)), dtype=object)\n",
    "        self.bias = np.empty((len(p_miss), len(p_obs)), dtype=object)\n",
    "\n",
    "        for miss in self.p_miss:\n",
    "            print(f\"Currently simulating for p_miss: {miss}\")\n",
    "            for obs in p_obs:\n",
    "                train, test = self._run_mice_imputations(M = M, p_miss = miss, p_obs=obs).values()\n",
    "\n",
    "                acc_train = train['acc']\n",
    "                conc_train = train['c_index']\n",
    "                acc_test = test['acc']\n",
    "                conc_test = test['c_index']\n",
    "                bias = train['bias']\n",
    "                        \n",
    "                self.acc_train[p_miss.index(miss), p_obs.index(obs)] = acc_train\n",
    "                self.acc_test[p_miss.index(miss), p_obs.index(obs)] = acc_test\n",
    "                self.conc_train[p_miss.index(miss), p_obs.index(obs)] = conc_train\n",
    "                self.conc_test[p_miss.index(miss), p_obs.index(obs)] = conc_test\n",
    "                self.bias[p_miss.index(miss), p_obs.index(obs)] = bias\n",
    "\n",
    "    def get_mice_results(self):\n",
    "        \"\"\" \n",
    "        Get the results of the mice imputation simulation.\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        dict\n",
    "            A dictionary containing the accuracy, concordance index and bias for the multivariate imputation simulation.\n",
    "            The dictionary values are dataframes.\n",
    "        \"\"\"\n",
    "        acc_train, acc_test = self._get_mice_acc(self.acc_train), self._get_mice_acc(self.acc_test)\n",
    "        conc_train, conc_test = self._get_mice_c_index(self.conc_train), self._get_mice_c_index(self.conc_test)\n",
    "        bias = self._get_mice_bias()\n",
    "\n",
    "        return {'acc_train': acc_train, 'acc_test': acc_test, 'c_index_train': conc_train, 'c_index_test': conc_test, 'bias': bias}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rx</th>\n",
       "      <th>sex</th>\n",
       "      <th>obstruct</th>\n",
       "      <th>perfor</th>\n",
       "      <th>adhere</th>\n",
       "      <th>nodes</th>\n",
       "      <th>status</th>\n",
       "      <th>differ</th>\n",
       "      <th>extent</th>\n",
       "      <th>surg</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Lev+5FU</td>\n",
       "      <td>M</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>3.0</td>\n",
       "      <td>True</td>\n",
       "      <td>2.0</td>\n",
       "      <td>Serosa</td>\n",
       "      <td>S</td>\n",
       "      <td>1521</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Lev+5FU</td>\n",
       "      <td>M</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>1.0</td>\n",
       "      <td>False</td>\n",
       "      <td>2.0</td>\n",
       "      <td>Serosa</td>\n",
       "      <td>S</td>\n",
       "      <td>3087</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Obs</td>\n",
       "      <td>F</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>Y</td>\n",
       "      <td>3.0</td>\n",
       "      <td>True</td>\n",
       "      <td>2.0</td>\n",
       "      <td>Muscule</td>\n",
       "      <td>S</td>\n",
       "      <td>963</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Lev+5FU</td>\n",
       "      <td>F</td>\n",
       "      <td>Y</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>3.0</td>\n",
       "      <td>True</td>\n",
       "      <td>2.0</td>\n",
       "      <td>Serosa</td>\n",
       "      <td>L</td>\n",
       "      <td>293</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Obs</td>\n",
       "      <td>M</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>4.0</td>\n",
       "      <td>True</td>\n",
       "      <td>2.0</td>\n",
       "      <td>Serosa</td>\n",
       "      <td>L</td>\n",
       "      <td>659</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        rx sex obstruct perfor adhere nodes  status  differ   extent surg  \\\n",
       "0  Lev+5FU   M        N      N      N   3.0    True     2.0   Serosa    S   \n",
       "1  Lev+5FU   M        N      N      N   1.0   False     2.0   Serosa    S   \n",
       "2      Obs   F        N      N      Y   3.0    True     2.0  Muscule    S   \n",
       "3  Lev+5FU   F        Y      N      N   3.0    True     2.0   Serosa    L   \n",
       "4      Obs   M        N      N      N   4.0    True     2.0   Serosa    L   \n",
       "\n",
       "   time  \n",
       "0  1521  \n",
       "1  3087  \n",
       "2   963  \n",
       "3   293  \n",
       "4   659  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('../../data/colon/colon.csv', sep=',', index_col=0)\n",
    "# Start by subetting data where etype == 2\n",
    "df = df[df['etype'] == 2]\n",
    "# drop node4 column \n",
    "df = df.drop('node4', axis=1)\n",
    "# df.drop_duplicates(inplace=True)\n",
    "df.dropna(axis=0, inplace=True)\n",
    "#df['status'] = df['status'].map({0: False, 1: True}) \n",
    "cox_ph_cols = df.columns.drop(['id', 'study', 'age', 'etype'])\n",
    "\n",
    "to_keep = [col for col in df.columns if col not in ['etype', 'study', 'id', 'age']]\n",
    "df = df[to_keep]\n",
    "df = df.reset_index(drop=True)\n",
    "\n",
    "df['sex'] = df['sex'].map({1: 'M', 0: 'F'})\n",
    "df['obstruct'] = df['obstruct'].map({1: 'Y', 0: 'N'})\n",
    "df['perfor'] = df['perfor'].map({1: 'Y', 0: 'N'})\n",
    "df['adhere'] = df['adhere'].map({1: 'Y', 0: 'N'})\n",
    "#df['differ'] = df['differ'].map({1: 'Well', 2: 'Moderate', 3: 'Poor'})\n",
    "df['extent'] = df['extent'].map({1: 'Submucosa', 2: 'Muscule', 3: 'Serosa', 4: 'Contiguous_structures'})\n",
    "df['surg'] = df['surg'].map({1: 'L', 0: 'S'})\n",
    "df['status'] = df['status'].map({1: True, 0: False})\n",
    "df[\"nodes\"] = pd.cut(df[\"nodes\"], bins = [-1, 1, 3, 7, 100], labels = [1.0, 2.0, 3.0, 4.0])\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_colnames = ['rx', 'sex', 'obstruct', 'perfor', 'adhere', 'nodes', 'differ', 'extent', 'surg']\n",
    "num_colnames = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running simulations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate data for R"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "M = 50\n",
    "G = 50\n",
    "\n",
    "p_miss = [0.05, 0.1, 0.2, 0.3, 0.4, 0.5]\n",
    "p_obs = [8/9, 7/9, 6/9, 5/9, 4/9, 3/9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_R = MiceSimulationMCAR(df, duration_col = 'time', event_col = 'status', cat_colnames = cat_colnames)\n",
    "to_R._generate_mice_datasets(M = M, p_miss = p_miss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_R = MiceSimulationMAR(df, duration_col = 'time', event_col = 'status', cat_colnames = cat_colnames)\n",
    "to_R._generate_mice_datasets(M = 50, p_miss = p_miss, p_obs = p_obs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MCAR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mice_mcar = MiceSimulationMCAR(df, duration_col = 'time', event_col = 'status', cat_colnames = cat_colnames)\n",
    "mice_mcar.simulate_mice(M = M, G = G, p_miss = p_miss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_train, acc_test, conc_train, conc_test, bias = mice_mcar.get_mice_results().values()\n",
    "acc_train.to_csv(f'../results/acc_train_MCAR{M}_MICE.csv')\n",
    "acc_test.to_csv(f'../results/acc_test_MCAR{M}_MICE.csv')\n",
    "conc_train.to_csv(f'../results/c_index_train_MCAR{M}_MICE.csv')\n",
    "conc_test.to_csv(f'../results/c_index_test_MCAR{M}_MICE.csv')\n",
    "bias.to_csv(f'../results/bias_MCAR{M}_MICE.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MAR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Currently simulating for p_miss: 0.05\n",
      "Currently simulating for p_miss: 0.1\n",
      "Currently simulating for p_miss: 0.2\n",
      "Currently simulating for p_miss: 0.3\n",
      "Currently simulating for p_miss: 0.4\n",
      "Currently simulating for p_miss: 0.5\n"
     ]
    }
   ],
   "source": [
    "mice_mar = MiceSimulationMAR(df, duration_col = 'time', event_col = 'status', cat_colnames = cat_colnames)\n",
    "mice_mar.simulate_mice(M = M, G = G, p_miss = p_miss, p_obs = p_obs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_train, acc_test, conc_train, conc_test, bias = mice_mar.get_mice_results().values()\n",
    "acc_train.to_csv(f'../results/acc_train_MAR{M}_MICE.csv')\n",
    "acc_test.to_csv(f'../results/acc_test_MAR{M}_MICE.csv')\n",
    "conc_train.to_csv(f'../results/c_index_train_MAR{M}_MICE.csv')\n",
    "conc_test.to_csv(f'../results/c_index_test_MAR{M}_MICE.csv')\n",
    "bias.to_csv(f'../results/bias_MAR{M}_MICE.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "masters",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
