{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load in the data for the new study after preprocessing (in the file: data_both_registers.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "\n",
    "from encoding import *\n",
    "from rubins_rules import *\n",
    "from Survival_functions import *\n",
    "\n",
    "pd.set_option('display.max_columns', None)  \n",
    "pd.set_option('display.max_rows', None)\n",
    "\n",
    "# Replace the path with the actual path to your file\n",
    "file_path = '../../../Both/new_study.xlsx'\n",
    "df = pd.read_excel(file_path, index_col='PATNO')\n",
    "df = df.rename(columns={'OS (days)': 'time'})\n",
    "df = df.rename(columns={'Status': 'status'})\n",
    "\n",
    "df['status'] = df['status'].map({'Dead': True, 'Alive': False})\n",
    "\n",
    "print(df.shape)\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patients_with_false_status = df[df['status'] == False].index.tolist()\n",
    "patients_with_false_status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_categorical_features(df):\n",
    "    categorical_features = [column for column in df.columns if df[column].dtype == 'object']\n",
    "    return categorical_features\n",
    "\n",
    "categorical_features = find_categorical_features(df)\n",
    "features_df = pd.DataFrame(index=categorical_features)\n",
    "\n",
    "# Add column 'Variables' containing unique values for each feature\n",
    "variables_list = []\n",
    "for feature in categorical_features:\n",
    "    unique_values = df[feature].unique()\n",
    "    variables_list.append(\"/ \".join(str(val) for val in unique_values))\n",
    "\n",
    "features_df['Variables'] = variables_list\n",
    "features_df.T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load in all packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "mcv = RepeatedStratifiedKFold(n_splits=5, n_repeats=5, random_state=173637)\n",
    "from sksurv.linear_model import CoxPHSurvivalAnalysis\n",
    "from sksurv.ensemble import RandomSurvivalForest\n",
    "from sksurv.linear_model import CoxnetSurvivalAnalysis\n",
    "from sksurv.metrics import (integrated_brier_score)\n",
    "from sklearn.inspection import permutation_importance\n",
    "from sklearn.metrics import make_scorer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coxnet_df = df.copy()\n",
    "X, y, tuple_y, target_columns = x_y_baseline(coxnet_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TargetEncoding\n",
    "\n",
    "Target Encoder transfer NaN values into values, so we must iterate through the features that must target encode, remove rows that have NaN and do a TargetEncoder.\n",
    "\n",
    "- TargetEncoder used not the information about the others featues, it just use the target value to calculate on\n",
    "\n",
    "\n",
    "TargetEncoding is done in the function Baseline_target_encoding(), that are defines in the file \"encoding.py\". The columns that must be encoded by using TargetEncoding is called \"target_columns\" here in this notebook. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COXNET\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoding of ordinal and binary catogorical variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the function x_y_baseline() all ordinal variables and binary catogorical variables are enocoded. And we also defines which columns that must be TargetEncoded (nominal columns)\n",
    "- Binary variables without missing values: get_dummies encoded with `drop_first=True`. Got the same dimension. \n",
    "- Binary variables with missing values: encoded manually to 0 and 1 using `map`. \n",
    "\n",
    "The function x_y_baseline also split the dataframe to X and y. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coxnet_df = df.copy()\n",
    "X, y, tuple_y, target_columns = x_y_baseline(coxnet_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loop iteratively through all the hyperparameters. Fit on the given survival analysis model with the given hyperparamet (s). Inside the repeated cross-validation (MCV), we use the function Baseline_target_encoding(). This use TargetEncoding on the nominal columns for both training and testing set. Also in this function we also do this steps for both training and testing set:\n",
    "\n",
    "- Scaling\n",
    "- KNN Imputer with `n_neighbors=10`\n",
    "- Scaling back to original (`inverse_transform`)\n",
    "- Binary variables with missing values earlier are then rounded to the nearest whole number, resulting in either 0 or 1. Then these numbers are mapped back to their original categorical values before encoding them using get dummies with `drop_first=True`.\n",
    "- Scale both training and testing set. \n",
    "\n",
    "It returns $X_{train}$ and $X_{test}$ that are finally preprocessed without any missing values and are now ready be used in the models. \n",
    "\n",
    "The code calculate:\n",
    "- the integrated brier score (mean over all folds)\n",
    "- concordance for test and train set (mean over all folds)\n",
    "- Permutation importance \n",
    "\n",
    "for all hyperparameters or all comibinations of all hyperparameters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Repeated Statified cross validation for Coxnet model in sksurv\n",
    "alphas = [0, 0.0001, 0.0005, 0.001, 0.005, 0.01, 0.05, 0.1, 0.5, 1, 3, 5, 10, 20, 50, 70, 100, 200, 500, 700, 1000]\n",
    "#alphas = [0, 0.0001, 0.0003, 0.0005, 0.0007, 0.001, 0.003,  0.006, 0.007, 0.008, 0.009,\n",
    "#          0.01, 0.03, 0.05, 0.07, 0.1, 0.3, 0.5, 0.7, 1, 3, 5, 10, 20, 50, 70, 100, 200, 500, 700, 1000]\n",
    "\n",
    "l1_ratios = [0.0001, 0.001, 0.01, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]\n",
    "#l1_ratios = [0.0001, 0.001, 0.01, 0.1]\n",
    "\n",
    "\n",
    "\n",
    "results_coxnet = {}\n",
    "feature_importance_coxnet = {}\n",
    "coefficients_coxnet = {}\n",
    "conc_coxnet = {}\n",
    "\n",
    "for l1_ratio in l1_ratios:\n",
    "    for alpha in alphas:\n",
    "        with warnings.catch_warnings():\n",
    "            warnings.simplefilter(\"ignore\", category=UserWarning)\n",
    "            \n",
    "            coxnet = CoxnetSurvivalAnalysis(l1_ratio=l1_ratio, alphas=[alpha], fit_baseline_model=True)\n",
    "            conc_train = []\n",
    "            conc_test = []\n",
    "            brier = []\n",
    "            permut = []\n",
    "            coef = []\n",
    "            feature_importance = []\n",
    "            \n",
    "            print(f'l1_ratio: {l1_ratio}')\n",
    "        \n",
    "            for i, (train, test) in enumerate(mcv.split(X, tuple_y)):\n",
    "                X_train, X_test = X.iloc[train], X.iloc[test]\n",
    "                y_train, y_test = y[train], y[test]\n",
    "                \n",
    "                X_train, X_test = Preprocessing(X_train=X_train, X_test=X_test, y_train=y_train, target_columns=target_columns)\n",
    "                # fix the times            \n",
    "                times_train_min = y_train['time'].min()\n",
    "                times_train_max = y_train['time'].max()\n",
    "                times_train = np.arange(0, times_train_max)\n",
    "                times_test_min = y_test['time'].min()\n",
    "                times_test_max = y_test['time'].max()\n",
    "                if times_test_max > times_train_max:\n",
    "                    y_test_red_index = y_test['time'] <= times_train_max\n",
    "                    y_test = y_test[y_test_red_index]\n",
    "                    X_test = X_test[y_test_red_index]\n",
    "                    times_test_max = y_test['time'].max()\n",
    "                times_test = np.arange(times_test_min, times_test_max)\n",
    "\n",
    "                \n",
    "                coxnet.fit(X_train, y_train)\n",
    "                \n",
    "                # Compute the C-index for test data and train data\n",
    "                conc_train.append(coxnet.score(X_train, y_train))\n",
    "                conc_test.append(coxnet.score(X_test, y_test))\n",
    "\n",
    "                # Integrated Brier Score\n",
    "                surv_prob_test = np.row_stack([fn(times_test) for fn in coxnet.predict_survival_function(X_test)])\n",
    "                brier.append(integrated_brier_score(y_train, y_test, surv_prob_test, times_test))\n",
    "\n",
    "                importance = permutation_importance(coxnet,\n",
    "                                                    X_test,\n",
    "                                                    y_test,\n",
    "                                                    n_repeats=10,\n",
    "                                                    random_state=1)\n",
    "                permut.append(importance.importances_mean)\n",
    "\n",
    "                feature_importance.append(importance)\n",
    "                coef.append(coxnet.coef_)\n",
    "        \n",
    "            feature_importance_coxnet[(alpha, l1_ratio)] = feature_importance\n",
    "            coefficients_coxnet[(alpha, l1_ratio)] = coef\n",
    "\n",
    "            # Evaluate and record the results after each alpha and l1_ratio combination\n",
    "            avg_conc_test = np.mean(conc_test)\n",
    "            avg_conc_train = np.mean(conc_train)\n",
    "            avg_brier = np.mean(brier)\n",
    "            avg_permut = np.mean(permut)\n",
    "\n",
    "            results_coxnet[(alpha, l1_ratio)] = [avg_conc_test, avg_conc_train, avg_brier, avg_permut]\n",
    "\n",
    "            conc_coxnet[(alpha, l1_ratio)] = conc_test\n",
    "\n",
    "result = [{\n",
    "    'Alpha': alpha,\n",
    "    'L1 Ratio': l1_ratio,\n",
    "    'Conc test': avg_conc_test,\n",
    "    'Conc train': avg_conc_train,\n",
    "    'Brier Score': avg_brier,\n",
    "    'Permut': avg_permut\n",
    "} for (alpha, l1_ratio), (avg_conc_test, avg_conc_train, avg_brier, avg_permut) in results_coxnet.items()]\n",
    "\n",
    "# Create the DataFrame\n",
    "results_coxnet = pd.DataFrame(result)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sort the result by the best Concordance (test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_coxnet = results_coxnet.sort_values(by='Conc test', ascending=False).reset_index(drop=True)\n",
    "\n",
    "# Print out the sorted DataFrame\n",
    "scores_coxnet.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model performance and feature selection across different alpha values with different L1 ratios. \n",
    "\n",
    "Use the function extract_alpha_coxnet(), that are in the Survival_functions.py. This function filters the Coxnet results for a given L1 ratio and computes the average coefficients for each unique alpha value within that L1 ratio. \n",
    "It then identifies the non-zero coefficients, constructs a dataframe mapping alphas to the number of selected features, and merges it with the input \n",
    "dataframe to obtain various model metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Your l1_ratios and figure setup\n",
    "l1_ratios = [0.0001, 0.001, 0.01, 0.1]\n",
    "fig, axs = plt.subplots(2, 2, figsize=(13, 8))  # Setup for 4 subplots\n",
    "\n",
    "# Iterate through l1_ratios and create subplots\n",
    "for i, l1_ratio in enumerate(l1_ratios):\n",
    "    alphas, _, conc_train, conc_test, brier, num_features, max_features, _ = extract_alpha_coxnet(\n",
    "        results_coxnet, coefficients_coxnet, l1_ratio_value=l1_ratio, feature_names=X_train.columns)\n",
    "    \n",
    "    ax = axs[i // 2, i % 2]  # Determine the subplot\n",
    "    x = np.arange(len(alphas))  # X locations for the groups\n",
    "    \n",
    "    # Plot concordance for train and test as lines\n",
    "    ax.plot(x, brier, color='#FF6347', label='Brier Score')  \n",
    "    ax.plot(x, conc_test, color='dodgerblue', label='C-Index (Test)')   \n",
    "    ax.plot(x, conc_train, color='#00008B', label='C-Index (Train)')\n",
    "    \n",
    "    # Set x-axis labels and ticks\n",
    "    ax.set_xticks(x)\n",
    "    ax.set_xticklabels(alphas, rotation=45)\n",
    "    ax.set_xlabel('Alpha')\n",
    "    ax.set_ylabel('Score')\n",
    "    ax.tick_params(axis='y')\n",
    "\n",
    "    # Create a twin y-axis for the number of features\n",
    "    ax2 = ax.twinx()\n",
    "    ax2.plot(x, num_features, linestyle='--', color='g', label='Number of Features')\n",
    "    ax2.set_ylabel('Number of features', color='g')\n",
    "    ax2.tick_params(axis='y', labelcolor='g')\n",
    "    #ax2.set_ylim(0, 50)\n",
    "\n",
    "    ax.set_title(f'L1 ratio: {l1_ratio}', fontsize=15, style='italic')\n",
    "    ax.grid()\n",
    "\n",
    "\n",
    "# Creating a unified legend for the entire figure\n",
    "labels = ['IBS', 'C-index Test', 'C-index Train', 'Number of Features']\n",
    "colors = ['#FF6347', 'dodgerblue', '#00008B', 'g']\n",
    "lines = [plt.Line2D([0], [0], color=color, marker='o', linestyle='-') for color in colors]\n",
    "plt.figlegend(lines, labels, bbox_to_anchor=(0.73, 0.99), ncol=4, labelspacing=0.)\n",
    "\n",
    "# Adjust layout and titles\n",
    "plt.tight_layout(rect=[0, 0.05, 1, 0.95])  # Adjust the rect parameter as needed\n",
    "#plt.suptitle('Coxnet Model Analysis Across Various L1 Ratios', fontsize=20, y=1.05)\n",
    "plt.subplots_adjust(top=0.9, bottom=0.1)  # Adjust top and bottom to make space for title and legend\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot Alpha vs weights (concordance and brier score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l1_ratio = scores_coxnet['L1 Ratio'].iloc[0]\n",
    "\n",
    "alphas, l1_ratio, conc_train, conc_test, brier, num_features, max_features, features_alpha = extract_alpha_coxnet(results_coxnet, \n",
    "                                                                                                                  coefficients_coxnet, \n",
    "                                                                                                                  l1_ratio_value=l1_ratio,\n",
    "                                                                                                                  feature_names=X_train.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_features = [\"Absolute Neutrophil Count\", \"Chemotherapy Type\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "n_features = len(all_features)\n",
    "n_cols = 2\n",
    "n_rows = (n_features + n_cols - 1) // n_cols\n",
    "\n",
    "fig, axes = plt.subplots(n_rows, n_cols, figsize=(n_cols * 5.5, n_rows * 3.5), constrained_layout=True)\n",
    "axes_flat = axes.flatten()\n",
    "\n",
    "# Initialize the min and max coefficient values\n",
    "min_coefficient = float('inf')\n",
    "max_coefficient = float('-inf')\n",
    "\n",
    "# First pass: find the global min and max across all plots\n",
    "for feature_index, feature_name in enumerate(X_train.columns):\n",
    "    if feature_name in all_features:\n",
    "        feature_coefficients = []\n",
    "\n",
    "        for alpha in alphas:\n",
    "            specific_coefficients = coefficients_coxnet[(alpha, l1_ratio)]\n",
    "            coefficients_array = np.array(specific_coefficients)\n",
    "            mean_list = np.mean(coefficients_array, axis=0)\n",
    "\n",
    "            if feature_index < len(mean_list):\n",
    "                feature_coefficients.append(mean_list[feature_index])\n",
    "\n",
    "        if feature_coefficients:\n",
    "            # Update the global min and max\n",
    "            min_coefficient = min(min_coefficient, min(feature_coefficients))\n",
    "            max_coefficient = max(max_coefficient, max(feature_coefficients))\n",
    "\n",
    "# Initialize a counter for the number of plotted features\n",
    "plot_count = 0\n",
    "# Second pass: plot using the global min and max\n",
    "for feature_index, feature_name in enumerate(X_train.columns):\n",
    "    if feature_name in all_features:\n",
    "        feature_coefficients = []\n",
    "\n",
    "        for alpha in alphas:\n",
    "            specific_coefficients = coefficients_coxnet[(alpha, l1_ratio)]\n",
    "            coefficients_array = np.array(specific_coefficients)\n",
    "            mean_list = np.mean(coefficients_array, axis=0)\n",
    "\n",
    "            if feature_index < len(mean_list):\n",
    "                feature_coefficients.append(mean_list[feature_index])\n",
    "\n",
    "        if feature_coefficients:\n",
    "            ax = axes_flat[plot_count]\n",
    "            ax.plot(alphas, feature_coefficients, marker='.', color='g')\n",
    "            ax.set_xlabel('Alpha')\n",
    "            ax.set_title(feature_name)\n",
    "            ax.set_xscale('log')\n",
    "            ax.set_ylabel('Coefficient', color='g')\n",
    "            ax.tick_params(axis='y', labelcolor='g')\n",
    "            ax.grid(True)\n",
    "\n",
    "            # Set the same y-axis limits for coefficient plots\n",
    "            ax.set_ylim(min_coefficient, max_coefficient)\n",
    "\n",
    "            ax2 = ax.twinx()\n",
    "            ax2.plot(alphas, conc_test, color='dodgerblue')\n",
    "            ax2.plot(alphas, brier, color='salmon')\n",
    "            ax2.set_ylabel('Score')\n",
    "            ax2.tick_params(axis='y')\n",
    "            ax2.set_ylim(0, 1)\n",
    "\n",
    "            handles1, labels1 = ax.get_legend_handles_labels()\n",
    "            handles2, labels2 = ax2.get_legend_handles_labels()\n",
    "            handles = handles1 + handles2\n",
    "            labels = labels1 + labels2\n",
    "            ax.legend(handles, labels, loc='best')\n",
    "\n",
    "            plot_count += 1  # Increment the plotted feature counter\n",
    "\n",
    "# Hide unused axes\n",
    "for i in range(plot_count, n_rows * n_cols):\n",
    "    fig.delaxes(axes_flat[i])\n",
    "\n",
    "# Creating a unified legend for the entire figure\n",
    "labels = ['Coefficient', 'C-index Test', 'IBS']\n",
    "colors = ['g', 'dodgerblue', 'salmon', ]\n",
    "lines = [plt.Line2D([0], [0], color=color, marker='o', linestyle='-') for color in colors]\n",
    "plt.figlegend(lines, labels, bbox_to_anchor=(0.69, 1.105), ncol=3, labelspacing=0.)\n",
    "\n",
    "# Adjust layout and titles\n",
    "#plt.suptitle('Coxnet Model Analysis Across Various L1 Ratios', fontsize=20, y=1.05)\n",
    "#plt.subplots_adjust(top=0.9, bottom=0.1)  # Adjust top and bottom to make space for title and legend\n",
    "\n",
    "plt.suptitle(f'(L1 ratio: {l1_ratio})', fontsize=12, style='italic', y=1.15)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_features = len(X_train.columns)\n",
    "n_cols = 3  # Number of columns in the subplot grid\n",
    "n_rows = (n_features + n_cols - 1) // n_cols  # Rounds up to ensure enough rows\n",
    "\n",
    "fig, axes = plt.subplots(n_rows, n_cols, figsize=(n_cols * 6, n_rows * 4), constrained_layout=True)  # Adjust for layout\n",
    "\n",
    "# Flatten the axes array for easy iteration\n",
    "axes_flat = axes.flatten()\n",
    "\n",
    "for feature_index, feature_name in enumerate(X_train.columns):\n",
    "    feature_coefficients = []\n",
    "\n",
    "    for alpha in alphas:\n",
    "        specific_coefficients = coefficients_coxnet[(alpha, l1_ratio)]\n",
    "        coefficients_array = np.array(specific_coefficients)\n",
    "        mean_list = np.mean(coefficients_array, axis=0)\n",
    "\n",
    "        if feature_index < len(mean_list):\n",
    "            feature_coefficients.append(mean_list[feature_index])\n",
    "        else:\n",
    "            break\n",
    "    \n",
    "    if feature_coefficients:\n",
    "        ax = axes_flat[feature_index]  # Use the flattened array of axes\n",
    "        ax.plot(alphas, feature_coefficients, marker='.', color= 'dodgerblue', label='Coefficient')\n",
    "        ax.set_xlabel('Alpha')\n",
    "        ax.set_title(feature_name)\n",
    "        ax.set_xscale('log')\n",
    "        ax.set_ylabel('Coefficient', color = 'dodgerblue')\n",
    "        ax.tick_params(axis='y', labelcolor='dodgerblue')\n",
    "\n",
    "        ax.grid(True)\n",
    "\n",
    "        ax2 = ax.twinx()\n",
    "        ax2.plot(alphas, conc_test, color='g', label='C-Index')\n",
    "        ax2.plot(alphas, brier, color='salmon', label='IBS')\n",
    "        ax2.set_ylabel('Score')\n",
    "        ax2.tick_params(axis='y')\n",
    "        ax2.set_ylim(0, 1)\n",
    "\n",
    "        # Handling the legend\n",
    "        handles1, labels1 = ax.get_legend_handles_labels()\n",
    "        handles2, labels2 = ax2.get_legend_handles_labels()\n",
    "        handles = handles1 + handles2\n",
    "        labels = labels1 + labels2\n",
    "        ax.legend(handles, labels, loc='best')  # You can adjust the location as needed\n",
    "\n",
    "# Hide any unused axes if the number of features is not a multiple of n_cols\n",
    "for i in range(feature_index + 1, n_rows * n_cols):\n",
    "    fig.delaxes(axes_flat[i])\n",
    "\n",
    "plt.suptitle(f'Regularization Effects: Balancing Model Permformance and Feature Selection (L1 ratio: {l1_ratio})', fontsize=20, style='italic')\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A horizontal bar chart of coefficients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = scores_coxnet['Alpha'].iloc[8]\n",
    "l1_ratio = scores_coxnet['L1 Ratio'].iloc[8]\n",
    "specific_coefficients = coefficients_coxnet[(alpha, l1_ratio)]\n",
    "coefficients_array = np.array(specific_coefficients)\n",
    "mean_list = np.mean(coefficients_array, axis=0)\n",
    "\n",
    "best_coefs = pd.DataFrame(mean_list, index=X_train.columns, columns=[\"coefficient\"])\n",
    "\n",
    "non_zero = np.sum(best_coefs.iloc[:, 0] != 0)\n",
    "print(f\"Number of non-zero coefficients: {non_zero}\")\n",
    "\n",
    "non_zero_coefs = best_coefs.query(\"coefficient != 0\")\n",
    "coef_order = non_zero_coefs.abs().sort_values(\"coefficient\").index\n",
    "\n",
    "_, ax = plt.subplots(figsize=(10, 10))\n",
    "non_zero_coefs.loc[coef_order].plot.barh(ax=ax, legend=False)\n",
    "ax.set_xlabel(\"coefficient\")\n",
    "ax.set_title(f\"Coxnet (alpha: {alpha:}, L1 Ratio: {l1_ratio})\") \n",
    "ax.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COX PH"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y, tuple_y, target_columns = x_y_baseline(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross validation for CoxPH model in sksurv\n",
    "alphas = [0, 0.0001, 0.0005, 0.001, 0.005, 0.01, 0.05, 0.1, 0.5, 1, 3, 5, 10, 20, 50, 70, 100, 200, 500, 700, 1000]\n",
    "\n",
    "results_coxph = {}\n",
    "feature_importance_ph = {}\n",
    "coefficients_coxph = {}\n",
    "conc_coxph = {}\n",
    "\n",
    "for ind, alpha in enumerate(alphas):\n",
    "        conc_train = []\n",
    "        conc_test = []\n",
    "        brier = []\n",
    "        permut = []\n",
    "        feat_impor = []\n",
    "        coef = []\n",
    "\n",
    "        coxph = CoxPHSurvivalAnalysis(alpha=alpha, ties=\"efron\")\n",
    "\n",
    "        print(f'alpha: {alpha}')\n",
    "        \n",
    "        for i, (train, test) in enumerate(mcv.split(X, tuple_y)):\n",
    "            X_train, X_test = X.iloc[train], X.iloc[test]\n",
    "            y_train, y_test = y[train], y[test]\n",
    "            \n",
    "            X_train, X_test = Preprocessing(X_train=X_train, X_test=X_test, y_train=y_train, target_columns=target_columns)\n",
    "                \n",
    "            # Train Model            \n",
    "            times_train_min = y_train['time'].min()\n",
    "            times_train_max = y_train['time'].max()\n",
    "            times_train = np.arange(0, times_train_max)\n",
    "            times_test_min = y_test['time'].min()\n",
    "            times_test_max = y_test['time'].max()\n",
    "            if times_test_max > times_train_max:\n",
    "                y_test_red_index = y_test['time'] <= times_train_max\n",
    "                y_test = y_test[y_test_red_index]\n",
    "                X_test = X_test[y_test_red_index]\n",
    "                times_test_max = y_test['time'].max()\n",
    "            times_test = np.arange(times_test_min, times_test_max)\n",
    "\n",
    "            \n",
    "            coxph.fit(X_train, y_train)\n",
    "            \n",
    "            # Compute the C-index for test data and train data\n",
    "            conc_train.append(coxph.score(X_train, y_train))\n",
    "            conc_test.append(coxph.score(X_test, y_test))\n",
    "\n",
    "            # Brier Score\n",
    "            surv_prob_test = np.row_stack([fn(times_test) for fn in coxph.predict_survival_function(X_test)])\n",
    "            brier.append(integrated_brier_score(y_train, y_test, surv_prob_test, times_test))\n",
    "\n",
    "            importance = permutation_importance(coxph,\n",
    "                                                X_test,\n",
    "                                                y_test,\n",
    "                                                n_repeats=10,\n",
    "                                                random_state=1)\n",
    "            permut.append(importance.importances_mean)\n",
    "\n",
    "            feat_impor.append(importance)\n",
    "            coef.append(coxph.coef_)\n",
    "    \n",
    "        feature_importance_ph[(alpha)] = feat_impor\n",
    "        coefficients_coxph[(alpha)] = coef\n",
    "\n",
    "        # Evaluate and record the results after each alpha\n",
    "        avg_conc_test = np.mean(conc_test)\n",
    "        avg_conc_train = np.mean(conc_train)\n",
    "        avg_brier = np.mean(brier)\n",
    "        avg_permut = np.mean(permut)\n",
    "\n",
    "        results_coxph[(alpha)] = [avg_conc_test, avg_conc_train, avg_brier, avg_permut]\n",
    "        conc_coxph[(alpha)] = conc_test\n",
    "\n",
    "result = [{\n",
    "    'Alpha': alpha,\n",
    "    'Conc test': avg_conc_test,\n",
    "    'Conc train': avg_conc_train,\n",
    "    'Brier Score': avg_brier,\n",
    "    'Permut': avg_permut\n",
    "} for (alpha), (avg_conc_test, avg_conc_train, avg_brier, avg_permut) in results_coxph.items()]\n",
    "\n",
    "# Create the DataFrame\n",
    "results_coxph = pd.DataFrame(result)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sort the result by the best Concordance (test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_coxph = results_coxph.sort_values(by='Conc test', ascending=False).reset_index(drop=True)\n",
    "\n",
    "# Print out the sorted DataFrame\n",
    "scores_coxph.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot Alpha vs weights (concordance and brier score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_features = len(X_train.columns)\n",
    "n_cols = 3  # Number of columns in the subplot grid\n",
    "n_rows = (n_features + n_cols - 1) // n_cols  # Rounds up to ensure enough rows\n",
    "\n",
    "fig, axes = plt.subplots(n_rows, n_cols, figsize=(n_cols * 6, n_rows * 4), constrained_layout=True)  # Adjust for layout\n",
    "\n",
    "# Flatten the axes array for easy iteration\n",
    "axes_flat = axes.flatten()\n",
    "\n",
    "for feature_index, feature_name in enumerate(X_train.columns):\n",
    "    feature_coefficients = []\n",
    "\n",
    "    for alpha in alphas:\n",
    "        specific_coefficients = coefficients_coxph[(alpha)]\n",
    "        coefficients_array = np.array(specific_coefficients)\n",
    "        mean_list = np.mean(coefficients_array, axis=0)\n",
    "\n",
    "        if feature_index < len(mean_list):\n",
    "            feature_coefficients.append(mean_list[feature_index])\n",
    "        else:\n",
    "            break\n",
    "    \n",
    "    if feature_coefficients:\n",
    "        ax = axes_flat[feature_index]  # Use the flattened array of axes\n",
    "        ax.plot(alphas, feature_coefficients, marker='.', color= 'dodgerblue', label='Coefficient')\n",
    "        ax.set_xlabel('Alpha')\n",
    "        ax.set_title(feature_name)\n",
    "        ax.set_xscale('log')\n",
    "        ax.set_ylabel('Coefficient', color = 'dodgerblue')\n",
    "        ax.tick_params(axis='y', labelcolor='dodgerblue')\n",
    "\n",
    "        ax.grid(True)\n",
    "\n",
    "        ax2 = ax.twinx()\n",
    "        ax2.plot(alphas, conc_test, color='g', label='Concordance')\n",
    "        ax2.plot(alphas, brier, color='salmon', label='Brier score')\n",
    "        ax2.set_ylabel('Score')\n",
    "        ax2.tick_params(axis='y')\n",
    "        ax2.set_ylim(0, 1)\n",
    "\n",
    "        # Handling the legend\n",
    "        handles1, labels1 = ax.get_legend_handles_labels()\n",
    "        handles2, labels2 = ax2.get_legend_handles_labels()\n",
    "        handles = handles1 + handles2\n",
    "        labels = labels1 + labels2\n",
    "        ax.legend(handles, labels, loc='best')  # You can adjust the location as needed\n",
    "\n",
    "# Hide any unused axes if the number of features is not a multiple of n_cols\n",
    "for i in range(feature_index + 1, n_rows * n_cols):\n",
    "    fig.delaxes(axes_flat[i])\n",
    "\n",
    "plt.suptitle(f'Regularization Effects: Balancing Model Accuracy and Feature Significance', fontsize=20, style='italic')\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Survival curves of all patients: Mean survival function across folds with variability and event observation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assuming best_alpha, best_l1_ratio, coxnet_best, and Baseline_target_encoding are defined\n",
    "best_alpha = scores_coxph['Alpha'].iloc[1]\n",
    "\n",
    "# Extract all PATNO numbers\n",
    "patient_patnos = X.index\n",
    "\n",
    "# Number of rows and columns for subplot\n",
    "n_cols = 3\n",
    "n_rows = len(patient_patnos) // n_cols + (len(patient_patnos) % n_cols > 0)\n",
    "\n",
    "# Create a figure with subplots\n",
    "plt.figure(figsize=(5 * n_cols, 4 * n_rows))\n",
    "\n",
    "for j, patient_patno in enumerate(patient_patnos, start=1):\n",
    "    survival_probabilities = []\n",
    "    survival_times = []\n",
    "\n",
    "    observed_event_time = df['time'].loc[patient_patno]\n",
    "    \n",
    "    for i, (train, test) in enumerate(mcv.split(X, tuple_y)):\n",
    "        X_train, X_test = X.iloc[train], X.iloc[test]\n",
    "        y_train, y_test = y[train], y[test]\n",
    "        \n",
    "        X_train, X_test = Preprocessing(X_train=X_train, X_test=X_test, y_train=y_train, target_columns=target_columns)\n",
    "        \n",
    "        coxph_best = CoxPHSurvivalAnalysis(alpha=alpha, ties=\"efron\")\n",
    "        coxph_best.fit(X_train, y_train)\n",
    "\n",
    "        if patient_patno in X_test.index:\n",
    "            patient_data = X_test.loc[[patient_patno]]\n",
    "            surv_func = coxph_best.predict_survival_function(patient_data)\n",
    "            survival_probabilities.append(surv_func[0].y)\n",
    "            survival_times.append(surv_func[0].x)\n",
    "    \n",
    "    if survival_times:  # Check if there are any survival times collected\n",
    "        ax = plt.subplot(n_rows, n_cols, j)\n",
    "        \n",
    "        # Plot each survival curve as a thin line\n",
    "        for prob, times in zip(survival_probabilities, survival_times):\n",
    "            plt.plot(times[:len(prob)], prob, color='dodgerblue', linewidth=0.5, alpha=0.5)\n",
    "        \n",
    "        # Compute mean and standard deviation\n",
    "        min_length = min(len(arr) for arr in survival_times)\n",
    "        truncated_probs = [arr[:min_length] for arr in survival_probabilities]\n",
    "        stacked_probabilities = np.vstack(truncated_probs)\n",
    "        mean_survival = np.mean(stacked_probabilities, axis=0)\n",
    "        std_survival = np.std(stacked_probabilities, axis=0)\n",
    "        common_times = np.array(survival_times[0][:min_length])\n",
    "        \n",
    "        # Plot the mean survival function as a thick line\n",
    "        ax.plot(common_times, mean_survival, color='blue', linewidth=2, label=\"Mean Survival Function\")\n",
    "    \n",
    "        # Add standard deviation shading\n",
    "        ax.fill_between(common_times, mean_survival - std_survival, mean_survival + std_survival, color='grey', alpha=0.2, label=\"Std Dev\")        \n",
    "        ax.axvline(x=observed_event_time, color='red', linestyle='--', linewidth=2, label='Observed Event Time')\n",
    "    \n",
    "        ax.set_title(f\"Patient {patient_patno}\")\n",
    "        ax.set_xlabel(\"Time in days\")\n",
    "        ax.set_ylabel(\"Survival probability\")\n",
    "        ax.legend()\n",
    "        ax.grid(True)\n",
    "    else:\n",
    "        # If no data collected for a patient, this space will be empty\n",
    "        print(f\"No survival data collected for patient {patient_patno}.\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RandomSurvivalForest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RandomSurvivalForest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y, tuple_y, target_columns = x_y_baseline(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_estimators = [10, 20, 30, 40, 50]  \n",
    "max_depths = [2, 3, 4, 5]\n",
    "min_samples_splits = [2, 6, 8] \n",
    "min_samples_leafs = [1, 2, 3, 4, 5, 6]  \n",
    "\n",
    "results_rf = {}\n",
    "feature_importance_rf = {}\n",
    "\n",
    "for n_estimator in n_estimators:\n",
    "    for max_depth in max_depths:\n",
    "        for min_samples_split in min_samples_splits:  \n",
    "            for min_samples_leaf in min_samples_leafs: \n",
    "                rf = RandomSurvivalForest(\n",
    "                    n_estimators=n_estimator, \n",
    "                    max_depth=max_depth,\n",
    "                    min_samples_split=min_samples_split,\n",
    "                    min_samples_leaf=min_samples_leaf,\n",
    "                    random_state=173637)\n",
    "\n",
    "                conc_train = []\n",
    "                conc_test = []\n",
    "                brier = []\n",
    "                permut = []\n",
    "                feat_impor = []\n",
    "                coef = []\n",
    "                \n",
    "                #print(f'n_estimator: {n_estimator}, max_depth: {max_depth}, min_samples_split: {min_samples_split}, min_samples_leaf: {min_samples_leaf}')\n",
    "                print(f'n_estimator: {n_estimator}, max_depth: {max_depth}')\n",
    "                for i, (train, test) in enumerate(mcv.split(X, tuple_y)):\n",
    "                    X_train, X_test = X.iloc[train], X.iloc[test]\n",
    "                    y_train, y_test = y[train], y[test]\n",
    "                    \n",
    "                    X_train, X_test = Preprocessing(X_train=X_train, X_test=X_test, y_train=y_train, target_columns=target_columns)\n",
    "                        \n",
    "                    # fix the times            \n",
    "                    times_train_min = y_train['time'].min()\n",
    "                    times_train_max = y_train['time'].max()\n",
    "                    times_train = np.arange(0, times_train_max)\n",
    "                    times_test_min = y_test['time'].min()\n",
    "                    times_test_max = y_test['time'].max()\n",
    "                    if times_test_max > times_train_max:\n",
    "                        y_test_red_index = y_test['time'] <= times_train_max\n",
    "                        y_test = y_test[y_test_red_index]\n",
    "                        X_test = X_test[y_test_red_index]\n",
    "                        times_test_max = y_test['time'].max()\n",
    "                    times_test = np.arange(times_test_min, times_test_max)\n",
    "\n",
    "                    \n",
    "                    rf.fit(X_train, y_train)\n",
    "                    \n",
    "                    # Compute the C-index for test data and train data\n",
    "                    conc_train.append(rf.score(X_train, y_train))\n",
    "                    conc_test.append(rf.score(X_test, y_test))\n",
    "\n",
    "                    # Brier Score\n",
    "                    surv_prob_test = np.row_stack([fn(times_test) for fn in rf.predict_survival_function(X_test)])\n",
    "                    brier.append(integrated_brier_score(y_train, y_test, surv_prob_test, times_test))\n",
    "\n",
    "                    importance = permutation_importance(rf,\n",
    "                                                        X_test,\n",
    "                                                        y_test,\n",
    "                                                        n_repeats=10,\n",
    "                                                        random_state=1)\n",
    "                    permut.append(importance.importances_mean)\n",
    "\n",
    "                    feat_impor.append(importance)\n",
    "\n",
    "                feature_importance_rf[(n_estimator, max_depth, min_samples_split, min_samples_leaf)] = feat_impor\n",
    "\n",
    "                # Evaluate and record the results after each n_estimator and max_depth combination\n",
    "                avg_conc_test = np.mean(conc_test)\n",
    "                avg_conc_train = np.mean(conc_train)\n",
    "                avg_brier = np.mean(brier)\n",
    "                avg_permut = np.mean(permut)\n",
    "\n",
    "                results_rf[(n_estimator, max_depth, min_samples_split, min_samples_leaf)] = [avg_conc_test, avg_conc_train, avg_brier, avg_permut]\n",
    "\n",
    "result = [{\n",
    "    'n_estimator': n_estimator,\n",
    "    'max_depth': max_depth,\n",
    "    'min_samples_split': min_samples_split,\n",
    "    'min_samples_leaf': min_samples_leaf,\n",
    "    'Conc test': avg_conc_test,\n",
    "    'Conc train': avg_conc_train,\n",
    "    'Brier Score': avg_brier,\n",
    "    'Permut': avg_permut\n",
    "} for (n_estimator, max_depth, min_samples_split, min_samples_leaf), (avg_conc_test, avg_conc_train, avg_brier, avg_permut) in results_rf.items()]\n",
    "\n",
    "# Create the DataFrame\n",
    "results_rf = pd.DataFrame(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sort the result by the best Concordance (test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_rf = results_rf.sort_values(by='Conc test', ascending=False).reset_index(drop=True)\n",
    "\n",
    "# Print out the sorted DataFrame\n",
    "scores_rf.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Survival curves of all patients: Mean survival function across folds with variability and event observation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assuming best_alpha, best_l1_ratio, coxnet_best, and Baseline_target_encoding are defined\n",
    "n_estimator = scores_rf['n_estimator'].iloc[0]\n",
    "max_depth = scores_rf['max_depth'].iloc[0]\n",
    "min_samples_split = scores_rf['min_samples_split'].iloc[0]\n",
    "min_samples_leaf = scores_rf['min_samples_leaf'].iloc[0]\n",
    "\n",
    "# Extract all PATNO numbers\n",
    "patient_patnos = X.index\n",
    "\n",
    "# Number of rows and columns for subplot\n",
    "n_cols = 3\n",
    "n_rows = len(patient_patnos) // n_cols + (len(patient_patnos) % n_cols > 0)\n",
    "\n",
    "# Create a figure with subplots\n",
    "plt.figure(figsize=(5 * n_cols, 4 * n_rows))\n",
    "\n",
    "for j, patient_patno in enumerate(patient_patnos, start=1):\n",
    "    survival_probabilities = []\n",
    "    survival_times = []\n",
    "\n",
    "    observed_event_time = df['time'].loc[patient_patno]\n",
    "\n",
    "    \n",
    "    for i, (train, test) in enumerate(mcv.split(X, tuple_y)):\n",
    "        X_train, X_test = X.iloc[train], X.iloc[test]\n",
    "        y_train, y_test = y[train], y[test]\n",
    "        \n",
    "        X_train, X_test = Preprocessing(X_train=X_train, X_test=X_test, y_train=y_train, target_columns=target_columns)\n",
    "        \n",
    "        rf = RandomSurvivalForest(n_estimators=n_estimator, max_depth=max_depth, min_samples_split=min_samples_split, min_samples_leaf=min_samples_leaf)\n",
    "        rf.fit(X_train, y_train)\n",
    "\n",
    "        if patient_patno in X_test.index:\n",
    "            patient_data = X_test.loc[[patient_patno]]\n",
    "            surv_func = rf.predict_survival_function(patient_data)\n",
    "            survival_probabilities.append(surv_func[0].y)\n",
    "            survival_times.append(surv_func[0].x)\n",
    "    \n",
    "    if survival_times:  # Check if there are any survival times collected\n",
    "        ax = plt.subplot(n_rows, n_cols, j)\n",
    "        \n",
    "        # Plot each survival curve as a thin line\n",
    "        for prob, times in zip(survival_probabilities, survival_times):\n",
    "            plt.plot(times[:len(prob)], prob, color='dodgerblue', linewidth=0.5, alpha=0.5)\n",
    "        \n",
    "        # Compute mean and standard deviation\n",
    "        min_length = min(len(arr) for arr in survival_times)\n",
    "        truncated_probs = [arr[:min_length] for arr in survival_probabilities]\n",
    "        stacked_probabilities = np.vstack(truncated_probs)\n",
    "        mean_survival = np.mean(stacked_probabilities, axis=0)\n",
    "        std_survival = np.std(stacked_probabilities, axis=0)\n",
    "        common_times = np.array(survival_times[0][:min_length])\n",
    "        \n",
    "        # Plot the mean survival function as a thick line\n",
    "        ax.plot(common_times, mean_survival, color='blue', linewidth=2, label=\"Mean Survival Function\")\n",
    "    \n",
    "        # Add standard deviation shading\n",
    "        ax.fill_between(common_times, mean_survival - std_survival, mean_survival + std_survival, color='grey', alpha=0.2, label=\"Std Dev\")        \n",
    "        ax.axvline(x=observed_event_time, color='red', linestyle='--', linewidth=2, label='Observed Event Time')\n",
    "    \n",
    "        ax.set_title(f\"Patient {patient_patno}\")\n",
    "        ax.set_xlabel(\"Time in days\")\n",
    "        ax.set_ylabel(\"Survival probability\")\n",
    "        ax.legend()\n",
    "        ax.grid(True)\n",
    "    else:\n",
    "        # If no data collected for a patient, this space will be empty\n",
    "        print(f\"No survival data collected for patient {patient_patno}.\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Componentwise Gradient Boosting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y, tuple_y, target_columns = x_y_baseline(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sksurv.ensemble import ComponentwiseGradientBoostingSurvivalAnalysis\n",
    "# loss: coxph\n",
    "n_estimators = [10, 20, 30, 40, 50]\n",
    "learning_rates = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1]\n",
    "subsamples = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1]\n",
    "\n",
    "results_cgb = {}\n",
    "feature_importance_cgb = {}\n",
    "conc_cgb = {}\n",
    "\n",
    "for learning_rate in learning_rates: \n",
    "        for n_estimator in n_estimators:\n",
    "            for subsample in subsamples:\n",
    "                with warnings.catch_warnings():\n",
    "                    warnings.simplefilter(\"ignore\", category=UserWarning)\n",
    "                    cgb = ComponentwiseGradientBoostingSurvivalAnalysis(n_estimators=n_estimator,\n",
    "                                                                        learning_rate=learning_rate,\n",
    "                                                                        subsample = subsample,\n",
    "                                                                        random_state=173637)\n",
    "                    conc_train = []\n",
    "                    conc_test = []\n",
    "                    brier = []\n",
    "                    permut = []\n",
    "                    feat_impor = []\n",
    "                    coef = []\n",
    "                    \n",
    "                    print(f'learning_rate: {learning_rate}, n_estimator: {n_estimator}')\n",
    "                \n",
    "                    for i, (train, test) in enumerate(mcv.split(X, tuple_y)):\n",
    "                        X_train, X_test = X.iloc[train], X.iloc[test]\n",
    "                        y_train, y_test = y[train], y[test]\n",
    "                        \n",
    "                        X_train, X_test = Preprocessing(X_train=X_train, X_test=X_test, y_train=y_train, target_columns=target_columns)\n",
    "                            \n",
    "                        # fix the times            \n",
    "                        times_train_min = y_train['time'].min()\n",
    "                        times_train_max = y_train['time'].max()\n",
    "                        times_train = np.arange(0, times_train_max)\n",
    "                        times_test_min = y_test['time'].min()\n",
    "                        times_test_max = y_test['time'].max()\n",
    "                        if times_test_max > times_train_max:\n",
    "                            y_test_red_index = y_test['time'] <= times_train_max\n",
    "                            y_test = y_test[y_test_red_index]\n",
    "                            X_test = X_test[y_test_red_index]\n",
    "                            times_test_max = y_test['time'].max()\n",
    "                        times_test = np.arange(times_test_min, times_test_max)\n",
    "\n",
    "                        \n",
    "                        cgb.fit(X_train, y_train)\n",
    "                        \n",
    "                        # Compute the C-index for test data and train data\n",
    "                        conc_train.append(cgb.score(X_train, y_train))\n",
    "                        conc_test.append(cgb.score(X_test, y_test))\n",
    "\n",
    "                        # Brier Score\n",
    "                        surv_prob_test = np.row_stack([fn(times_test) for fn in cgb.predict_survival_function(X_test)])\n",
    "                        brier.append(integrated_brier_score(y_train, y_test, surv_prob_test, times_test))\n",
    "\n",
    "                        importance = permutation_importance(cgb,\n",
    "                                                            X_test,\n",
    "                                                            y_test,\n",
    "                                                            n_repeats=10,\n",
    "                                                            random_state=1)\n",
    "                        permut.append(importance.importances_mean)\n",
    "\n",
    "                        feat_impor.append(importance)\n",
    "                \n",
    "                    feature_importance_cgb[(n_estimator, learning_rate, subsample)] = feat_impor\n",
    "\n",
    "                    # Evaluate and record the results after each n_estimator and max_depth combination\n",
    "                    avg_conc_test = np.mean(conc_test)\n",
    "                    avg_conc_train = np.mean(conc_train)\n",
    "                    avg_brier = np.mean(brier)\n",
    "                    avg_permut = np.mean(permut)\n",
    "\n",
    "                    results_cgb[(n_estimator, learning_rate, subsample)] = [avg_conc_test, avg_conc_train, avg_brier, avg_permut]\n",
    "                    conc_cgb[(n_estimator, learning_rate, subsample)] = conc_test\n",
    "\n",
    "\n",
    "\n",
    "result = [{\n",
    "    'n_estimator': n_estimator,\n",
    "    'learning_rate': learning_rate, \n",
    "    'subsample': subsample,\n",
    "    'Conc test': avg_conc_test,\n",
    "    'Conc train': avg_conc_train,   \n",
    "    'Brier Score': avg_brier,\n",
    "    'Permut': avg_permut\n",
    "} for (n_estimator, learning_rate, subsample), (avg_conc_test, avg_conc_train, avg_brier, avg_permut) in results_cgb.items()]\n",
    "\n",
    "# Create the DataFrame\n",
    "results_cgb = pd.DataFrame(result)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sort the result by the best Concordance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_cgb = results_cgb.sort_values(by='Conc test', ascending=False).reset_index(drop=True)\n",
    "\n",
    "# Print out the sorted DataFrame\n",
    "scores_cgb.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Survival curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assuming best_alpha, best_l1_ratio, coxnet_best, and Baseline_target_encoding are defined\n",
    "n_estimator_cgb = scores_cgb['n_estimator'].iloc[0]\n",
    "subsample = scores_cgb['subsample'].iloc[0]\n",
    "learning_rate = scores_cgb['learning_rate'].iloc[0]\n",
    "\n",
    "# Extract all PATNO numbers\n",
    "patient_patnos = X.index\n",
    "#patient_patnos = [9007, 9024, 9040, 9065, 9056, 9033, 9008]\n",
    "\n",
    "print(patient_patnos)\n",
    "\n",
    "# Number of rows and columns for subplot\n",
    "n_cols = 3\n",
    "n_rows = len(patient_patnos) // n_cols + (len(patient_patnos) % n_cols > 0)\n",
    "\n",
    "# Create a figure with subplots\n",
    "plt.figure(figsize=(5 * n_cols, 4 * n_rows))\n",
    "\n",
    "expected_survival_times = {}\n",
    "\n",
    "for j, patient_patno in enumerate(patient_patnos, start=1):\n",
    "    survival_probabilities = []\n",
    "    survival_times = []\n",
    "\n",
    "    observed_event_time = df['time'].loc[patient_patno]\n",
    "\n",
    "    \n",
    "    for i, (train, test) in enumerate(mcv.split(X, tuple_y)):\n",
    "        X_train, X_test = X.iloc[train], X.iloc[test]\n",
    "        y_train, y_test = y[train], y[test]\n",
    "        \n",
    "        X_train, X_test = Preprocessing(X_train=X_train, X_test=X_test, y_train=y_train, target_columns=target_columns)\n",
    "        \n",
    "        cgb = ComponentwiseGradientBoostingSurvivalAnalysis(n_estimators=n_estimator_cgb,\n",
    "                                                            learning_rate=learning_rate,\n",
    "                                                            subsample = subsample,\n",
    "                                                            random_state=173637)\n",
    "        cgb.fit(X_train, y_train)\n",
    "\n",
    "        if patient_patno in X_test.index:\n",
    "            patient_data = X_test.loc[[patient_patno]]\n",
    "            surv_func = cgb.predict_survival_function(patient_data)\n",
    "            survival_probabilities.append(surv_func[0].y)\n",
    "            survival_times.append(surv_func[0].x)\n",
    "    \n",
    "    if survival_times:  # Check if there are any survival times collected\n",
    "        ax = plt.subplot(n_rows, n_cols, j)\n",
    "        \n",
    "        # Plot each survival curve as a thin line\n",
    "        for prob, times in zip(survival_probabilities, survival_times):\n",
    "            plt.plot(times[:len(prob)], prob, color='dodgerblue', linewidth=0.5, alpha=0.5)\n",
    "        \n",
    "        # Compute mean and standard deviation\n",
    "        min_length = min(len(arr) for arr in survival_times)\n",
    "        truncated_probs = [arr[:min_length] for arr in survival_probabilities]\n",
    "        stacked_probabilities = np.vstack(truncated_probs)\n",
    "        mean_survival = np.mean(stacked_probabilities, axis=0)\n",
    "        std_survival = np.std(stacked_probabilities, axis=0)\n",
    "        common_times = np.array(survival_times[0][:min_length])\n",
    "\n",
    "        # Now calculate the expected survival time for the patient\n",
    "        expected_survival_time = approximate_integral(mean_survival, common_times)\n",
    "        expected_survival_times[patient_patno] = expected_survival_time\n",
    "        \n",
    "        # Plot the mean survival function as a thick line\n",
    "        ax.plot(common_times, mean_survival, color='blue', linewidth=2, label=\"Mean Survival Function\")\n",
    "    \n",
    "        # Add standard deviation shading\n",
    "        ax.fill_between(common_times, mean_survival - std_survival, mean_survival + std_survival, color='grey', alpha=0.2, label=\"Std Dev\")\n",
    "        ax.axvline(x=observed_event_time, color='red', linestyle='--', linewidth=2, label=f'Observed Event Time: {observed_event_time:.0f} days')\n",
    "        ax.axvline(x=expected_survival_time, color='green', linestyle='--', linewidth=2, label=f'Expected Survival Time: {expected_survival_time:.0f} days')       \n",
    "    \n",
    "        ax.set_title(f\"Patient {patient_patno}\")\n",
    "        ax.set_xlabel(\"Time in days\")\n",
    "        ax.set_ylabel(\"Survival probability\")\n",
    "        ax.legend()\n",
    "        ax.grid(True)\n",
    "    else:\n",
    "        # If no data collected for a patient, this space will be empty\n",
    "        print(f\"No survival data collected for patient {patient_patno}.\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Average Permutation importance for all models\n",
    "Refer to the plot at the bottom of this section to view the average permutation importance across all models (Coxnet, CoxPH, RSF)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Coxnet\n",
    "alpha_coxnet = scores_coxnet['Alpha'].iloc[8]\n",
    "l1_ratio = scores_coxnet['L1 Ratio'].iloc[8]\n",
    "best_feature_importance_coxnet = feature_importance_coxnet[(alpha_coxnet, l1_ratio)]\n",
    "# CoxPH\n",
    "alpha_coxph = scores_coxph['Alpha'].iloc[1]\n",
    "best_feature_importance_ph = feature_importance_ph[(alpha_coxph)]\n",
    "# RSF\n",
    "n_estimator_rsf = scores_rf['n_estimator'].iloc[0]\n",
    "max_depth_rsf = scores_rf['max_depth'].iloc[0]\n",
    "min_samples_split_rsf = scores_rf['min_samples_split'].iloc[0]\n",
    "min_samples_leaf_rsf = scores_rf['min_samples_leaf'].iloc[0]\n",
    "best_feature_importance_rf = feature_importance_rf[(n_estimator_rsf, max_depth_rsf, min_samples_split_rsf, min_samples_leaf_rsf)]\n",
    "\n",
    "n_estimator_cgb = scores_cgb['n_estimator'].iloc[0]\n",
    "learning_rate_cgb = scores_cgb['learning_rate'].iloc[0]\n",
    "subsample_cgb = scores_cgb['subsample'].iloc[0]\n",
    "best_feature_importance_cgb = feature_importance_cgb[(n_estimator_cgb, learning_rate_cgb, subsample_cgb)]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Coxnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "average_importance_coxnet = np.mean([imp.importances_mean for imp in best_feature_importance_coxnet], axis=0)\n",
    "\n",
    "importances_coxnet = pd.DataFrame({\n",
    "    'Feature': X_train.columns,\n",
    "    'Importance coxnet': average_importance_coxnet\n",
    "})\n",
    "top_importances_coxnet = importances_coxnet.sort_values(by='Importance coxnet', ascending=True).reset_index(drop=True)\n",
    "#alpha_coxnet = scores_coxnet['Alpha'].iloc[8]\n",
    "#l1_ratio = scores_coxnet['L1 Ratio'].iloc[8]\n",
    "\n",
    "\n",
    "# Plot the bar chart\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.barh(top_importances_coxnet[\"Feature\"].iloc[-20:], top_importances_coxnet[\"Importance coxnet\"].iloc[-20:], color='skyblue')\n",
    "plt.xlabel(\"Average Importance\")\n",
    "plt.ylabel('Feature')\n",
    "plt.title(f\"Average Permutation importance of Coxnet (alpha:{alpha_coxnet}, L1 ratio: {l1_ratio}) \")\n",
    "plt.axvline(x=0, color=\"k\", linestyle=\"--\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "average_importance_coxnet = np.mean([imp.importances_mean for imp in best_feature_importance_coxnet], axis=0)\n",
    "\n",
    "importances_coxnet = pd.DataFrame({\n",
    "    'Feature': X_train.columns,\n",
    "    'Importance coxnet': average_importance_coxnet\n",
    "})\n",
    "top_importances_coxnet = importances_coxnet.sort_values(by='Importance coxnet', ascending=True).reset_index(drop=True)\n",
    "\n",
    "\n",
    "# Plot the bar chart\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.barh(top_importances_coxnet[\"Feature\"].iloc[-30:], top_importances_coxnet[\"Importance coxnet\"].iloc[-30:], color='skyblue')\n",
    "plt.xlabel(\"Average Decrease in Concordance (Coxnet)\")\n",
    "plt.ylabel('Feature')\n",
    "plt.title(\"Average Permutation importance \")\n",
    "plt.axvline(x=0, color=\"k\", linestyle=\"--\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### COX PH:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "average_importance_ph = np.mean([imp.importances_mean for imp in best_feature_importance_ph], axis=0)\n",
    "\n",
    "importances_ph = pd.DataFrame({\n",
    "    'Feature': X_train.columns,\n",
    "    'Importance ph': average_importance_ph\n",
    "})\n",
    "top_importances_ph = importances_ph.sort_values(by='Importance ph', ascending=True).reset_index(drop=True)\n",
    "\n",
    "# Plot the bar chart\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.barh(top_importances_ph[\"Feature\"].iloc[-20:], top_importances_ph[\"Importance ph\"].iloc[-20:], color='skyblue')\n",
    "plt.xlabel(\"Average Decrease in Concordance \")\n",
    "plt.ylabel('Feature')\n",
    "plt.title(\"Average Permutation importance \")\n",
    "plt.axvline(x=0, color=\"k\", linestyle=\"--\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RSF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "average_importance_rf = np.mean([imp.importances_mean for imp in best_feature_importance_rf], axis=0)\n",
    "\n",
    "importances_rf = pd.DataFrame({\n",
    "    'Feature': X_train.columns,\n",
    "    'Importance rsf': average_importance_rf\n",
    "})\n",
    "top_importances_rf = importances_rf.sort_values(by='Importance rsf', ascending=True).reset_index(drop=True)\n",
    "\n",
    "# Plot the bar chart\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.barh(top_importances_rf[\"Feature\"].iloc[-20:], top_importances_rf[\"Importance rsf\"].iloc[-20:], color='skyblue')\n",
    "plt.xlabel(\"Average Decrease in Concordance\")\n",
    "plt.ylabel('Feature')\n",
    "plt.title(\"Average Permutation importance(RSF)\")\n",
    "plt.axvline(x=0, color=\"k\", linestyle=\"--\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CGB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "average_importance_cgb = np.mean([imp.importances_mean for imp in best_feature_importance_cgb], axis=0)\n",
    "\n",
    "importances_cgb = pd.DataFrame({\n",
    "    'Feature': X_train.columns,\n",
    "    'Importance cgb': average_importance_cgb\n",
    "})\n",
    "top_importances_cgb = importances_cgb.sort_values(by='Importance cgb', ascending=True).reset_index(drop=True)\n",
    "\n",
    "# Plot the bar chart\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.barh(top_importances_cgb[\"Feature\"].iloc[-20:], top_importances_cgb[\"Importance cgb\"].iloc[-20:], color='skyblue')\n",
    "plt.xlabel(\"Average Decrease in Concordance\")\n",
    "plt.ylabel('Feature')\n",
    "plt.title(\"Average Permutation importance(CGB)\")\n",
    "plt.axvline(x=0, color=\"k\", linestyle=\"--\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Top 10 Feature Importance from Three Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.colors as mcolors\n",
    "\n",
    "top_importances_coxnet = importances_coxnet.sort_values(by='Importance coxnet', ascending=False).reset_index(drop=True)\n",
    "top_importances_ph = importances_ph.sort_values(by='Importance ph', ascending=False).reset_index(drop=True)\n",
    "top_importances_rf = importances_rf.sort_values(by='Importance rsf', ascending=False).reset_index(drop=True)\n",
    "top_importances_cgb = importances_cgb.sort_values(by='Importance cgb', ascending=False).reset_index(drop=True)\n",
    "\n",
    "\n",
    "#merged_df = pd.merge(top_importances_coxnet, top_importances_ph, on='Feature', suffixes=('_coxnet', '_ph'))\n",
    "merged_df = pd.merge(top_importances_coxnet, top_importances_ph, on='Feature')\n",
    "merged_df = pd.merge(merged_df, top_importances_rf, on='Feature', how='outer')\n",
    "merged_df = pd.merge(merged_df, top_importances_cgb, on='Feature', how='outer')\n",
    "\n",
    "numeric_columns = ['Importance coxnet', 'Importance ph', 'Importance rsf', 'Importance cgb']\n",
    "merged_df['Average_Importance'] = merged_df[numeric_columns].mean(axis=1)\n",
    "\n",
    "top_merged_df = merged_df.nlargest(15, 'Average_Importance')\n",
    "\n",
    "# Plotting\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "\n",
    "# We will have bars for each of the top 10 features, from each model\n",
    "n = len(top_merged_df['Feature'])\n",
    "index = np.arange(n)\n",
    "bar_width = 0.15  # Reduce bar width to fit three bars\n",
    "\n",
    "color_keys = list(mcolors.TABLEAU_COLORS.keys())  # Get color keys from TABLEAU_COLORS\n",
    "\n",
    "bars1 = ax.bar(index - bar_width, top_merged_df['Importance coxnet'], bar_width, label='Coxnet', color=mcolors.TABLEAU_COLORS[color_keys[0]])\n",
    "bars2 = ax.bar(index, top_merged_df['Importance ph'], bar_width, label='Cox PH', color=mcolors.TABLEAU_COLORS[color_keys[1]])\n",
    "bars3 = ax.bar(index + bar_width, top_merged_df['Importance rsf'], bar_width, label='RSF', color=mcolors.TABLEAU_COLORS[color_keys[2]])\n",
    "bars4 = ax.bar(index + 2*bar_width, top_merged_df['Importance cgb'], bar_width, label='CGB',color=mcolors.TABLEAU_COLORS[color_keys[3]])\n",
    "\n",
    "ax.plot(index, top_merged_df['Average_Importance'], color=mcolors.TABLEAU_COLORS[color_keys[4]], marker= 'o', label='Average Importance')\n",
    "\n",
    "# Adding labels, title, and legend\n",
    "ax.set_xlabel('Feature')\n",
    "ax.set_ylabel('Importance')\n",
    "#ax.set_title('Top 10 Feature Importance from Three Models')\n",
    "ax.set_xticks(index)\n",
    "ax.set_xticklabels(top_merged_df['Feature'], rotation=45)\n",
    "ax.legend()\n",
    "\n",
    "# Showing the plot\n",
    "plt.setp(ax.get_xticklabels(), rotation=45, ha='right')\n",
    "plt.tight_layout()  # Adjust layout to fit all labels\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Time dependent brier score between Coxnet/CoxPH/RSF/CGB\n",
    "\n",
    "Plot the time dependent brier score for each model.\n",
    "Since we use RepeatedKFold, this code takes the average for every 5 folds first, and then takes the average of these averages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sksurv.metrics import brier_score\n",
    "\n",
    "# Configurations for models\n",
    "best_alpha_coxnet = scores_coxnet['Alpha'].iloc[8]\n",
    "best_l1_ratio = scores_coxnet['L1 Ratio'].iloc[8]\n",
    "best_alpha_coxph = scores_coxph['Alpha'].iloc[1]\n",
    "n_estimator = scores_rf['n_estimator'].iloc[0]\n",
    "max_depth = scores_rf['max_depth'].iloc[0]\n",
    "min_samples_split = scores_rf['min_samples_split'].iloc[0]\n",
    "min_samples_leaf = scores_rf['min_samples_leaf'].iloc[0]\n",
    "\n",
    "n_estimator_cgb = scores_cgb['n_estimator'].iloc[0]\n",
    "subsample = scores_cgb['subsample'].iloc[0]\n",
    "learning_rate = scores_cgb['learning_rate'].iloc[0]\n",
    "\n",
    "\n",
    "# Initialize RandomSurvivalForest\n",
    "best_rsf = RandomSurvivalForest(\n",
    "    n_estimators=n_estimator,\n",
    "    max_depth=max_depth,\n",
    "    min_samples_split=min_samples_split,\n",
    "    min_samples_leaf=min_samples_leaf\n",
    ")\n",
    "\n",
    "best_cgb = ComponentwiseGradientBoostingSurvivalAnalysis(n_estimators=n_estimator_cgb,\n",
    "                                                         learning_rate=learning_rate,\n",
    "                                                         subsample = subsample,\n",
    "                                                         random_state=173637)\n",
    "# Initialize storage for results\n",
    "results = {\n",
    "    \"Coxnet\": {\"times\": [], \"scores\": []},\n",
    "    \"CoxPH\": {\"times\": [], \"scores\": []},\n",
    "    \"RSF\": {\"times\": [], \"scores\": []},\n",
    "    \"CGB\": {\"times\": [], \"scores\": []}\n",
    "    }\n",
    "\n",
    "# Run cross-validation for all models\n",
    "for model_name, model in [\n",
    "    (\"Coxnet\", CoxnetSurvivalAnalysis(l1_ratio=best_l1_ratio, alphas=[best_alpha_coxnet], fit_baseline_model=True)),\n",
    "    (\"CoxPH\", CoxPHSurvivalAnalysis(alpha=best_alpha_coxph, ties=\"efron\")),\n",
    "    (\"RSF\", best_rsf), (\"CGB\", best_cgb)]:\n",
    "    \n",
    "    for train, test in mcv.split(X, tuple_y):\n",
    "        X_train, X_test = X.iloc[train], X.iloc[test]\n",
    "        y_train, y_test = y[train], y[test]\n",
    "\n",
    "        X_train, X_test = Preprocessing(X_train=X_train, X_test=X_test, y_train=y_train, target_columns=target_columns)\n",
    "\n",
    "        # We have to slice the times\n",
    "        y_test = pd.DataFrame(y_test, columns=['status', 'time'])\n",
    "\n",
    "        max_train = max(y_train[\"time\"])\n",
    "        times_x_axis_train = np.arange(0, max_train)\n",
    "        y_test = y_test[y_test[\"time\"] <= max_train]\n",
    "        indices = y_test[y_test[\"time\"] <= max_train].index\n",
    "        X_test = X_test.iloc[indices]\n",
    "        times_x_axis_test = np.arange(min(y_test[\"time\"]), max(y_test[\"time\"]))\n",
    "\n",
    "        # Fit model\n",
    "        model.fit(X_train, y_train)\n",
    "        survival_pred = model.predict_survival_function(X_test)\n",
    "        survival_preds = np.asarray([[fn(t) for t in times_x_axis_test] for fn in survival_pred])\n",
    "        time, score = brier_score(y_train, y_test.to_records(index=False), survival_preds, times_x_axis_test)\n",
    "\n",
    "        results[model_name][\"times\"].append(time)\n",
    "        results[model_name][\"scores\"].append(score)\n",
    "\n",
    "# Plotting\n",
    "fig, ax = plt.subplots(figsize=(12, 8))\n",
    "\n",
    "color=mcolors.TABLEAU_COLORS[color_keys[0]]\n",
    "\n",
    "colors = {\"Coxnet\": mcolors.TABLEAU_COLORS[color_keys[0]], \"CoxPH\": mcolors.TABLEAU_COLORS[color_keys[1]], \"RSF\": mcolors.TABLEAU_COLORS[color_keys[2]], \"CGB\": mcolors.TABLEAU_COLORS[color_keys[3]]}\n",
    "color = {\"Coxnet\": \"dodgerblue\", \"CoxPH\": \"lightsalmon\", \"RSF\": 'limegreen', \"CGB\": \"indianred\"}\n",
    "labels = {\"Coxnet\": \"Coxnet mean Brier Score\", \"CoxPH\": \"CoxPH mean Brier Score\", \"RSF\": \"RSF mean Brier Score\",\n",
    "          \"CGB\": \"CGB mean Brier Score\"}\n",
    "\n",
    "for model_name in [\"Coxnet\", \"CoxPH\", \"RSF\", \"CGB\"]:\n",
    "    final_mean_briers = []\n",
    "    final_common_times = []\n",
    "    for i in range(0, len(results[model_name][\"scores\"]), 5):\n",
    "        current_times = results[model_name][\"times\"][i:i + 5]\n",
    "        current_scores = results[model_name][\"scores\"][i:i + 5]\n",
    "        min_length = min(len(arr) for arr in current_times)\n",
    "        truncated_scores = [arr[:min_length] for arr in current_scores]\n",
    "        stacked_scores = np.vstack(truncated_scores)\n",
    "        mean_scores = np.mean(stacked_scores, axis=0)\n",
    "        final_mean_briers.append(mean_scores)\n",
    "        final_common_times.append(np.array(current_times[0][:min_length]))\n",
    "    \n",
    "    # Plot each brier score as a thin line (dette kan fjernes!)\n",
    "    for prob, times in zip(final_mean_briers, final_common_times):\n",
    "        plt.plot(times[:len(prob)], prob, color=color[model_name], linewidth=0.5, alpha=0.5)\n",
    "\n",
    "    min_length = min(len(arr) for arr in final_common_times)\n",
    "    truncated_mean_briers = [arr[:min_length] for arr in final_mean_briers]\n",
    "    stacked_mean_briers = np.vstack(truncated_mean_briers)\n",
    "    overall_mean = np.mean(stacked_mean_briers, axis=0)\n",
    "    overall_std = np.std(stacked_mean_briers, axis=0)\n",
    "    common_times = np.array(final_common_times[0][:min_length])\n",
    "\n",
    "    # Plot the mean and standard deviation for each model\n",
    "    ax.plot(common_times, overall_mean, color=colors[model_name], linewidth=2, label=f\"{labels[model_name]}\")\n",
    "    ax.fill_between(common_times, overall_mean - overall_std, overall_mean + overall_std, color=colors[model_name], alpha=0.2)\n",
    "\n",
    "# Finalizing the plot with titles and labels\n",
    "#ax.set_title(\"Comparison of Mean Time-dependent Brier Scores: Coxnet vs. CoxPH vs. RSF vs CGB\", fontsize=16)\n",
    "ax.set_xlabel(\"Time in days\", fontsize=14)\n",
    "ax.set_ylabel(\"Mean Time dependent Brier score\", fontsize=14)\n",
    "ax.legend(fontsize=12)\n",
    "ax.grid(True)\n",
    "\n",
    "# Retrieve model statistics\n",
    "brier_coxnet = scores_coxnet['Brier Score'].iloc[8]\n",
    "conc_coxnet = scores_coxnet['Conc test'].iloc[8]\n",
    "conc_coxnet_train = scores_coxnet['Conc train'].iloc[8]\n",
    "alpha_coxnet = scores_coxnet['Alpha'].iloc[8]\n",
    "l1_coxnet = scores_coxnet['L1 Ratio'].iloc[8]\n",
    "brier_coxph = scores_coxph['Brier Score'].iloc[1]\n",
    "conc_coxph = scores_coxph['Conc test'].iloc[1]\n",
    "conc_coxph_train = scores_coxph['Conc train'].iloc[1]\n",
    "alpha_coxph = scores_coxph['Alpha'].iloc[1]\n",
    "brier_rsf = scores_rf['Brier Score'].iloc[0]\n",
    "conc_rsf = scores_rf['Conc test'].iloc[0]\n",
    "conc_rsf_train = scores_rf['Conc train'].iloc[0]\n",
    "n_estimator_rsf = scores_rf['n_estimator'].iloc[0]\n",
    "max_depth_rsf = scores_rf['max_depth'].iloc[0]\n",
    "min_samples_split_rsf = scores_rf['min_samples_split'].iloc[0]\n",
    "min_samples_leaf_rsf = scores_rf['min_samples_leaf'].iloc[0]\n",
    "brier_cgb = scores_cgb['Brier Score'].iloc[0]\n",
    "conc_cgb = scores_cgb['Conc test'].iloc[0]\n",
    "conc_cgb_train = scores_cgb['Conc train'].iloc[0]\n",
    "n_estimator_cgb = scores_cgb['n_estimator'].iloc[0]\n",
    "learning_rate_cgb = scores_cgb['learning_rate'].iloc[0]\n",
    "subsample_cgb = scores_cgb['subsample'].iloc[0]\n",
    "\n",
    "# Adding a text box with Brier scores and concordance indices\n",
    "\n",
    "\n",
    "textstr = '\\n'.join((\n",
    "    f'Coxnet (alpha: {alpha_coxnet}, L1 ratio: {l1_coxnet}):',\n",
    "    f'IBS: {brier_coxnet:.3f}',\n",
    "    f'C-Index: (test: {conc_coxnet:.3f}, train:{conc_coxph_train:.3f})',\n",
    "    f'--------------------------------------------------',\n",
    "    f'CoxPH (alpha: {alpha_coxph}):',\n",
    "    f'IBS: {brier_coxph:.3f}',\n",
    "    f'C-Index: (test: {conc_coxph:.3f}, train: {conc_coxnet_train:.3f})',\n",
    "    f'--------------------------------------------------',\n",
    "    f'RSF: (n_estimator: {n_estimator_rsf}, max_depth: {max_depth_rsf}',\n",
    "    f'min_samples_split: {min_samples_split_rsf}, min_samples_leaf: {min_samples_leaf_rsf})',\n",
    "    f'IBS: {brier_rsf:.3f}',\n",
    "    f'C-index: (test: {conc_rsf:.3f}, train: {conc_rsf_train:.3f})',\n",
    "    f'-------------------------------------------------',\n",
    "    f'CGB: (n_estimator: {n_estimator_cgb} ,learning_rate: {learning_rate_cgb}, subsample: {subsample_cgb})',\n",
    "    f'IBS: {brier_cgb:.3f}',\n",
    "    f'C-index: (test: {conc_cgb:.3f}, train: {conc_cgb_train:.3f})'\n",
    "))\n",
    "\n",
    "# Place a text box in upper left in axes coords\n",
    "props = dict(boxstyle='round', facecolor='whitesmoke', alpha=0.5)\n",
    "ax.text(0.72, 0.81, textstr, transform=ax.transAxes, fontsize=10,\n",
    "        verticalalignment='top', bbox=props)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Surivival Curves for some pasients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sksurv.linear_model import CoxnetSurvivalAnalysis\n",
    "from sksurv.ensemble import ComponentwiseGradientBoostingSurvivalAnalysis\n",
    "\n",
    "def approximate_integral(survival_probabilities, time_points):\n",
    "    # Calculate time steps for each interval\n",
    "    time_steps = np.diff(time_points)\n",
    "    survival_probabilities_mid = (survival_probabilities[:-1] + survival_probabilities[1:]) / 2\n",
    "    # Perform the integration using the trapezoidal rule\n",
    "    return np.sum(survival_probabilities_mid * time_steps)\n",
    "\n",
    "\n",
    "# Model initialization with provided parameters\n",
    "coxnet_best = CoxnetSurvivalAnalysis(l1_ratio=scores_coxnet['L1 Ratio'].iloc[8], alphas=[scores_coxnet['Alpha'].iloc[8]], fit_baseline_model=True)\n",
    "cgb_best = ComponentwiseGradientBoostingSurvivalAnalysis(n_estimators=scores_cgb['n_estimator'].iloc[0], learning_rate=scores_cgb['learning_rate'].iloc[0], subsample=scores_cgb['subsample'].iloc[0], random_state=173637)\n",
    "\n",
    "#censored_patnos = [9001, 9033, 9056, 9065, 9099, 9110, 9111, 9114]\n",
    "#patient_patnos_filtered = [patno for patno in patient_patnos if patno not in censored_patnos]\n",
    "\n",
    "#patient_patnos_filtered = [9117, 9118, 9119, 9120, 9121]\n",
    "patient_patnos_filtered = [9007, 9024, 9065, 9040]\n",
    "\n",
    "\n",
    "# Plot configuration\n",
    "n_cols = 2\n",
    "n_rows = len(patient_patnos_filtered)\n",
    "plt.figure(figsize=(5 * n_cols, 4 * n_rows))\n",
    "\n",
    "# Process each patient for each model\n",
    "for patient_index, patient_patno in enumerate(patient_patnos_filtered):\n",
    "\n",
    "    # Initialize survival probabilities and times for both models\n",
    "    survival_probabilities_coxnet = []\n",
    "    survival_times_coxnet = []\n",
    "    survival_probabilities_cgb = []\n",
    "    survival_times_cgb = []\n",
    "\n",
    "    observed_event_time = df['time'].loc[patient_patno]\n",
    "\n",
    "    # Cross-validation and prediction for Coxnet model\n",
    "    for i, (train, test) in enumerate(mcv.split(X, tuple_y)):\n",
    "        X_train, X_test = X.iloc[train], X.iloc[test]\n",
    "        y_train, y_test = y[train], y[test]\n",
    "\n",
    "        X_train, X_test = Preprocessing(X_train=X_train, X_test=X_test, y_train=y_train, target_columns=target_columns)\n",
    "            \n",
    "        coxnet_best.fit(X_train, y_train)\n",
    "        if patient_patno in X_test.index:\n",
    "            patient_data = X_test.loc[[patient_patno]]\n",
    "            surv_func = coxnet_best.predict_survival_function(patient_data)\n",
    "            survival_probabilities_coxnet.append(surv_func[0].y)\n",
    "            survival_times_coxnet.append(surv_func[0].x)\n",
    "\n",
    "    # Cross-validation and prediction for CGB model\n",
    "    for i, (train, test) in enumerate(mcv.split(X, tuple_y)):\n",
    "        X_train, X_test = X.iloc[train], X.iloc[test]\n",
    "        y_train, y_test = y[train], y[test]\n",
    "\n",
    "        X_train, X_test = Preprocessing(X_train=X_train, X_test=X_test, y_train=y_train, target_columns=target_columns)\n",
    "            \n",
    "        cgb_best.fit(X_train, y_train)\n",
    "        if patient_patno in X_test.index:\n",
    "            patient_data = X_test.loc[[patient_patno]]\n",
    "            surv_func = cgb_best.predict_survival_function(patient_data)\n",
    "            survival_probabilities_cgb.append(surv_func[0].y)\n",
    "            survival_times_cgb.append(surv_func[0].x)\n",
    "\n",
    "    # Plotting for Coxnet model\n",
    "    ax = plt.subplot(n_rows, n_cols, patient_index * n_cols + 1)\n",
    "    if survival_times_coxnet:\n",
    "        # Plot each survival curve as a thin line\n",
    "        for prob, times in zip(survival_probabilities_coxnet, survival_times_coxnet):\n",
    "            plt.plot(times[:len(prob)], prob, color='dodgerblue', linewidth=0.5, alpha=0.5)\n",
    "\n",
    "        # Calculate mean and standard deviation\n",
    "        min_length = min(len(arr) for arr in survival_times_coxnet)\n",
    "        truncated_probs = [arr[:min_length] for arr in survival_probabilities_coxnet]\n",
    "        stacked_probabilities = np.vstack(truncated_probs)\n",
    "        mean_survival = np.mean(stacked_probabilities, axis=0)\n",
    "        std_survival = np.std(stacked_probabilities, axis=0)\n",
    "        common_times = np.array(survival_times_coxnet[0][:min_length])\n",
    "\n",
    "        expected_survival_time = approximate_integral(mean_survival, common_times)\n",
    "\n",
    "        ax.plot(common_times, mean_survival, color='blue', linewidth=2, label=\"Mean Survival Function (Coxnet)\")\n",
    "        ax.fill_between(common_times, mean_survival - std_survival, mean_survival + std_survival, color='grey', alpha=0.2, label=\"Std Dev (Coxnet)\")\n",
    "        ax.axvline(x=observed_event_time, color='red', linestyle='--', linewidth=2, label=f'Observed: {observed_event_time:.0f} days')\n",
    "        ax.axvline(x=expected_survival_time, color='green', linestyle='--', linewidth=2, label=f'Expected: {expected_survival_time:.0f} days')\n",
    "\n",
    "        ax.set_title(f\"Patient {patient_patno} - Coxnet (alpha: 3, L1 Ratio: 0.01)\")\n",
    "        ax.set_xlabel(\"Time in days\")\n",
    "        ax.set_ylabel(\"Survival probability\")\n",
    "        ax.legend()\n",
    "        ax.grid(True)\n",
    "    else:\n",
    "        ax.text(0.5, 0.5, \"No data (Coxnet)\", horizontalalignment='center', verticalalignment='center', transform=ax.transAxes)\n",
    "\n",
    "    # Plotting for CGB model\n",
    "    ax = plt.subplot(n_rows, n_cols, patient_index * n_cols + 2)\n",
    "    if survival_times_cgb:\n",
    "        # Plot each survival curve as a thin line\n",
    "        for prob, times in zip(survival_probabilities_cgb, survival_times_cgb):\n",
    "            plt.plot(times[:len(prob)], prob, color='orange', linewidth=0.5, alpha=0.5)\n",
    "\n",
    "        # Calculate mean and standard deviation\n",
    "        min_length = min(len(arr) for arr in survival_times_cgb)\n",
    "        truncated_probs = [arr[:min_length] for arr in survival_probabilities_cgb]\n",
    "        stacked_probabilities = np.vstack(truncated_probs)\n",
    "        mean_survival = np.mean(stacked_probabilities, axis=0)\n",
    "        std_survival = np.std(stacked_probabilities, axis=0)\n",
    "        common_times = np.array(survival_times_cgb[0][:min_length])\n",
    "\n",
    "        expected_survival_time = approximate_integral(mean_survival, common_times)\n",
    "\n",
    "        ax.plot(common_times, mean_survival, color='orange', linewidth=2, label=\"Mean Survival Function (CGB)\")\n",
    "        ax.fill_between(common_times, mean_survival - std_survival, mean_survival + std_survival, color='grey', alpha=0.2, label=\"Std Dev (CGB)\")\n",
    "        ax.axvline(x=observed_event_time, color='red', linestyle='--', linewidth=2, label=f'Observed: {observed_event_time:.0f} days')\n",
    "        ax.axvline(x=expected_survival_time, color='green', linestyle='--', linewidth=2, label=f'Expected: {expected_survival_time:.0f} days')\n",
    "\n",
    "        ax.set_title(f\"Patient {patient_patno} - CGB\")\n",
    "        ax.set_xlabel(\"Time in days\")\n",
    "        ax.set_ylabel(\"Survival probability\")\n",
    "        ax.legend()\n",
    "        ax.grid(True)\n",
    "    else:\n",
    "        ax.text(0.5, 0.5, \"No data (CGB)\", horizontalalignment='center', verticalalignment='center', transform=ax.transAxes)\n",
    "    \n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sksurv.linear_model import CoxnetSurvivalAnalysis\n",
    "from sksurv.ensemble import ComponentwiseGradientBoostingSurvivalAnalysis\n",
    "\n",
    "def approximate_integral(survival_probabilities, time_points):\n",
    "    # Calculate time steps for each interval\n",
    "    time_steps = np.diff(time_points)\n",
    "    survival_probabilities_mid = (survival_probabilities[:-1] + survival_probabilities[1:]) / 2\n",
    "    # Perform the integration using the trapezoidal rule\n",
    "    return np.sum(survival_probabilities_mid * time_steps)\n",
    "\n",
    "# Define the patients to analyze\n",
    "#patient_patnos = [9007, 9008, 9024, 9040, 9065, 9056, 9033]\n",
    "patient_patnos = X.index\n",
    "\n",
    "#patient_patnos = [9007, 9024, 9065, 9035, 9040, 9100, 9008]\n",
    "#patient_patnos = [9007, 9024, 9065, 9040]\n",
    "\n",
    "\n",
    "# Model initialization with provided parameters\n",
    "coxnet_best = CoxnetSurvivalAnalysis(l1_ratio=scores_coxnet['L1 Ratio'].iloc[0], alphas=[scores_coxnet['Alpha'].iloc[0]], fit_baseline_model=True)\n",
    "cgb_best = ComponentwiseGradientBoostingSurvivalAnalysis(n_estimators=scores_cgb['n_estimator'].iloc[0], learning_rate=scores_cgb['learning_rate'].iloc[0], subsample=scores_cgb['subsample'].iloc[0], random_state=173637)\n",
    "\n",
    "# Plot configuration\n",
    "censored_patnos = [9001, 9033, 9056, 9065, 9099, 9110, 9111, 9114]\n",
    "patient_patnos_filtered = [patno for patno in patient_patnos if patno not in censored_patnos]\n",
    "print(patient_patnos_filtered)\n",
    "\n",
    "# Plot configuration\n",
    "n_cols = 2\n",
    "n_rows = len(patient_patnos_filtered)\n",
    "plt.figure(figsize=(5 * n_cols, 4 * n_rows))\n",
    "\n",
    "# Process each patient for each model\n",
    "for patient_index, patient_patno in enumerate(patient_patnos_filtered):\n",
    "\n",
    "# Process each patient for each model\n",
    "#for patient_index, patient_patno in enumerate(patient_patnos):\n",
    "    models = [coxnet_best, cgb_best]\n",
    "    for model_index, model in enumerate(models):\n",
    "        survival_probabilities = []\n",
    "        survival_times = []\n",
    "\n",
    "        observed_event_time = df['time'].loc[patient_patno]\n",
    "\n",
    "        # Cross-validation and prediction for each model\n",
    "        for i, (train, test) in enumerate(mcv.split(X, tuple_y)):\n",
    "            X_train, X_test = X.iloc[train], X.iloc[test]\n",
    "            y_train, y_test = y[train], y[test]\n",
    "\n",
    "            X_train, X_test = Preprocessing(X_train=X_train, X_test=X_test, y_train=y_train, target_columns=target_columns)\n",
    "            \n",
    "            model.fit(X_train, y_train)\n",
    "            if patient_patno in X_test.index:\n",
    "                patient_data = X_test.loc[[patient_patno]]\n",
    "                surv_func = model.predict_survival_function(patient_data)\n",
    "                survival_probabilities.append(surv_func[0].y)\n",
    "                survival_times.append(surv_func[0].x)\n",
    "\n",
    "        # Plotting\n",
    "        ax = plt.subplot(n_rows, n_cols, patient_index * n_cols + model_index + 1)\n",
    "        if survival_times:\n",
    "            # Plot each survival curve as a thin line\n",
    "            for prob, times in zip(survival_probabilities, survival_times):\n",
    "                plt.plot(times[:len(prob)], prob, color='dodgerblue', linewidth=0.5, alpha=0.5)\n",
    "\n",
    "            # Calculate mean and standard deviation\n",
    "            min_length = min(len(arr) for arr in survival_times)\n",
    "            truncated_probs = [arr[:min_length] for arr in survival_probabilities]\n",
    "            stacked_probabilities = np.vstack(truncated_probs)\n",
    "            mean_survival = np.mean(stacked_probabilities, axis=0)\n",
    "            std_survival = np.std(stacked_probabilities, axis=0)\n",
    "            common_times = np.array(survival_times[0][:min_length])\n",
    "\n",
    "            expected_survival_time = approximate_integral(mean_survival, common_times)\n",
    "            \n",
    "            ax.plot(common_times, mean_survival, color='blue', linewidth=2, label=\"Mean Survival Function\")\n",
    "            ax.fill_between(common_times, mean_survival - std_survival, mean_survival + std_survival, color='grey', alpha=0.2, label=\"Std Dev\")\n",
    "            ax.axvline(x=observed_event_time, color='red', linestyle='--', linewidth=2, label=f'Observed: {observed_event_time:.0f} days')\n",
    "            ax.axvline(x=expected_survival_time, color='green', linestyle='--', linewidth=2, label=f'Expected: {expected_survival_time:.0f} days')\n",
    "            \n",
    "            model_name = 'Coxnet ' if model_index == 0 else 'CGB'\n",
    "            ax.set_title(f\"Patient {patient_patno} - {model_name}\")\n",
    "            ax.set_xlabel(\"Time in days\")\n",
    "            ax.set_ylabel(\"Survival probability\")\n",
    "            ax.legend()\n",
    "            ax.grid(True)\n",
    "        else:\n",
    "            ax.text(0.5, 0.5, \"No data\", horizontalalignment='center', verticalalignment='center', transform=ax.transAxes)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
